# [2.2] Core Features: Tool Chaining and Orchestration System

## Overview

### Current State
- Individual tools execute in isolation
- No automatic data flow between tools
- Manual coordination of multi-step workflows
- Limited workflow composition capabilities

### Target State
- Declarative tool workflow definitions
- Automatic data transformation between tools
- Conditional execution and branching logic
- Error handling with rollback mechanisms
- Visual workflow representation and monitoring

### Why This Matters
- **Automation**: Enable complex multi-tool workflows without manual intervention
- **Productivity**: Chain tools to accomplish complex tasks automatically
- **Reliability**: Built-in error handling and rollback capabilities
- **Flexibility**: Support both linear chains and complex DAG workflows

## Technical Requirements

### 1. Workflow Definition System

```rust
use std::collections::{HashMap, HashSet};
use serde::{Deserialize, Serialize};
use uuid::Uuid;
use tokio::sync::{RwLock, mpsc};

/// Declarative workflow definition
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowDefinition {
    pub id: Uuid,
    pub name: String,
    pub description: Option<String>,
    pub version: String,
    pub steps: Vec<WorkflowStep>,
    pub connections: Vec<StepConnection>,
    pub error_handling: ErrorHandlingStrategy,
    pub timeout: Duration,
    pub retry_policy: WorkflowRetryPolicy,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkflowStep {
    pub id: String,
    pub tool_name: String,
    pub parameters: ParameterMapping,
    pub conditions: Vec<ExecutionCondition>,
    pub timeout: Option<Duration>,
    pub retry_policy: Option<StepRetryPolicy>,
    pub rollback_action: Option<RollbackAction>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StepConnection {
    pub from_step: String,
    pub to_step: String,
    pub data_mapping: DataMapping,
    pub condition: Option<ConnectionCondition>,
}

/// Parameter mapping for workflow steps
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ParameterMapping {
    /// Static value
    Static(serde_json::Value),
    /// Reference to previous step output
    FromStep {
        step_id: String,
        output_path: String,
    },
    /// Reference to workflow input
    FromInput {
        input_path: String,
    },
    /// Computed value using expression
    Computed {
        expression: String,
        dependencies: Vec<String>,
    },
    /// Template with variable substitution
    Template {
        template: String,
        variables: HashMap<String, ParameterMapping>,
    },
}

/// Data transformation and mapping between steps
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataMapping {
    /// Transformations to apply to the data
    pub transformations: Vec<DataTransformation>,
    /// Output field mapping
    pub field_mapping: HashMap<String, String>,
    /// Filter conditions
    pub filters: Vec<DataFilter>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DataTransformation {
    /// Extract specific fields
    Extract { fields: Vec<String> },
    /// Rename fields
    Rename { mapping: HashMap<String, String> },
    /// Convert data types
    Convert { field: String, target_type: DataType },
    /// Aggregate data
    Aggregate { operation: AggregateOperation },
    /// Custom transformation function
    Function { name: String, arguments: HashMap<String, serde_json::Value> },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DataType {
    String,
    Number,
    Boolean,
    Array,
    Object,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AggregateOperation {
    Count,
    Sum { field: String },
    Average { field: String },
    Max { field: String },
    Min { field: String },
    GroupBy { field: String },
}
```

### 2. Workflow Execution Engine

```rust
/// Workflow execution engine
pub struct WorkflowExecutor {
    /// Parallel execution engine
    parallel_engine: Arc<ParallelExecutionEngine>,
    
    /// Tool registry
    tool_registry: Arc<ToolRegistry>,
    
    /// Data transformation engine
    transform_engine: Arc<DataTransformationEngine>,
    
    /// Active workflow instances
    active_workflows: Arc<RwLock<HashMap<Uuid, WorkflowInstance>>>,
    
    /// Workflow event stream
    event_stream: Arc<WorkflowEventStream>,
    
    /// Expression evaluator
    expression_evaluator: Arc<ExpressionEvaluator>,
}

/// Runtime workflow instance
#[derive(Debug)]
pub struct WorkflowInstance {
    pub id: Uuid,
    pub definition: WorkflowDefinition,
    pub status: WorkflowStatus,
    pub current_step: Option<String>,
    pub step_results: HashMap<String, StepResult>,
    pub workflow_data: WorkflowData,
    pub error_context: Option<WorkflowError>,
    pub started_at: Instant,
    pub completed_at: Option<Instant>,
    pub execution_log: Vec<WorkflowLogEntry>,
}

#[derive(Debug, Clone)]
pub enum WorkflowStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Cancelled,
    RollingBack,
    RolledBack,
}

#[derive(Debug, Clone)]
pub struct StepResult {
    pub step_id: String,
    pub tool_name: String,
    pub status: StepStatus,
    pub result: Option<ToolResult>,
    pub error: Option<ToolError>,
    pub started_at: Instant,
    pub completed_at: Option<Instant>,
    pub execution_time: Duration,
}

#[derive(Debug, Clone)]
pub enum StepStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Skipped,
    RolledBack,
}

impl WorkflowExecutor {
    pub fn new(
        parallel_engine: Arc<ParallelExecutionEngine>,
        tool_registry: Arc<ToolRegistry>,
    ) -> Self {
        Self {
            parallel_engine,
            tool_registry,
            transform_engine: Arc::new(DataTransformationEngine::new()),
            active_workflows: Arc::new(RwLock::new(HashMap::new())),
            event_stream: Arc::new(WorkflowEventStream::new()),
            expression_evaluator: Arc::new(ExpressionEvaluator::new()),
        }
    }
    
    /// Execute a workflow with given inputs
    pub async fn execute_workflow(
        &self,
        definition: WorkflowDefinition,
        inputs: WorkflowInputs,
    ) -> Result<WorkflowHandle, WorkflowError> {
        // Validate workflow definition
        self.validate_workflow(&definition).await?;
        
        // Create workflow instance
        let instance_id = Uuid::new_v4();
        let instance = WorkflowInstance {
            id: instance_id,
            definition: definition.clone(),
            status: WorkflowStatus::Pending,
            current_step: None,
            step_results: HashMap::new(),
            workflow_data: WorkflowData::new(inputs),
            error_context: None,
            started_at: Instant::now(),
            completed_at: None,
            execution_log: Vec::new(),
        };
        
        // Store instance
        self.active_workflows.write().await.insert(instance_id, instance);
        
        // Create handle
        let handle = WorkflowHandle::new(instance_id);
        
        // Start execution
        self.start_workflow_execution(instance_id).await?;
        
        Ok(handle)
    }
    
    /// Start workflow execution
    async fn start_workflow_execution(&self, instance_id: Uuid) -> Result<(), WorkflowError> {
        let executor = self.clone();
        
        tokio::spawn(async move {
            if let Err(error) = executor.execute_workflow_steps(instance_id).await {
                executor.handle_workflow_error(instance_id, error).await;
            }
        });
        
        Ok(())
    }
    
    /// Execute workflow steps according to the dependency graph
    async fn execute_workflow_steps(&self, instance_id: Uuid) -> Result<(), WorkflowError> {
        // Update status to running
        self.update_workflow_status(instance_id, WorkflowStatus::Running).await;
        
        // Build execution graph
        let execution_graph = self.build_execution_graph(instance_id).await?;
        
        // Execute steps according to dependencies
        let mut completed_steps = HashSet::new();
        let mut failed_steps = HashSet::new();
        
        while !execution_graph.is_complete(&completed_steps) {
            // Get ready steps
            let ready_steps = execution_graph.get_ready_steps(&completed_steps, &failed_steps);
            
            if ready_steps.is_empty() {
                // Check for deadlock or all steps failed
                if failed_steps.is_empty() {
                    return Err(WorkflowError::Deadlock);
                } else {
                    return Err(WorkflowError::StepsFailed(failed_steps.into_iter().collect()));
                }
            }
            
            // Execute ready steps in parallel
            let step_futures: Vec<_> = ready_steps
                .into_iter()
                .map(|step_id| self.execute_workflow_step(instance_id, step_id))
                .collect();
            
            let results = futures::future::join_all(step_futures).await;
            
            // Process results
            for (step_id, result) in results {
                match result {
                    Ok(_) => {
                        completed_steps.insert(step_id);
                    }
                    Err(_) => {
                        failed_steps.insert(step_id);
                        
                        // Check if this is a critical failure
                        if self.is_critical_step(&step_id).await {
                            return Err(WorkflowError::CriticalStepFailed(step_id));
                        }
                    }
                }
            }
        }
        
        // Update status to completed
        self.update_workflow_status(instance_id, WorkflowStatus::Completed).await;
        
        Ok(())
    }
    
    /// Execute a single workflow step
    async fn execute_workflow_step(
        &self,
        instance_id: Uuid,
        step_id: String,
    ) -> Result<(), WorkflowError> {
        // Get step definition and current workflow data
        let (step_def, workflow_data) = {
            let workflows = self.active_workflows.read().await;
            let instance = workflows
                .get(&instance_id)
                .ok_or(WorkflowError::InstanceNotFound)?;
            
            let step = instance
                .definition
                .steps
                .iter()
                .find(|s| s.id == step_id)
                .ok_or(WorkflowError::StepNotFound(step_id.clone()))?;
                
            (step.clone(), instance.workflow_data.clone())
        };
        
        // Check execution conditions
        if !self.check_execution_conditions(&step_def, &workflow_data).await? {
            self.mark_step_skipped(instance_id, step_id).await;
            return Ok(());
        }
        
        // Resolve parameters
        let parameters = self.resolve_step_parameters(&step_def, &workflow_data).await?;
        
        // Create tool call
        let tool_call = ToolCall {
            name: step_def.tool_name.clone(),
            arguments: parameters,
        };
        
        // Mark step as running
        self.mark_step_running(instance_id, step_id.clone()).await;
        
        // Execute tool
        let result = self.execute_tool_with_timeout(tool_call, step_def.timeout).await;
        
        // Handle result
        match result {
            Ok(tool_result) => {
                // Apply data transformations
                let transformed_data = self
                    .transform_engine
                    .transform_step_output(&step_def, tool_result.clone())
                    .await?;
                
                // Update workflow data
                self.update_workflow_data(instance_id, &step_id, transformed_data).await;
                
                // Mark step as completed
                self.mark_step_completed(instance_id, step_id, tool_result).await;
                
                Ok(())
            }
            Err(error) => {
                // Handle error based on retry policy
                if let Some(retry_policy) = &step_def.retry_policy {
                    if self.should_retry_step(&step_id, &error, retry_policy).await {
                        return self.execute_workflow_step(instance_id, step_id).await;
                    }
                }
                
                // Mark step as failed
                self.mark_step_failed(instance_id, step_id, error.clone()).await;
                
                Err(WorkflowError::StepExecutionFailed {
                    step_id,
                    error: Box::new(error),
                })
            }
        }
    }
}
```

### 3. Data Transformation Engine

```rust
/// Engine for transforming data between workflow steps
pub struct DataTransformationEngine {
    /// Registered transformation functions
    transform_functions: Arc<RwLock<HashMap<String, TransformFunction>>>,
    
    /// JSON path processor
    json_processor: Arc<JsonPathProcessor>,
    
    /// Template engine
    template_engine: Arc<TemplateEngine>,
}

type TransformFunction = Box<dyn Fn(serde_json::Value) -> Result<serde_json::Value, TransformError> + Send + Sync>;

impl DataTransformationEngine {
    pub fn new() -> Self {
        let mut engine = Self {
            transform_functions: Arc::new(RwLock::new(HashMap::new())),
            json_processor: Arc::new(JsonPathProcessor::new()),
            template_engine: Arc::new(TemplateEngine::new()),
        };
        
        // Register built-in transformations
        engine.register_builtin_transforms();
        
        engine
    }
    
    /// Transform step output according to data mapping
    pub async fn transform_step_output(
        &self,
        step_def: &WorkflowStep,
        tool_result: ToolResult,
    ) -> Result<serde_json::Value, TransformError> {
        let mut data = tool_result.into_json();
        
        // Apply transformations from step connections
        for connection in &step_def.connections {
            data = self.apply_data_mapping(&connection.data_mapping, data).await?;
        }
        
        Ok(data)
    }
    
    /// Apply data mapping transformations
    async fn apply_data_mapping(
        &self,
        mapping: &DataMapping,
        data: serde_json::Value,
    ) -> Result<serde_json::Value, TransformError> {
        let mut result = data;
        
        // Apply filters first
        for filter in &mapping.filters {
            result = self.apply_filter(filter, result).await?;
        }
        
        // Apply transformations
        for transformation in &mapping.transformations {
            result = self.apply_transformation(transformation, result).await?;
        }
        
        // Apply field mapping
        if !mapping.field_mapping.is_empty() {
            result = self.apply_field_mapping(&mapping.field_mapping, result).await?;
        }
        
        Ok(result)
    }
    
    /// Apply a single data transformation
    async fn apply_transformation(
        &self,
        transformation: &DataTransformation,
        data: serde_json::Value,
    ) -> Result<serde_json::Value, TransformError> {
        match transformation {
            DataTransformation::Extract { fields } => {
                self.extract_fields(fields, data).await
            }
            DataTransformation::Rename { mapping } => {
                self.rename_fields(mapping, data).await
            }
            DataTransformation::Convert { field, target_type } => {
                self.convert_field_type(field, target_type, data).await
            }
            DataTransformation::Aggregate { operation } => {
                self.aggregate_data(operation, data).await
            }
            DataTransformation::Function { name, arguments } => {
                self.apply_function(name, arguments, data).await
            }
        }
    }
    
    /// Extract specific fields from data
    async fn extract_fields(
        &self,
        fields: &[String],
        data: serde_json::Value,
    ) -> Result<serde_json::Value, TransformError> {
        if let serde_json::Value::Object(obj) = data {
            let mut result = serde_json::Map::new();
            
            for field in fields {
                if let Some(value) = obj.get(field) {
                    result.insert(field.clone(), value.clone());
                }
            }
            
            Ok(serde_json::Value::Object(result))
        } else {
            Err(TransformError::InvalidDataType {
                expected: "object".to_string(),
                actual: data.to_string(),
            })
        }
    }
    
    /// Rename fields in the data
    async fn rename_fields(
        &self,
        mapping: &HashMap<String, String>,
        data: serde_json::Value,
    ) -> Result<serde_json::Value, TransformError> {
        if let serde_json::Value::Object(mut obj) = data {
            for (old_name, new_name) in mapping {
                if let Some(value) = obj.remove(old_name) {
                    obj.insert(new_name.clone(), value);
                }
            }
            
            Ok(serde_json::Value::Object(obj))
        } else {
            Err(TransformError::InvalidDataType {
                expected: "object".to_string(),
                actual: data.to_string(),
            })
        }
    }
    
    /// Convert field to target data type
    async fn convert_field_type(
        &self,
        field: &str,
        target_type: &DataType,
        data: serde_json::Value,
    ) -> Result<serde_json::Value, TransformError> {
        if let serde_json::Value::Object(mut obj) = data {
            if let Some(value) = obj.get(field) {
                let converted = match target_type {
                    DataType::String => serde_json::Value::String(value.to_string()),
                    DataType::Number => {
                        if let Some(s) = value.as_str() {
                            if let Ok(n) = s.parse::<f64>() {
                                serde_json::Value::Number(serde_json::Number::from_f64(n).unwrap())
                            } else {
                                return Err(TransformError::ConversionFailed {
                                    field: field.to_string(),
                                    value: value.clone(),
                                    target_type: format!("{:?}", target_type),
                                });
                            }
                        } else {
                            return Err(TransformError::ConversionFailed {
                                field: field.to_string(),
                                value: value.clone(),
                                target_type: format!("{:?}", target_type),
                            });
                        }
                    }
                    DataType::Boolean => {
                        if let Some(s) = value.as_str() {
                            serde_json::Value::Bool(s.parse().unwrap_or(false))
                        } else if let Some(b) = value.as_bool() {
                            serde_json::Value::Bool(b)
                        } else {
                            serde_json::Value::Bool(false)
                        }
                    }
                    _ => value.clone(),
                };
                
                obj.insert(field.to_string(), converted);
            }
            
            Ok(serde_json::Value::Object(obj))
        } else {
            Err(TransformError::InvalidDataType {
                expected: "object".to_string(),
                actual: data.to_string(),
            })
        }
    }
    
    /// Register built-in transformation functions
    fn register_builtin_transforms(&mut self) {
        // String transformations
        self.register_transform("uppercase", Box::new(|value| {
            if let Some(s) = value.as_str() {
                Ok(serde_json::Value::String(s.to_uppercase()))
            } else {
                Err(TransformError::InvalidInput)
            }
        }));
        
        self.register_transform("lowercase", Box::new(|value| {
            if let Some(s) = value.as_str() {
                Ok(serde_json::Value::String(s.to_lowercase()))
            } else {
                Err(TransformError::InvalidInput)
            }
        }));
        
        // Array transformations
        self.register_transform("array_length", Box::new(|value| {
            if let Some(arr) = value.as_array() {
                Ok(serde_json::Value::Number(
                    serde_json::Number::from(arr.len())
                ))
            } else {
                Err(TransformError::InvalidInput)
            }
        }));
        
        // Date/time transformations
        self.register_transform("format_timestamp", Box::new(|value| {
            if let Some(timestamp) = value.as_i64() {
                let datetime = chrono::DateTime::from_timestamp(timestamp, 0)
                    .ok_or(TransformError::InvalidInput)?;
                let formatted = datetime.format("%Y-%m-%d %H:%M:%S").to_string();
                Ok(serde_json::Value::String(formatted))
            } else {
                Err(TransformError::InvalidInput)
            }
        }));
    }
    
    pub fn register_transform(&mut self, name: &str, function: TransformFunction) {
        self.transform_functions
            .blocking_write()
            .insert(name.to_string(), function);
    }
}
```

### 4. Conditional Execution System

```rust
/// System for handling conditional execution in workflows
pub struct ConditionalExecutionEngine {
    /// Expression evaluator for conditions
    evaluator: Arc<ExpressionEvaluator>,
    
    /// Condition evaluation cache
    condition_cache: Arc<RwLock<HashMap<String, ConditionResult>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionCondition {
    pub id: String,
    pub expression: String,
    pub variables: HashMap<String, VariableSource>,
    pub fallback_value: Option<bool>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum VariableSource {
    StepOutput { step_id: String, path: String },
    WorkflowInput { path: String },
    Constant(serde_json::Value),
    SystemVariable(SystemVariable),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SystemVariable {
    CurrentTime,
    WorkflowId,
    StepId,
    ExecutionCount,
}

#[derive(Debug, Clone)]
pub struct ConditionResult {
    pub condition_id: String,
    pub result: bool,
    pub evaluated_at: Instant,
    pub variables_used: HashMap<String, serde_json::Value>,
}

impl ConditionalExecutionEngine {
    pub fn new() -> Self {
        Self {
            evaluator: Arc::new(ExpressionEvaluator::new()),
            condition_cache: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    /// Evaluate execution conditions for a step
    pub async fn evaluate_conditions(
        &self,
        conditions: &[ExecutionCondition],
        workflow_data: &WorkflowData,
    ) -> Result<bool, ConditionError> {
        // If no conditions, step should execute
        if conditions.is_empty() {
            return Ok(true);
        }
        
        // Evaluate all conditions (AND logic)
        for condition in conditions {
            let result = self.evaluate_single_condition(condition, workflow_data).await?;
            
            if !result {
                return Ok(false);
            }
        }
        
        Ok(true)
    }
    
    /// Evaluate a single condition
    async fn evaluate_single_condition(
        &self,
        condition: &ExecutionCondition,
        workflow_data: &WorkflowData,
    ) -> Result<bool, ConditionError> {
        // Check cache first
        if let Some(cached) = self.condition_cache.read().await.get(&condition.id) {
            // Use cached result if still valid (within 1 second)
            if cached.evaluated_at.elapsed() < Duration::from_secs(1) {
                return Ok(cached.result);
            }
        }
        
        // Resolve variables
        let variables = self.resolve_variables(&condition.variables, workflow_data).await?;
        
        // Evaluate expression
        let result = match self.evaluator.evaluate(&condition.expression, &variables).await {
            Ok(value) => {
                if let Some(boolean) = value.as_bool() {
                    boolean
                } else {
                    // Try to convert to boolean
                    match value {
                        serde_json::Value::Number(n) => n.as_f64().unwrap_or(0.0) != 0.0,
                        serde_json::Value::String(s) => !s.is_empty(),
                        serde_json::Value::Array(a) => !a.is_empty(),
                        serde_json::Value::Object(o) => !o.is_empty(),
                        serde_json::Value::Null => false,
                        _ => condition.fallback_value.unwrap_or(false),
                    }
                }
            }
            Err(_) => condition.fallback_value.unwrap_or(false),
        };
        
        // Cache result
        let condition_result = ConditionResult {
            condition_id: condition.id.clone(),
            result,
            evaluated_at: Instant::now(),
            variables_used: variables,
        };
        
        self.condition_cache
            .write()
            .await
            .insert(condition.id.clone(), condition_result);
        
        Ok(result)
    }
    
    /// Resolve variables for condition evaluation
    async fn resolve_variables(
        &self,
        variable_sources: &HashMap<String, VariableSource>,
        workflow_data: &WorkflowData,
    ) -> Result<HashMap<String, serde_json::Value>, ConditionError> {
        let mut variables = HashMap::new();
        
        for (name, source) in variable_sources {
            let value = match source {
                VariableSource::StepOutput { step_id, path } => {
                    workflow_data.get_step_output(step_id, path)?
                }
                VariableSource::WorkflowInput { path } => {
                    workflow_data.get_input(path)?
                }
                VariableSource::Constant(value) => value.clone(),
                VariableSource::SystemVariable(sys_var) => {
                    self.resolve_system_variable(sys_var, workflow_data).await?
                }
            };
            
            variables.insert(name.clone(), value);
        }
        
        Ok(variables)
    }
    
    /// Resolve system variables
    async fn resolve_system_variable(
        &self,
        sys_var: &SystemVariable,
        workflow_data: &WorkflowData,
    ) -> Result<serde_json::Value, ConditionError> {
        match sys_var {
            SystemVariable::CurrentTime => {
                let timestamp = chrono::Utc::now().timestamp();
                Ok(serde_json::Value::Number(serde_json::Number::from(timestamp)))
            }
            SystemVariable::WorkflowId => {
                Ok(serde_json::Value::String(workflow_data.workflow_id.to_string()))
            }
            SystemVariable::StepId => {
                Ok(serde_json::Value::String(
                    workflow_data.current_step_id.clone().unwrap_or_default()
                ))
            }
            SystemVariable::ExecutionCount => {
                Ok(serde_json::Value::Number(
                    serde_json::Number::from(workflow_data.execution_count)
                ))
            }
        }
    }
}

/// Expression evaluator for complex conditions
pub struct ExpressionEvaluator {
    /// CEL evaluator for complex expressions
    cel_evaluator: Arc<CelEvaluator>,
}

impl ExpressionEvaluator {
    pub fn new() -> Self {
        Self {
            cel_evaluator: Arc::new(CelEvaluator::new()),
        }
    }
    
    /// Evaluate an expression with given variables
    pub async fn evaluate(
        &self,
        expression: &str,
        variables: &HashMap<String, serde_json::Value>,
    ) -> Result<serde_json::Value, ExpressionError> {
        // Support simple boolean operators
        if expression.contains("&&") || expression.contains("||") {
            return self.evaluate_boolean_expression(expression, variables).await;
        }
        
        // Support comparison operators
        if expression.contains("==") || expression.contains("!=") 
           || expression.contains("<") || expression.contains(">") {
            return self.evaluate_comparison_expression(expression, variables).await;
        }
        
        // Use CEL for complex expressions
        self.cel_evaluator.evaluate(expression, variables).await
    }
    
    /// Evaluate simple boolean expressions
    async fn evaluate_boolean_expression(
        &self,
        expression: &str,
        variables: &HashMap<String, serde_json::Value>,
    ) -> Result<serde_json::Value, ExpressionError> {
        // Simple implementation for basic boolean logic
        // In production, use a proper expression parser
        
        if expression.contains("&&") {
            let parts: Vec<&str> = expression.split("&&").collect();
            for part in parts {
                let result = self.evaluate_simple_expression(part.trim(), variables).await?;
                if !result.as_bool().unwrap_or(false) {
                    return Ok(serde_json::Value::Bool(false));
                }
            }
            Ok(serde_json::Value::Bool(true))
        } else if expression.contains("||") {
            let parts: Vec<&str> = expression.split("||").collect();
            for part in parts {
                let result = self.evaluate_simple_expression(part.trim(), variables).await?;
                if result.as_bool().unwrap_or(false) {
                    return Ok(serde_json::Value::Bool(true));
                }
            }
            Ok(serde_json::Value::Bool(false))
        } else {
            self.evaluate_simple_expression(expression, variables).await
        }
    }
    
    /// Evaluate simple variable references and constants
    async fn evaluate_simple_expression(
        &self,
        expression: &str,
        variables: &HashMap<String, serde_json::Value>,
    ) -> Result<serde_json::Value, ExpressionError> {
        let expr = expression.trim();
        
        // Check if it's a variable reference
        if let Some(value) = variables.get(expr) {
            return Ok(value.clone());
        }
        
        // Check if it's a boolean constant
        match expr {
            "true" => Ok(serde_json::Value::Bool(true)),
            "false" => Ok(serde_json::Value::Bool(false)),
            _ => {
                // Try to parse as number
                if let Ok(n) = expr.parse::<f64>() {
                    Ok(serde_json::Value::Number(serde_json::Number::from_f64(n).unwrap()))
                } else {
                    // Treat as string literal
                    Ok(serde_json::Value::String(expr.to_string()))
                }
            }
        }
    }
}
```

### 5. Error Handling and Rollback System

```rust
/// Error handling strategies for workflows
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ErrorHandlingStrategy {
    /// Fail fast - stop on first error
    FailFast,
    /// Continue on error - skip failed steps
    ContinueOnError,
    /// Retry with backoff
    RetryWithBackoff {
        max_retries: usize,
        initial_delay: Duration,
        backoff_multiplier: f64,
    },
    /// Rollback on error
    RollbackOnError,
    /// Custom error handler
    Custom {
        handler_name: String,
        configuration: HashMap<String, serde_json::Value>,
    },
}

/// Rollback action for workflow steps
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RollbackAction {
    pub action_type: RollbackActionType,
    pub parameters: HashMap<String, serde_json::Value>,
    pub timeout: Duration,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RollbackActionType {
    /// Call a specific tool to undo the action
    Tool { name: String },
    /// Execute a custom rollback function
    Function { name: String },
    /// Restore from a snapshot
    RestoreSnapshot { snapshot_id: String },
    /// No rollback needed
    None,
}

/// Workflow error types
#[derive(Debug, Clone)]
pub enum WorkflowError {
    /// Validation errors
    InvalidDefinition(String),
    DependencyNotFound(String),
    CyclicDependency(Vec<String>),
    
    /// Execution errors
    InstanceNotFound,
    StepNotFound(String),
    StepExecutionFailed { step_id: String, error: Box<ToolError> },
    CriticalStepFailed(String),
    StepsFailed(Vec<String>),
    Deadlock,
    Timeout,
    
    /// Data errors
    DataTransformationFailed(String),
    ParameterResolutionFailed(String),
    ConditionEvaluationFailed(String),
    
    /// Rollback errors
    RollbackFailed { step_id: String, error: String },
    PartialRollback(Vec<String>),
}

/// Rollback coordinator for handling workflow failures
pub struct RollbackCoordinator {
    /// Tool registry for rollback operations
    tool_registry: Arc<ToolRegistry>,
    
    /// Rollback operation history
    rollback_history: Arc<RwLock<HashMap<Uuid, Vec<RollbackOperation>>>>,
    
    /// Snapshot manager
    snapshot_manager: Arc<SnapshotManager>,
}

#[derive(Debug, Clone)]
pub struct RollbackOperation {
    pub operation_id: Uuid,
    pub step_id: String,
    pub action: RollbackAction,
    pub status: RollbackStatus,
    pub started_at: Instant,
    pub completed_at: Option<Instant>,
    pub error: Option<String>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum RollbackStatus {
    Pending,
    InProgress,
    Completed,
    Failed,
    Skipped,
}

impl RollbackCoordinator {
    pub fn new(tool_registry: Arc<ToolRegistry>) -> Self {
        Self {
            tool_registry,
            rollback_history: Arc::new(RwLock::new(HashMap::new())),
            snapshot_manager: Arc::new(SnapshotManager::new()),
        }
    }
    
    /// Execute rollback for a failed workflow
    pub async fn rollback_workflow(
        &self,
        workflow_id: Uuid,
        failed_step: String,
        completed_steps: Vec<String>,
    ) -> Result<RollbackResult, WorkflowError> {
        let mut rollback_operations = Vec::new();
        
        // Create rollback operations for completed steps in reverse order
        for step_id in completed_steps.iter().rev() {
            if let Some(rollback_action) = self.get_rollback_action(step_id).await {
                let operation = RollbackOperation {
                    operation_id: Uuid::new_v4(),
                    step_id: step_id.clone(),
                    action: rollback_action,
                    status: RollbackStatus::Pending,
                    started_at: Instant::now(),
                    completed_at: None,
                    error: None,
                };
                
                rollback_operations.push(operation);
            }
        }
        
        // Store rollback history
        self.rollback_history
            .write()
            .await
            .insert(workflow_id, rollback_operations.clone());
        
        // Execute rollback operations
        let mut successful_rollbacks = 0;
        let mut failed_rollbacks = Vec::new();
        
        for mut operation in rollback_operations {
            operation.status = RollbackStatus::InProgress;
            
            match self.execute_rollback_operation(&operation).await {
                Ok(_) => {
                    operation.status = RollbackStatus::Completed;
                    operation.completed_at = Some(Instant::now());
                    successful_rollbacks += 1;
                }
                Err(error) => {
                    operation.status = RollbackStatus::Failed;
                    operation.error = Some(error.to_string());
                    failed_rollbacks.push(operation.step_id.clone());
                }
            }
        }
        
        Ok(RollbackResult {
            workflow_id,
            total_operations: rollback_operations.len(),
            successful_rollbacks,
            failed_rollbacks,
        })
    }
    
    /// Execute a single rollback operation
    async fn execute_rollback_operation(
        &self,
        operation: &RollbackOperation,
    ) -> Result<(), RollbackError> {
        match &operation.action.action_type {
            RollbackActionType::Tool { name } => {
                // Execute rollback tool
                let tool_call = ToolCall {
                    name: name.clone(),
                    arguments: operation.action.parameters.clone(),
                };
                
                tokio::time::timeout(operation.action.timeout, async {
                    self.tool_registry.execute_tool(tool_call).await
                })
                .await
                .map_err(|_| RollbackError::Timeout)?
                .map_err(|e| RollbackError::ToolError(e))?;
                
                Ok(())
            }
            RollbackActionType::Function { name } => {
                // Execute custom rollback function
                self.execute_rollback_function(name, &operation.action.parameters).await
            }
            RollbackActionType::RestoreSnapshot { snapshot_id } => {
                // Restore from snapshot
                self.snapshot_manager.restore_snapshot(snapshot_id).await
                    .map_err(|e| RollbackError::SnapshotError(e))
            }
            RollbackActionType::None => {
                // No rollback needed
                Ok(())
            }
        }
    }
}
```

### 6. Workflow Visualization and Monitoring

```rust
/// Workflow visualization and monitoring system
pub struct WorkflowMonitor {
    /// Active workflow instances
    workflow_instances: Arc<RwLock<HashMap<Uuid, WorkflowInstance>>>,
    
    /// Event stream for real-time updates
    event_stream: Arc<WorkflowEventStream>,
    
    /// Metrics collector
    metrics_collector: Arc<WorkflowMetricsCollector>,
    
    /// Visualization generator
    viz_generator: Arc<WorkflowVisualizationGenerator>,
}

#[derive(Debug, Clone, Serialize)]
pub struct WorkflowVisualization {
    pub workflow_id: Uuid,
    pub graph: WorkflowGraph,
    pub current_state: WorkflowState,
    pub execution_timeline: Vec<ExecutionEvent>,
    pub performance_metrics: WorkflowMetrics,
}

#[derive(Debug, Clone, Serialize)]
pub struct WorkflowGraph {
    pub nodes: Vec<WorkflowNode>,
    pub edges: Vec<WorkflowEdge>,
    pub layout: GraphLayout,
}

#[derive(Debug, Clone, Serialize)]
pub struct WorkflowNode {
    pub id: String,
    pub tool_name: String,
    pub status: StepStatus,
    pub position: Position,
    pub metadata: NodeMetadata,
}

#[derive(Debug, Clone, Serialize)]
pub struct WorkflowEdge {
    pub from: String,
    pub to: String,
    pub data_flow: DataFlowInfo,
    pub condition: Option<String>,
}

impl WorkflowMonitor {
    /// Get real-time workflow visualization
    pub async fn get_workflow_visualization(
        &self,
        workflow_id: Uuid,
    ) -> Result<WorkflowVisualization, MonitoringError> {
        let instance = self.workflow_instances
            .read()
            .await
            .get(&workflow_id)
            .cloned()
            .ok_or(MonitoringError::WorkflowNotFound)?;
        
        let graph = self.viz_generator
            .generate_workflow_graph(&instance.definition, &instance.step_results)
            .await?;
        
        let current_state = self.get_current_workflow_state(&instance).await?;
        let timeline = self.get_execution_timeline(workflow_id).await?;
        let metrics = self.metrics_collector.get_workflow_metrics(workflow_id).await?;
        
        Ok(WorkflowVisualization {
            workflow_id,
            graph,
            current_state,
            execution_timeline: timeline,
            performance_metrics: metrics,
        })
    }
    
    /// Stream workflow events for real-time updates
    pub async fn stream_workflow_events(
        &self,
        workflow_id: Uuid,
    ) -> impl Stream<Item = WorkflowEvent> {
        self.event_stream.subscribe(workflow_id).await
    }
}
```

## Architecture Changes

### Current Architecture
```
┌─────────────────┐
│   Tool System   │
├─────────────────┤
│ Individual Exec │
│ Manual Chains   │
│ No Automation   │
└─────────────────┘
```

### Target Architecture
```
┌─────────────────────────────────────────────────────────┐
│              Workflow Orchestration System              │
├─────────────────────────────────────────────────────────┤
│ ┌─────────────┐ ┌─────────────┐ ┌─────────────────────┐│
│ │  Workflow   │ │    Data     │ │   Conditional       ││
│ │  Executor   │ │Transformation│ │   Execution         ││
│ └─────────────┘ └─────────────┘ └─────────────────────┘│
├─────────────────────────────────────────────────────────┤
│ ┌─────────────┐ ┌─────────────┐ ┌─────────────────────┐│
│ │   Error     │ │  Rollback   │ │    Monitoring       ││
│ │  Handling   │ │Coordinator  │ │ & Visualization     ││
│ └─────────────┘ └─────────────┘ └─────────────────────┘│
└─────────────────────────────────────────────────────────┘
```

## Implementation Plan

### Phase 1: Core Workflow Engine (Week 1-2)
1. Implement `WorkflowDefinition` and core data structures
2. Create `WorkflowExecutor` with basic execution logic
3. Build dependency graph and execution planning
4. Set up workflow instance management

### Phase 2: Data Transformation (Week 2-3)
1. Implement `DataTransformationEngine`
2. Create built-in transformation functions
3. Build parameter resolution system
4. Add template and expression support

### Phase 3: Conditional Execution (Week 3-4)
1. Implement `ConditionalExecutionEngine`
2. Create expression evaluator
3. Build variable resolution system
4. Add condition caching

### Phase 4: Error Handling & Rollback (Week 4-5)
1. Implement rollback coordinator
2. Create error handling strategies
3. Build snapshot management
4. Add recovery mechanisms

### Phase 5: Visualization & Monitoring (Week 5-6)
1. Implement workflow monitoring
2. Create visualization system
3. Build real-time event streaming
4. Add performance metrics

## Testing Strategy

### Unit Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_workflow_execution_basic() {
        let executor = create_test_executor().await;
        
        let workflow = WorkflowDefinition {
            id: Uuid::new_v4(),
            name: "test_workflow".to_string(),
            description: None,
            version: "1.0".to_string(),
            steps: vec![
                WorkflowStep {
                    id: "step1".to_string(),
                    tool_name: "read_file".to_string(),
                    parameters: ParameterMapping::Static(json!({
                        "path": "test.txt"
                    })),
                    conditions: vec![],
                    timeout: None,
                    retry_policy: None,
                    rollback_action: None,
                },
                WorkflowStep {
                    id: "step2".to_string(),
                    tool_name: "process_text".to_string(),
                    parameters: ParameterMapping::FromStep {
                        step_id: "step1".to_string(),
                        output_path: "content".to_string(),
                    },
                    conditions: vec![],
                    timeout: None,
                    retry_policy: None,
                    rollback_action: None,
                },
            ],
            connections: vec![
                StepConnection {
                    from_step: "step1".to_string(),
                    to_step: "step2".to_string(),
                    data_mapping: DataMapping {
                        transformations: vec![],
                        field_mapping: HashMap::new(),
                        filters: vec![],
                    },
                    condition: None,
                },
            ],
            error_handling: ErrorHandlingStrategy::FailFast,
            timeout: Duration::from_secs(300),
            retry_policy: WorkflowRetryPolicy::default(),
        };
        
        let inputs = WorkflowInputs::new();
        let handle = executor.execute_workflow(workflow, inputs).await.unwrap();
        let result = handle.wait().await.unwrap();
        
        assert!(result.status == WorkflowStatus::Completed);
        assert_eq!(result.step_results.len(), 2);
    }
    
    #[tokio::test]
    async fn test_data_transformation() {
        let engine = DataTransformationEngine::new();
        
        let input_data = json!({
            "name": "john doe",
            "age": "25",
            "active": "true"
        });
        
        let transformations = vec![
            DataTransformation::Convert {
                field: "age".to_string(),
                target_type: DataType::Number,
            },
            DataTransformation::Convert {
                field: "active".to_string(),
                target_type: DataType::Boolean,
            },
            DataTransformation::Function {
                name: "uppercase".to_string(),
                arguments: HashMap::new(),
            },
        ];
        
        let mapping = DataMapping {
            transformations,
            field_mapping: HashMap::new(),
            filters: vec![],
        };
        
        let result = engine.apply_data_mapping(&mapping, input_data).await.unwrap();
        
        assert_eq!(result["name"], "JOHN DOE");
        assert_eq!(result["age"], 25);
        assert_eq!(result["active"], true);
    }
    
    #[tokio::test]
    async fn test_conditional_execution() {
        let engine = ConditionalExecutionEngine::new();
        
        let condition = ExecutionCondition {
            id: "test_condition".to_string(),
            expression: "age > 18 && active == true".to_string(),
            variables: HashMap::from([
                ("age".to_string(), VariableSource::Constant(json!(25))),
                ("active".to_string(), VariableSource::Constant(json!(true))),
            ]),
            fallback_value: Some(false),
        };
        
        let workflow_data = WorkflowData::new(WorkflowInputs::new());
        let result = engine
            .evaluate_conditions(&[condition], &workflow_data)
            .await
            .unwrap();
        
        assert!(result);
    }
    
    #[tokio::test]
    async fn test_rollback_mechanism() {
        let coordinator = create_test_rollback_coordinator().await;
        
        let workflow_id = Uuid::new_v4();
        let completed_steps = vec!["step1".to_string(), "step2".to_string()];
        
        let result = coordinator
            .rollback_workflow(workflow_id, "step3".to_string(), completed_steps)
            .await
            .unwrap();
        
        assert_eq!(result.total_operations, 2);
        assert_eq!(result.successful_rollbacks, 2);
        assert!(result.failed_rollbacks.is_empty());
    }
}
```

### Integration Tests

```rust
#[tokio::test]
async fn test_complex_workflow_with_conditions() {
    let executor = create_test_executor().await;
    
    let workflow = create_complex_workflow_definition();
    let inputs = WorkflowInputs::from_json(json!({
        "user_id": 123,
        "process_all": true
    }));
    
    let handle = executor.execute_workflow(workflow, inputs).await.unwrap();
    
    // Monitor progress
    let mut progress_stream = handle.progress_stream().await;
    let mut events = Vec::new();
    
    tokio::spawn(async move {
        while let Some(event) = progress_stream.next().await {
            events.push(event);
        }
    });
    
    let result = handle.wait().await.unwrap();
    
    assert!(result.status == WorkflowStatus::Completed);
    assert!(events.len() > 0);
}

#[tokio::test]
async fn test_workflow_error_and_rollback() {
    let executor = create_test_executor().await;
    
    // Create workflow that will fail
    let workflow = create_failing_workflow_definition();
    let inputs = WorkflowInputs::new();
    
    let handle = executor.execute_workflow(workflow, inputs).await.unwrap();
    let result = handle.wait().await;
    
    // Should fail and trigger rollback
    assert!(result.is_err());
    
    // Verify rollback occurred
    let rollback_history = executor.get_rollback_history(handle.workflow_id).await;
    assert!(rollback_history.is_some());
}
```

### Performance Tests

```rust
#[tokio::test]
async fn benchmark_workflow_execution() {
    let executor = create_test_executor().await;
    
    // Create workflow with many parallel steps
    let workflow = create_large_parallel_workflow(100);
    let inputs = WorkflowInputs::new();
    
    let start = Instant::now();
    let handle = executor.execute_workflow(workflow, inputs).await.unwrap();
    let result = handle.wait().await.unwrap();
    let duration = start.elapsed();
    
    println!("Executed 100-step workflow in {:?}", duration);
    
    assert!(result.status == WorkflowStatus::Completed);
    assert!(duration < Duration::from_secs(30)); // Should complete in reasonable time
}
```

## Dependencies & Integration

### Direct Dependencies
- **Issue 2.1**: Parallel Tool Execution System
  - Required for concurrent step execution within workflows
  - Provides resource management and scheduling

### Integration Points
- **Tool System**: Extends with workflow capabilities
- **Streaming System**: Real-time workflow progress updates
- **Configuration System**: TOML-based workflow definitions
- **Error Handling**: Enhanced error recovery and rollback

### API Changes
- New workflow management endpoints
- Enhanced tool execution with data flow
- Streaming workflow events and progress

## Security Considerations

### Workflow Validation
- Comprehensive validation of workflow definitions
- Prevention of malicious workflow patterns
- Resource limits and timeout enforcement

### Data Flow Security
- Secure data transformation between steps
- Input validation and sanitization
- Preventing data leakage between workflows

### Expression Evaluation
- Sandboxed expression evaluation
- Prevention of code injection attacks
- Limited function access in expressions

## Acceptance Criteria

1. **Workflow Definition**
   - [ ] Support declarative workflow definitions in TOML/JSON
   - [ ] Validate workflow structure and dependencies
   - [ ] Handle complex DAG workflows with branches

2. **Data Flow**
   - [ ] Automatic data transformation between steps
   - [ ] Template-based parameter mapping
   - [ ] Type conversion and validation

3. **Conditional Execution**
   - [ ] Expression-based execution conditions
   - [ ] Variable resolution from multiple sources
   - [ ] Condition caching for performance

4. **Error Handling**
   - [ ] Multiple error handling strategies
   - [ ] Automatic rollback on failure
   - [ ] Partial rollback with recovery

5. **Performance**
   - [ ] Execute workflows faster than sequential tool calls
   - [ ] Handle workflows with 50+ steps
   - [ ] Real-time progress monitoring

6. **Integration**
   - [ ] Works with parallel execution system
   - [ ] Integrates with streaming updates
   - [ ] TOML configuration support

## Quality Control

### 1. Unit Tests (500 LOC)

#### Core Workflow Engine Tests
- **Workflow Definition Validation**
  - Schema validation for workflow definitions
  - Dependency cycle detection
  - Parameter mapping validation
  - Error handling strategy validation

- **Execution Engine Tests**
  - Basic workflow execution flow
  - Dependency resolution ordering
  - Step result propagation
  - Timeout and cancellation handling

- **Data Transformation Tests**
  - Field extraction and renaming
  - Type conversion accuracy
  - Template parameter substitution
  - Custom transformation functions

#### Conditional Execution Tests
- **Expression Evaluation**
  - Boolean logic operators (&&, ||, !)
  - Comparison operators (==, !=, <, >)
  - Variable resolution from multiple sources
  - Complex nested expressions

- **Condition Caching**
  - Cache hit/miss performance
  - Cache invalidation logic
  - Variable dependency tracking
  - Memory usage optimization

### 2. Integration Tests (400 LOC)

#### End-to-End Workflow Tests
- **Simple Linear Workflows**
  - Sequential step execution
  - Data flow between steps
  - Error propagation and handling
  - Completion status verification

- **Complex DAG Workflows**
  - Parallel branch execution
  - Conditional step execution
  - Dynamic workflow paths
  - Resource sharing between branches

- **Error Handling and Rollback**
  - Automatic rollback on failure
  - Partial rollback scenarios
  - Error recovery mechanisms
  - Rollback operation verification

### 3. Performance Tests (300 LOC)

#### Workflow Execution Performance
- **Large Workflow Benchmarks**
  - 50+ step workflow execution
  - Parallel vs sequential performance
  - Memory usage during execution
  - Resource utilization efficiency

- **Data Transformation Performance**
  - Large dataset transformation
  - Complex expression evaluation
  - Template rendering performance
  - Transformation pipeline efficiency

### 4. Manual Testing

#### User Experience Testing
- **Workflow Definition**
  - TOML/JSON workflow definition ease
  - Validation error clarity
  - Documentation completeness
  - IDE integration support

- **Monitoring and Visualization**
  - Real-time progress updates
  - Workflow graph visualization
  - Error reporting clarity
  - Performance metrics accuracy

## Documentation Requirements

### 1. Architecture Documentation
- **Workflow Engine Design**
  - Execution flow architecture
  - Data transformation pipeline
  - Conditional execution system
  - Error handling and rollback mechanisms

- **Integration Architecture**
  - Parallel execution system integration
  - Tool system extension points
  - Streaming system integration
  - Configuration system integration

### 2. API Documentation
- **Workflow Definition Schema**
  - Complete TOML/JSON schema
  - Parameter mapping options
  - Condition expression syntax
  - Error handling strategies

- **Execution API**
  - Workflow submission endpoints
  - Progress monitoring APIs
  - Cancellation and control
  - Result retrieval methods

### 3. User Guides
- **Workflow Creation Guide**
  - Step-by-step workflow creation
  - Best practices for workflow design
  - Common patterns and examples
  - Troubleshooting guide

- **Expression and Transformation Guide**
  - Condition expression syntax
  - Data transformation examples
  - Template usage patterns
  - Custom function development

### 4. Integration Documentation
- **Tool Developer Guide**
  - Making tools workflow-compatible
  - Rollback action implementation
  - Data format requirements
  - Performance considerations

## Dependencies

### 1. Foundational Dependencies
- **Issue 1.1**: Enhanced ContentBlock System
  - Provides structured tool communication
  - Required for data flow between steps

- **Issue 1.2**: Unified Error Handling Framework
  - Provides robust error types and context
  - Required for workflow error management

- **Issue 1.3**: Tool Result Handling and Feedback Loop System
  - Provides `ToolExecutionResult` types
  - Required for step result processing

### 2. Direct Dependencies
- **Issue 2.1**: Parallel Tool Execution System
  - **REQUIRED** - Provides concurrent execution engine
  - Used for parallel step execution within workflows
  - Resource management and scheduling capabilities

### 3. System Integration Dependencies
- **Issue 1.4**: Basic Streaming Foundation
  - Provides SSE infrastructure for real-time updates
  - Required for workflow progress streaming

- **Issue 1.5**: Configuration System Foundation (if exists)
  - Provides TOML configuration support
  - Required for workflow definition storage

### 4. Tool System Dependencies
- **Existing Tool Infrastructure**
  - `AgentTool` trait definition
  - Tool registration and discovery
  - Tool execution capabilities
  - Result format standardization

### 5. External Dependencies
```toml
# Required crate additions to Cargo.toml
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1.0", features = ["full"] }
futures = "0.3"
uuid = { version = "1.0", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
cel-parser = "0.3"  # For expression evaluation
jsonpath-lib = "0.3"  # For JSON path operations
handlebars = "4.0"  # For template rendering
```

## References

### 1. Technical Resources
- [Workflow Patterns](http://www.workflowpatterns.com/)
- [BPMN 2.0 Specification](https://www.bpmn.org/)
- [Common Expression Language (CEL)](https://github.com/google/cel-spec)
- [JSON Path Specification](https://goessner.net/articles/JsonPath/)
- [Handlebars Template Engine](https://handlebarsjs.com/)

### 2. Anthropic Documentation
- [Claude API - Tool Use](https://docs.anthropic.com/claude/docs/tool-use)
- [Tool Use Best Practices](https://docs.anthropic.com/en/docs/tool-use)

### 3. GitHub Issues
- [Issue 2.1: Parallel Tool Execution System](./2.1-parallel-tool-execution.md)
- [Issue 1.3: Tool Result Handling System](./1.3-tool-result-handling-feedback.md)
- [Implementation Sequencing Guide](../implementation-sequencing.md)

### 4. Internal Documentation
- [Architecture Overview](../architecture/overview.md)
- [Tool System Documentation](../tools/overview.md)
- [Workflow Patterns Guide](../workflows/patterns.md)
- [Performance Guidelines](../development/performance.md)

## Estimated Lines of Code

**Implementation: ~1,500 LOC**
- Workflow definition and execution engine: ~500 LOC
- Data transformation system: ~300 LOC
- Conditional execution engine: ~250 LOC
- Error handling and rollback: ~200 LOC
- Workflow monitoring and visualization: ~150 LOC
- Integration with parallel execution: ~100 LOC

**Testing: ~1,200 LOC**
- Unit tests: ~500 LOC
- Integration tests: ~400 LOC
- Performance tests: ~300 LOC

**Total: ~2,700 LOC**

This comprehensive tool chaining and orchestration system enables complex multi-step workflows with advanced data transformation, conditional execution, and robust error handling capabilities.