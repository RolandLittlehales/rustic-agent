# [2.1] Core Features: Parallel Tool Execution System

## Overview

### Current State
- Sequential tool execution with blocking operations
- No concurrent tool handling capabilities
- Limited throughput for multi-tool operations
- No resource management or scheduling

### Target State
- Concurrent tool execution with dependency management
- Resource allocation and scheduling system
- Real-time progress tracking with streaming updates
- Integration with enhanced tool orchestrator
- Efficient handling of parallel tool workflows

### Why This Matters
- **Performance**: Execute independent tools concurrently for faster results
- **User Experience**: Real-time progress updates for long-running operations
- **Scalability**: Handle complex multi-tool workflows efficiently
- **Resource Management**: Prevent system overload with proper scheduling

## Technical Requirements

### 1. Parallel Execution Engine

```rust
use std::collections::{HashMap, HashSet, VecDeque};
use std::sync::Arc;
use tokio::sync::{Mutex, RwLock, Semaphore, mpsc};
use tokio::task::JoinSet;
use futures::future::BoxFuture;
use uuid::Uuid;

/// Tool execution context with resource tracking
#[derive(Debug, Clone)]
pub struct ToolExecutionContext {
    pub id: Uuid,
    pub tool_name: String,
    pub priority: ExecutionPriority,
    pub resource_requirements: ResourceRequirements,
    pub dependencies: HashSet<Uuid>,
    pub timeout: Duration,
    pub retry_policy: RetryPolicy,
}

/// Resource requirements for tool execution
#[derive(Debug, Clone)]
pub struct ResourceRequirements {
    pub cpu_weight: f32,        // 0.0 to 1.0
    pub memory_mb: usize,       // Estimated memory usage
    pub io_intensity: IOIntensity,
    pub network_required: bool,
    pub exclusive_resources: HashSet<String>, // Named exclusive resources
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum IOIntensity {
    Low,
    Medium,
    High,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub enum ExecutionPriority {
    Critical = 0,
    High = 1,
    Normal = 2,
    Low = 3,
    Background = 4,
}

/// Parallel tool execution engine
pub struct ParallelExecutionEngine {
    /// Maximum concurrent executions
    max_concurrency: usize,
    
    /// Resource allocation manager
    resource_manager: Arc<RwLock<ResourceManager>>,
    
    /// Execution queue with priority ordering
    execution_queue: Arc<Mutex<PriorityQueue<ToolExecutionContext>>>,
    
    /// Active executions
    active_executions: Arc<RwLock<HashMap<Uuid, ExecutionHandle>>>,
    
    /// Dependency graph
    dependency_graph: Arc<RwLock<DependencyGraph>>,
    
    /// Progress tracking
    progress_tracker: Arc<ProgressTracker>,
    
    /// Execution semaphore for concurrency control
    execution_semaphore: Arc<Semaphore>,
}

impl ParallelExecutionEngine {
    pub fn new(config: ParallelExecutionConfig) -> Self {
        Self {
            max_concurrency: config.max_concurrency,
            resource_manager: Arc::new(RwLock::new(ResourceManager::new(config.resource_limits))),
            execution_queue: Arc::new(Mutex::new(PriorityQueue::new())),
            active_executions: Arc::new(RwLock::new(HashMap::new())),
            dependency_graph: Arc::new(RwLock::new(DependencyGraph::new())),
            progress_tracker: Arc::new(ProgressTracker::new()),
            execution_semaphore: Arc::new(Semaphore::new(config.max_concurrency)),
        }
    }
    
    /// Submit a tool for execution
    pub async fn submit_tool<F, T>(
        &self,
        context: ToolExecutionContext,
        tool_fn: F,
    ) -> Result<ExecutionHandle, ParallelExecutionError>
    where
        F: FnOnce() -> BoxFuture<'static, Result<T, ToolError>> + Send + 'static,
        T: Send + 'static,
    {
        // Validate dependencies
        self.validate_dependencies(&context).await?;
        
        // Add to dependency graph
        self.dependency_graph
            .write()
            .await
            .add_node(context.id, context.dependencies.clone());
        
        // Create execution handle
        let handle = ExecutionHandle::new(context.id);
        
        // Queue the execution
        self.execution_queue
            .lock()
            .await
            .push(context.clone(), handle.clone());
        
        // Start execution scheduler if not running
        self.schedule_executions().await;
        
        Ok(handle)
    }
    
    /// Schedule and execute tools based on resources and dependencies
    async fn schedule_executions(&self) {
        let engine = self.clone();
        
        tokio::spawn(async move {
            loop {
                // Check for ready executions
                let ready_executions = engine.get_ready_executions().await;
                
                if ready_executions.is_empty() {
                    // No executions ready, wait a bit
                    tokio::time::sleep(Duration::from_millis(100)).await;
                    continue;
                }
                
                // Try to execute ready tools
                for context in ready_executions {
                    if let Ok(permit) = engine.execution_semaphore.clone().try_acquire_owned() {
                        // Check resource availability
                        if engine.try_allocate_resources(&context).await {
                            // Execute the tool
                            engine.execute_tool(context, permit).await;
                        } else {
                            // Resources not available, requeue
                            engine.requeue_execution(context).await;
                        }
                    } else {
                        // No execution slots available, requeue
                        engine.requeue_execution(context).await;
                    }
                }
            }
        });
    }
    
    /// Execute a tool with resource tracking
    async fn execute_tool(
        &self,
        context: ToolExecutionContext,
        permit: tokio::sync::OwnedSemaphorePermit,
    ) {
        let id = context.id;
        let engine = self.clone();
        
        tokio::spawn(async move {
            // Update progress
            engine.progress_tracker.start_execution(id).await;
            
            // Get the tool function from the queue
            let tool_result = match engine.get_tool_function(id).await {
                Some(tool_fn) => {
                    // Execute with timeout
                    match tokio::time::timeout(context.timeout, tool_fn()).await {
                        Ok(result) => result,
                        Err(_) => Err(ToolError::Timeout),
                    }
                }
                None => Err(ToolError::NotFound),
            };
            
            // Handle result
            match tool_result {
                Ok(result) => {
                    engine.handle_success(id, result).await;
                }
                Err(error) => {
                    engine.handle_failure(id, error, context.retry_policy).await;
                }
            }
            
            // Release resources
            engine.release_resources(&context).await;
            
            // Update dependency graph
            engine.dependency_graph.write().await.mark_completed(id);
            
            // Update progress
            engine.progress_tracker.complete_execution(id).await;
            
            // Release permit
            drop(permit);
        });
    }
}
```

### 2. Resource Management System

```rust
/// Resource manager for controlling system resource allocation
pub struct ResourceManager {
    /// Available CPU capacity (0.0 to 1.0)
    available_cpu: f32,
    
    /// Available memory in MB
    available_memory: usize,
    
    /// IO semaphore for controlling disk/network operations
    io_semaphore: Arc<Semaphore>,
    
    /// Exclusive resource locks
    exclusive_resources: HashMap<String, Option<Uuid>>,
    
    /// Resource allocation history
    allocation_history: VecDeque<ResourceAllocation>,
}

#[derive(Debug, Clone)]
struct ResourceAllocation {
    execution_id: Uuid,
    allocated_at: Instant,
    cpu: f32,
    memory: usize,
    exclusive_resources: HashSet<String>,
}

impl ResourceManager {
    pub fn new(limits: ResourceLimits) -> Self {
        Self {
            available_cpu: limits.max_cpu,
            available_memory: limits.max_memory_mb,
            io_semaphore: Arc::new(Semaphore::new(limits.max_io_operations)),
            exclusive_resources: HashMap::new(),
            allocation_history: VecDeque::with_capacity(1000),
        }
    }
    
    /// Try to allocate resources for a tool
    pub fn try_allocate(
        &mut self,
        execution_id: Uuid,
        requirements: &ResourceRequirements,
    ) -> Result<ResourceAllocation, ResourceError> {
        // Check CPU availability
        if self.available_cpu < requirements.cpu_weight {
            return Err(ResourceError::InsufficientCPU);
        }
        
        // Check memory availability
        if self.available_memory < requirements.memory_mb {
            return Err(ResourceError::InsufficientMemory);
        }
        
        // Check exclusive resources
        for resource in &requirements.exclusive_resources {
            if let Some(Some(owner)) = self.exclusive_resources.get(resource) {
                if *owner != execution_id {
                    return Err(ResourceError::ExclusiveResourceLocked(resource.clone()));
                }
            }
        }
        
        // Allocate resources
        self.available_cpu -= requirements.cpu_weight;
        self.available_memory -= requirements.memory_mb;
        
        // Lock exclusive resources
        for resource in &requirements.exclusive_resources {
            self.exclusive_resources.insert(resource.clone(), Some(execution_id));
        }
        
        // Record allocation
        let allocation = ResourceAllocation {
            execution_id,
            allocated_at: Instant::now(),
            cpu: requirements.cpu_weight,
            memory: requirements.memory_mb,
            exclusive_resources: requirements.exclusive_resources.clone(),
        };
        
        self.allocation_history.push_back(allocation.clone());
        
        // Trim history
        while self.allocation_history.len() > 1000 {
            self.allocation_history.pop_front();
        }
        
        Ok(allocation)
    }
    
    /// Release allocated resources
    pub fn release(&mut self, allocation: ResourceAllocation) {
        self.available_cpu += allocation.cpu;
        self.available_memory += allocation.memory;
        
        // Release exclusive resources
        for resource in allocation.exclusive_resources {
            self.exclusive_resources.remove(&resource);
        }
    }
    
    /// Get current resource utilization
    pub fn get_utilization(&self) -> ResourceUtilization {
        ResourceUtilization {
            cpu_usage: 1.0 - self.available_cpu,
            memory_usage_mb: self.total_memory - self.available_memory,
            active_allocations: self.allocation_history.len(),
        }
    }
}
```

### 3. Dependency Graph Management

```rust
/// Dependency graph for tracking tool execution dependencies
pub struct DependencyGraph {
    /// Adjacency list representation
    nodes: HashMap<Uuid, NodeInfo>,
    
    /// Reverse dependencies (who depends on me)
    reverse_deps: HashMap<Uuid, HashSet<Uuid>>,
    
    /// Completion status
    completed: HashSet<Uuid>,
}

#[derive(Debug, Clone)]
struct NodeInfo {
    dependencies: HashSet<Uuid>,
    submitted_at: Instant,
    completed_at: Option<Instant>,
    status: ExecutionStatus,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ExecutionStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Cancelled,
}

impl DependencyGraph {
    pub fn new() -> Self {
        Self {
            nodes: HashMap::new(),
            reverse_deps: HashMap::new(),
            completed: HashSet::new(),
        }
    }
    
    /// Add a node to the graph
    pub fn add_node(&mut self, id: Uuid, dependencies: HashSet<Uuid>) {
        // Validate dependencies exist
        for dep in &dependencies {
            if !self.nodes.contains_key(dep) {
                // Handle missing dependency
                continue;
            }
        }
        
        // Add node
        self.nodes.insert(id, NodeInfo {
            dependencies: dependencies.clone(),
            submitted_at: Instant::now(),
            completed_at: None,
            status: ExecutionStatus::Pending,
        });
        
        // Update reverse dependencies
        for dep in dependencies {
            self.reverse_deps
                .entry(dep)
                .or_insert_with(HashSet::new)
                .insert(id);
        }
    }
    
    /// Check if a node's dependencies are satisfied
    pub fn dependencies_satisfied(&self, id: &Uuid) -> bool {
        if let Some(node) = self.nodes.get(id) {
            node.dependencies.iter().all(|dep| self.completed.contains(dep))
        } else {
            false
        }
    }
    
    /// Mark a node as completed
    pub fn mark_completed(&mut self, id: Uuid) {
        self.completed.insert(id);
        
        if let Some(node) = self.nodes.get_mut(&id) {
            node.completed_at = Some(Instant::now());
            node.status = ExecutionStatus::Completed;
        }
    }
    
    /// Get nodes ready for execution
    pub fn get_ready_nodes(&self) -> Vec<Uuid> {
        self.nodes
            .iter()
            .filter(|(id, node)| {
                node.status == ExecutionStatus::Pending
                    && self.dependencies_satisfied(id)
            })
            .map(|(id, _)| *id)
            .collect()
    }
    
    /// Detect cycles in the dependency graph
    pub fn detect_cycles(&self) -> Option<Vec<Uuid>> {
        let mut visited = HashSet::new();
        let mut rec_stack = HashSet::new();
        let mut path = Vec::new();
        
        for node in self.nodes.keys() {
            if !visited.contains(node) {
                if let Some(cycle) = self.dfs_cycle_detect(
                    *node,
                    &mut visited,
                    &mut rec_stack,
                    &mut path,
                ) {
                    return Some(cycle);
                }
            }
        }
        
        None
    }
    
    fn dfs_cycle_detect(
        &self,
        node: Uuid,
        visited: &mut HashSet<Uuid>,
        rec_stack: &mut HashSet<Uuid>,
        path: &mut Vec<Uuid>,
    ) -> Option<Vec<Uuid>> {
        visited.insert(node);
        rec_stack.insert(node);
        path.push(node);
        
        if let Some(node_info) = self.nodes.get(&node) {
            for &dep in &node_info.dependencies {
                if !visited.contains(&dep) {
                    if let Some(cycle) = self.dfs_cycle_detect(dep, visited, rec_stack, path) {
                        return Some(cycle);
                    }
                } else if rec_stack.contains(&dep) {
                    // Found cycle
                    let cycle_start = path.iter().position(|&n| n == dep).unwrap();
                    return Some(path[cycle_start..].to_vec());
                }
            }
        }
        
        rec_stack.remove(&node);
        path.pop();
        None
    }
}
```

### 4. Progress Tracking System

```rust
use tokio::sync::broadcast;

/// Progress tracking for parallel executions
pub struct ProgressTracker {
    /// Execution progress states
    progress_states: Arc<RwLock<HashMap<Uuid, ExecutionProgress>>>,
    
    /// Progress update channel
    progress_tx: broadcast::Sender<ProgressUpdate>,
    
    /// Aggregated statistics
    statistics: Arc<RwLock<ExecutionStatistics>>,
}

#[derive(Debug, Clone)]
pub struct ExecutionProgress {
    pub id: Uuid,
    pub tool_name: String,
    pub status: ExecutionStatus,
    pub started_at: Option<Instant>,
    pub completed_at: Option<Instant>,
    pub progress_percentage: f32,
    pub current_step: Option<String>,
    pub error: Option<String>,
}

#[derive(Debug, Clone)]
pub struct ProgressUpdate {
    pub execution_id: Uuid,
    pub update_type: ProgressUpdateType,
    pub timestamp: Instant,
    pub details: Option<String>,
}

#[derive(Debug, Clone)]
pub enum ProgressUpdateType {
    Started,
    Progress(f32),
    StepChanged(String),
    Completed,
    Failed(String),
    Cancelled,
}

#[derive(Debug, Default)]
pub struct ExecutionStatistics {
    pub total_submitted: usize,
    pub currently_running: usize,
    pub completed_successfully: usize,
    pub failed: usize,
    pub cancelled: usize,
    pub average_execution_time: Duration,
    pub resource_utilization: ResourceUtilization,
}

impl ProgressTracker {
    pub fn new() -> Self {
        let (tx, _) = broadcast::channel(1000);
        
        Self {
            progress_states: Arc::new(RwLock::new(HashMap::new())),
            progress_tx: tx,
            statistics: Arc::new(RwLock::new(ExecutionStatistics::default())),
        }
    }
    
    /// Subscribe to progress updates
    pub fn subscribe(&self) -> broadcast::Receiver<ProgressUpdate> {
        self.progress_tx.subscribe()
    }
    
    /// Start tracking an execution
    pub async fn start_execution(&self, id: Uuid, tool_name: String) {
        let progress = ExecutionProgress {
            id,
            tool_name,
            status: ExecutionStatus::Running,
            started_at: Some(Instant::now()),
            completed_at: None,
            progress_percentage: 0.0,
            current_step: None,
            error: None,
        };
        
        self.progress_states.write().await.insert(id, progress);
        
        // Update statistics
        let mut stats = self.statistics.write().await;
        stats.total_submitted += 1;
        stats.currently_running += 1;
        
        // Send update
        let _ = self.progress_tx.send(ProgressUpdate {
            execution_id: id,
            update_type: ProgressUpdateType::Started,
            timestamp: Instant::now(),
            details: None,
        });
    }
    
    /// Update execution progress
    pub async fn update_progress(
        &self,
        id: Uuid,
        percentage: f32,
        current_step: Option<String>,
    ) {
        if let Some(progress) = self.progress_states.write().await.get_mut(&id) {
            progress.progress_percentage = percentage;
            progress.current_step = current_step.clone();
            
            let _ = self.progress_tx.send(ProgressUpdate {
                execution_id: id,
                update_type: ProgressUpdateType::Progress(percentage),
                timestamp: Instant::now(),
                details: current_step,
            });
        }
    }
    
    /// Get aggregated progress for all executions
    pub async fn get_aggregated_progress(&self) -> AggregatedProgress {
        let states = self.progress_states.read().await;
        let stats = self.statistics.read().await;
        
        let total_progress = if states.is_empty() {
            0.0
        } else {
            states.values().map(|p| p.progress_percentage).sum::<f32>() / states.len() as f32
        };
        
        AggregatedProgress {
            total_executions: states.len(),
            running: states.values().filter(|p| p.status == ExecutionStatus::Running).count(),
            completed: states.values().filter(|p| p.status == ExecutionStatus::Completed).count(),
            failed: states.values().filter(|p| p.status == ExecutionStatus::Failed).count(),
            overall_progress: total_progress,
            statistics: stats.clone(),
        }
    }
}
```

### 5. Execution Handle and Result Management

```rust
/// Handle for tracking and controlling tool execution
#[derive(Clone)]
pub struct ExecutionHandle {
    pub id: Uuid,
    result_rx: Arc<Mutex<oneshot::Receiver<ToolResult>>>,
    cancel_tx: Arc<Mutex<Option<oneshot::Sender<()>>>>,
    progress_rx: Arc<Mutex<broadcast::Receiver<ProgressUpdate>>>,
}

impl ExecutionHandle {
    pub fn new(id: Uuid) -> Self {
        let (result_tx, result_rx) = oneshot::channel();
        let (cancel_tx, cancel_rx) = oneshot::channel();
        let (progress_tx, progress_rx) = broadcast::channel(100);
        
        Self {
            id,
            result_rx: Arc::new(Mutex::new(result_rx)),
            cancel_tx: Arc::new(Mutex::new(Some(cancel_tx))),
            progress_rx: Arc::new(Mutex::new(progress_rx.1)),
        }
    }
    
    /// Wait for the execution to complete
    pub async fn wait(self) -> Result<ToolResult, ExecutionError> {
        match self.result_rx.lock().await.await {
            Ok(result) => Ok(result),
            Err(_) => Err(ExecutionError::Cancelled),
        }
    }
    
    /// Cancel the execution
    pub async fn cancel(&self) -> Result<(), ExecutionError> {
        if let Some(tx) = self.cancel_tx.lock().await.take() {
            tx.send(()).map_err(|_| ExecutionError::AlreadyCompleted)
        } else {
            Err(ExecutionError::AlreadyCancelled)
        }
    }
    
    /// Stream progress updates
    pub async fn progress_stream(&self) -> impl Stream<Item = ProgressUpdate> {
        let mut rx = self.progress_rx.lock().await.resubscribe();
        
        async_stream::stream! {
            while let Ok(update) = rx.recv().await {
                if update.execution_id == self.id {
                    yield update;
                }
            }
        }
    }
}
```

### 6. Integration with Tool System

```rust
/// Enhanced tool trait with parallel execution support
#[async_trait]
pub trait ParallelAgentTool: AgentTool {
    /// Get resource requirements for this tool
    fn resource_requirements(&self) -> ResourceRequirements {
        ResourceRequirements {
            cpu_weight: 0.1,
            memory_mb: 100,
            io_intensity: IOIntensity::Low,
            network_required: false,
            exclusive_resources: HashSet::new(),
        }
    }
    
    /// Get execution priority
    fn priority(&self) -> ExecutionPriority {
        ExecutionPriority::Normal
    }
    
    /// Check if this tool can run in parallel with another tool
    fn can_run_parallel_with(&self, other: &str) -> bool {
        true
    }
    
    /// Get progress updates during execution
    async fn report_progress(&self, progress: f32, message: Option<String>) {
        // Default implementation - tools can override
    }
}

/// Parallel execution coordinator for tools
pub struct ParallelToolCoordinator {
    engine: Arc<ParallelExecutionEngine>,
    tool_registry: Arc<RwLock<HashMap<String, Arc<dyn ParallelAgentTool>>>>,
}

impl ParallelToolCoordinator {
    /// Execute multiple tools in parallel
    pub async fn execute_parallel(
        &self,
        tool_calls: Vec<ToolCall>,
    ) -> Vec<Result<ToolResult, ToolError>> {
        let mut handles = Vec::new();
        let mut dependency_map = HashMap::new();
        
        // Build execution contexts
        for (idx, call) in tool_calls.iter().enumerate() {
            let context = ToolExecutionContext {
                id: Uuid::new_v4(),
                tool_name: call.name.clone(),
                priority: ExecutionPriority::Normal,
                resource_requirements: self.get_tool_requirements(&call.name).await,
                dependencies: self.resolve_dependencies(&call, &dependency_map),
                timeout: Duration::from_secs(300),
                retry_policy: RetryPolicy::default(),
            };
            
            dependency_map.insert(idx, context.id);
            
            // Submit for execution
            let handle = self.engine
                .submit_tool(context, self.create_tool_closure(call.clone()))
                .await
                .unwrap();
                
            handles.push(handle);
        }
        
        // Wait for all executions
        let mut results = Vec::new();
        for handle in handles {
            results.push(handle.wait().await.map_err(|e| e.into()));
        }
        
        results
    }
}
```

## Architecture Changes

### Current Architecture
```
┌─────────────────┐
│   Tool System   │
├─────────────────┤
│ Sequential Exec │
│ Blocking Calls  │
│ No Concurrency  │
└─────────────────┘
```

### Target Architecture
```
┌─────────────────────────────────────────────┐
│          Parallel Execution Engine          │
├─────────────────────────────────────────────┤
│ ┌─────────────┐ ┌─────────────┐ ┌─────────┐│
│ │  Resource   │ │ Dependency  │ │Progress ││
│ │  Manager    │ │   Graph     │ │Tracker  ││
│ └─────────────┘ └─────────────┘ └─────────┘│
├─────────────────────────────────────────────┤
│ ┌─────────────┐ ┌─────────────┐ ┌─────────┐│
│ │  Execution  │ │   Priority  │ │Streaming││
│ │  Scheduler  │ │    Queue    │ │Updates  ││
│ └─────────────┘ └─────────────┘ └─────────┘│
└─────────────────────────────────────────────┘
```

## Implementation Plan

### Phase 1: Core Infrastructure (Week 1-2)
1. Implement `ParallelExecutionEngine` base structure
2. Create `ResourceManager` with basic allocation
3. Build `DependencyGraph` with cycle detection
4. Set up `ProgressTracker` with streaming updates

### Phase 2: Execution System (Week 2-3)
1. Implement execution scheduler with priority queue
2. Create `ExecutionHandle` with cancellation support
3. Build resource allocation algorithms
4. Add timeout and retry mechanisms

### Phase 3: Tool Integration (Week 3-4)
1. Extend `AgentTool` trait for parallel execution
2. Implement `ParallelToolCoordinator`
3. Update existing tools with resource requirements
4. Create tool dependency resolution

### Phase 4: Advanced Features (Week 4-5)
1. Implement advanced scheduling algorithms
2. Add resource prediction and optimization
3. Create execution visualization
4. Build performance monitoring

### Phase 5: Testing & Optimization (Week 5-6)
1. Comprehensive unit tests for all components
2. Integration tests with real tools
3. Performance benchmarking
4. Resource utilization optimization

## Testing Strategy

### Unit Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_parallel_execution_basic() {
        let config = ParallelExecutionConfig {
            max_concurrency: 4,
            resource_limits: ResourceLimits::default(),
        };
        
        let engine = ParallelExecutionEngine::new(config);
        
        // Create test tools
        let tools = vec![
            create_test_tool("tool1", Duration::from_millis(100)),
            create_test_tool("tool2", Duration::from_millis(200)),
            create_test_tool("tool3", Duration::from_millis(150)),
        ];
        
        // Execute in parallel
        let start = Instant::now();
        let handles = submit_tools(&engine, tools).await;
        let results = wait_all(handles).await;
        let duration = start.elapsed();
        
        // Should complete faster than sequential (200ms vs 450ms)
        assert!(duration < Duration::from_millis(250));
        assert_eq!(results.len(), 3);
        assert!(results.iter().all(|r| r.is_ok()));
    }
    
    #[tokio::test]
    async fn test_dependency_resolution() {
        let mut graph = DependencyGraph::new();
        
        let id1 = Uuid::new_v4();
        let id2 = Uuid::new_v4();
        let id3 = Uuid::new_v4();
        
        // Create dependency chain: 3 -> 2 -> 1
        graph.add_node(id1, HashSet::new());
        graph.add_node(id2, HashSet::from([id1]));
        graph.add_node(id3, HashSet::from([id2]));
        
        // Initially only id1 should be ready
        let ready = graph.get_ready_nodes();
        assert_eq!(ready, vec![id1]);
        
        // After completing id1, id2 should be ready
        graph.mark_completed(id1);
        let ready = graph.get_ready_nodes();
        assert_eq!(ready, vec![id2]);
    }
    
    #[tokio::test]
    async fn test_resource_allocation() {
        let mut manager = ResourceManager::new(ResourceLimits {
            max_cpu: 1.0,
            max_memory_mb: 1024,
            max_io_operations: 10,
        });
        
        let req1 = ResourceRequirements {
            cpu_weight: 0.5,
            memory_mb: 512,
            io_intensity: IOIntensity::Low,
            network_required: false,
            exclusive_resources: HashSet::new(),
        };
        
        let req2 = ResourceRequirements {
            cpu_weight: 0.6,
            memory_mb: 256,
            io_intensity: IOIntensity::Low,
            network_required: false,
            exclusive_resources: HashSet::new(),
        };
        
        // First allocation should succeed
        let alloc1 = manager.try_allocate(Uuid::new_v4(), &req1).unwrap();
        
        // Second allocation should fail (insufficient CPU)
        let result = manager.try_allocate(Uuid::new_v4(), &req2);
        assert!(matches!(result, Err(ResourceError::InsufficientCPU)));
        
        // After releasing first allocation, second should succeed
        manager.release(alloc1);
        let alloc2 = manager.try_allocate(Uuid::new_v4(), &req2).unwrap();
        assert!(alloc2.cpu == 0.6);
    }
    
    #[tokio::test]
    async fn test_cycle_detection() {
        let mut graph = DependencyGraph::new();
        
        let id1 = Uuid::new_v4();
        let id2 = Uuid::new_v4();
        let id3 = Uuid::new_v4();
        
        // Create cycle: 1 -> 2 -> 3 -> 1
        graph.add_node(id1, HashSet::from([id3]));
        graph.add_node(id2, HashSet::from([id1]));
        graph.add_node(id3, HashSet::from([id2]));
        
        let cycle = graph.detect_cycles();
        assert!(cycle.is_some());
        assert_eq!(cycle.unwrap().len(), 3);
    }
}
```

### Integration Tests

```rust
#[tokio::test]
async fn test_real_tool_parallel_execution() {
    let coordinator = create_test_coordinator().await;
    
    // Create tool calls with different execution times
    let tool_calls = vec![
        ToolCall {
            name: "read_file".to_string(),
            arguments: json!({ "path": "test1.txt" }),
        },
        ToolCall {
            name: "list_directory".to_string(),
            arguments: json!({ "path": "." }),
        },
        ToolCall {
            name: "read_file".to_string(),
            arguments: json!({ "path": "test2.txt" }),
        },
    ];
    
    // Execute in parallel
    let start = Instant::now();
    let results = coordinator.execute_parallel(tool_calls).await;
    let duration = start.elapsed();
    
    // Verify parallel execution
    assert_eq!(results.len(), 3);
    assert!(results.iter().all(|r| r.is_ok()));
    
    // Should be faster than sequential execution
    println!("Parallel execution took: {:?}", duration);
}

#[tokio::test]
async fn test_progress_streaming() {
    let engine = create_test_engine();
    let progress_tracker = engine.progress_tracker.clone();
    
    // Subscribe to progress updates
    let mut progress_rx = progress_tracker.subscribe();
    
    // Submit a long-running tool
    let context = create_test_context("long_task", Duration::from_secs(5));
    let handle = engine.submit_tool(context, long_running_tool()).await.unwrap();
    
    // Collect progress updates
    let mut updates = Vec::new();
    tokio::spawn(async move {
        while let Ok(update) = progress_rx.recv().await {
            updates.push(update);
        }
    });
    
    // Wait for completion
    let result = handle.wait().await;
    assert!(result.is_ok());
    
    // Verify progress updates were received
    assert!(updates.len() > 0);
    assert!(updates.iter().any(|u| matches!(u.update_type, ProgressUpdateType::Started)));
    assert!(updates.iter().any(|u| matches!(u.update_type, ProgressUpdateType::Completed)));
}
```

### Performance Tests

```rust
#[tokio::test]
async fn benchmark_parallel_vs_sequential() {
    let coordinator = create_test_coordinator().await;
    
    // Generate many independent tool calls
    let tool_calls: Vec<_> = (0..20)
        .map(|i| ToolCall {
            name: "compute_intensive".to_string(),
            arguments: json!({ "input": i }),
        })
        .collect();
    
    // Sequential execution
    let start = Instant::now();
    for call in &tool_calls {
        coordinator.execute_single(call.clone()).await.unwrap();
    }
    let sequential_time = start.elapsed();
    
    // Parallel execution
    let start = Instant::now();
    let results = coordinator.execute_parallel(tool_calls).await;
    let parallel_time = start.elapsed();
    
    println!("Sequential: {:?}, Parallel: {:?}", sequential_time, parallel_time);
    println!("Speedup: {:.2}x", sequential_time.as_secs_f64() / parallel_time.as_secs_f64());
    
    // Should be significantly faster
    assert!(parallel_time < sequential_time / 2);
    assert!(results.iter().all(|r| r.is_ok()));
}

#[tokio::test]
async fn stress_test_high_concurrency() {
    let config = ParallelExecutionConfig {
        max_concurrency: 100,
        resource_limits: ResourceLimits {
            max_cpu: 4.0,
            max_memory_mb: 8192,
            max_io_operations: 50,
        },
    };
    
    let engine = ParallelExecutionEngine::new(config);
    
    // Submit many tools at once
    let mut handles = Vec::new();
    for i in 0..1000 {
        let context = create_test_context(&format!("tool_{}", i), Duration::from_millis(10));
        let handle = engine.submit_tool(context, quick_tool()).await.unwrap();
        handles.push(handle);
    }
    
    // Wait for all to complete
    let start = Instant::now();
    let results = join_all(handles.into_iter().map(|h| h.wait())).await;
    let duration = start.elapsed();
    
    println!("Processed 1000 tools in {:?}", duration);
    assert_eq!(results.len(), 1000);
    assert!(results.iter().all(|r| r.is_ok()));
}
```

## Dependencies & Integration

### Phase 1 Dependencies
- **Issue 1.3**: Tool Result Handling System
  - Provides `ToolResult` type and error handling
  - Required for execution result management
  
- **Issue 1.4**: Streaming Response Architecture  
  - Provides SSE infrastructure for progress updates
  - Required for real-time progress tracking

### Integration Points
- **Tool System**: Extends existing `AgentTool` trait
- **Streaming System**: Uses SSE for progress updates
- **Error Handling**: Integrates with enhanced error system
- **Configuration**: Uses TOML for parallel execution settings

### API Changes
- New `ParallelAgentTool` trait extending `AgentTool`
- Additional methods on `ToolCoordinator` for parallel execution
- New streaming endpoints for progress tracking

## Security Considerations

### Resource Isolation
- CPU and memory limits per tool execution
- Exclusive resource locking mechanism
- Prevention of resource starvation

### Execution Sandboxing
- Tool execution in isolated contexts
- Timeout enforcement for all executions
- Graceful handling of misbehaving tools

### Progress Information Security
- No sensitive data in progress updates
- Rate limiting on progress streams
- Authentication for progress endpoints

## Acceptance Criteria

1. **Parallel Execution**
   - [ ] Tools can execute concurrently with proper resource management
   - [ ] Dependency resolution prevents incorrect execution order
   - [ ] System respects configured concurrency limits

2. **Resource Management**
   - [ ] CPU and memory allocation works correctly
   - [ ] Exclusive resources properly locked/unlocked
   - [ ] Resource utilization stays within limits

3. **Progress Tracking**
   - [ ] Real-time progress updates stream to UI
   - [ ] Aggregated progress shows overall status
   - [ ] Individual tool progress is trackable

4. **Performance**
   - [ ] Parallel execution is faster than sequential for independent tools
   - [ ] System handles 100+ concurrent executions
   - [ ] Resource overhead is minimal

5. **Error Handling**
   - [ ] Failed tools don't block other executions
   - [ ] Dependency failures cascade properly
   - [ ] Recovery mechanisms work as expected

6. **Integration**
   - [ ] Works with existing tool system
   - [ ] Streaming updates integrate with SSE
   - [ ] Configuration through TOML works

## Quality Control

### 1. Unit Tests (400 LOC)

#### Core Component Tests
- **Parallel Execution Engine Tests**
  - Concurrent tool execution with resource limits
  - Dependency resolution and ordering
  - Timeout and cancellation handling
  - Resource allocation and deallocation

- **Resource Manager Tests**
  - CPU and memory allocation validation
  - Exclusive resource locking mechanisms
  - Resource utilization tracking
  - Allocation failure scenarios

- **Dependency Graph Tests**
  - Cycle detection algorithms
  - Dependency resolution ordering
  - Graph modification operations
  - Performance with large graphs

#### Progress Tracking Tests
- **Real-time Updates**
  - Progress broadcast functionality
  - Aggregated progress calculations
  - Streaming update delivery
  - Statistics collection accuracy

### 2. Integration Tests (300 LOC)

#### End-to-End Workflow Tests
- **Parallel Tool Execution**
  - Multiple tools running concurrently
  - Dependency-based execution ordering
  - Resource contention scenarios
  - Mixed priority executions

- **Tool Coordinator Integration**
  - Integration with existing tool system
  - Tool registration and discovery
  - Error propagation through execution chain
  - Performance optimization validation

### 3. Performance Tests (200 LOC)

#### Benchmarking
- **Concurrency Performance**
  - Parallel vs sequential execution comparison
  - Scalability with increasing concurrency
  - Resource utilization efficiency
  - Memory and CPU overhead measurement

- **Stress Testing**
  - High-concurrency tool execution (100+ tools)
  - Long-running tool scenarios
  - Resource exhaustion recovery
  - System stability under load

### 4. Manual Testing

#### User Experience Testing
- **Progress Visibility**
  - Real-time progress updates in UI
  - Cancellation responsiveness
  - Error reporting clarity
  - Performance impact on UI

- **Resource Management**
  - System responsiveness during high load
  - Tool execution prioritization
  - Resource limit enforcement
  - Recovery from resource constraints

## Documentation Requirements

### 1. Architecture Documentation
- **System Design Document**
  - Parallel execution architecture overview
  - Resource management strategy
  - Dependency resolution algorithms
  - Progress tracking implementation

- **Component Interaction Diagrams**
  - Tool execution flow
  - Resource allocation lifecycle
  - Progress update propagation
  - Error handling pathways

### 2. API Documentation
- **ParallelAgentTool Trait**
  - Resource requirement specification
  - Priority configuration
  - Progress reporting methods
  - Compatibility guidelines

- **Execution Handle API**
  - Cancellation mechanisms
  - Progress streaming
  - Result retrieval
  - Error handling

### 3. Configuration Guide
- **Parallel Execution Settings**
  - Concurrency limits configuration
  - Resource allocation tuning
  - Performance optimization guidelines
  - Monitoring and observability setup

### 4. Integration Documentation
- **Tool Developer Guide**
  - Converting tools for parallel execution
  - Resource requirement specification
  - Dependency declaration
  - Progress reporting best practices

## Dependencies

### 1. Foundational Dependencies
- **Issue 1.1**: Enhanced ContentBlock System
  - Provides foundation for structured tool communication
  - Required for tool result integration

- **Issue 1.2**: Unified Error Handling Framework
  - Provides robust error types and context
  - Required for execution error management

- **Issue 1.3**: Tool Result Handling and Feedback Loop System
  - Provides `ToolExecutionResult` types
  - Required for result processing and feedback

### 2. System Integration Dependencies
- **Issue 1.4**: Basic Streaming Foundation
  - Provides SSE infrastructure for progress updates
  - Required for real-time progress tracking

- **Issue 1.5**: Configuration System Foundation (if exists)
  - Provides TOML configuration support
  - Required for parallel execution settings

### 3. Tool System Dependencies
- **Existing Tool Infrastructure**
  - `AgentTool` trait definition
  - Tool registration system
  - Whitelist validation integration

### 4. External Dependencies
```toml
# Required crate additions to Cargo.toml
tokio = { version = "1.0", features = ["full"] }
futures = "0.3"
uuid = { version = "1.0", features = ["v4", "serde"] }
async-stream = "0.3"
priority-queue = "1.3"
```

## References

### 1. Technical Resources
- [Tokio Documentation - Concurrency](https://tokio.rs/tokio/tutorial/shared-state)
- [Rust Async Book - Pinning](https://rust-lang.github.io/async-book/04_pinning/01_chapter.html)
- [Resource Management Patterns](https://docs.rs/tokio/latest/tokio/sync/struct.Semaphore.html)
- [Dependency Graph Algorithms](https://en.wikipedia.org/wiki/Topological_sorting)

### 2. Anthropic Documentation
- [Claude API - Tool Use](https://docs.anthropic.com/claude/docs/tool-use)
- [Tool Use Best Practices](https://docs.anthropic.com/en/docs/tool-use)

### 3. GitHub Issues
- [Issue 1.3: Tool Result Handling System](./1.3-tool-result-handling-feedback.md)
- [Issue 1.4: Streaming Foundation](./1.4-streaming-foundation.md)
- [Implementation Sequencing Guide](../implementation-sequencing.md)

### 4. Internal Documentation
- [Architecture Overview](../architecture/overview.md)
- [Tool System Documentation](../tools/overview.md)
- [Performance Guidelines](../development/performance.md)

## Estimated Lines of Code

**Implementation: ~1,400 LOC**
- Core parallel execution engine: ~400 LOC
- Resource management system: ~300 LOC
- Dependency graph implementation: ~250 LOC
- Progress tracking system: ~200 LOC
- Execution handles and coordination: ~150 LOC
- Integration with existing tools: ~100 LOC

**Testing: ~900 LOC**
- Unit tests: ~400 LOC
- Integration tests: ~300 LOC
- Performance tests: ~200 LOC

**Total: ~2,300 LOC**

This comprehensive parallel tool execution system provides the foundation for advanced tool orchestration while maintaining resource efficiency and providing real-time progress visibility to users.