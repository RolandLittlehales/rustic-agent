# [3.4] Advanced Tools: Performance Optimization and Production Hardening

## Overview

This issue implements comprehensive performance monitoring, optimization strategies, and production hardening for the Claude Desktop Agent. It focuses on memory management, resource cleanup, caching strategies, connection pooling, and establishing production-ready deployment configurations with comprehensive monitoring and alerting systems.

**Why This Matters:**
- Current system lacks performance monitoring and optimization strategies
- Memory usage and resource cleanup need comprehensive management
- Production deployments require hardened configurations and monitoring
- Performance bottlenecks in tool execution and API communication need optimization
- Resource leaks and inefficient patterns can degrade long-running performance

**Current State:**
- Basic error handling and streaming are implemented (from 2.3 and 2.4)
- Tool execution system exists but lacks performance optimization
- No comprehensive monitoring or resource management
- Memory usage patterns are not optimized for long-running operations
- No production deployment configurations or hardening

**Target State:**
- Comprehensive performance monitoring and metrics collection
- Optimized memory management and automatic resource cleanup
- Connection pooling and caching strategies implemented
- Production-ready deployment configurations with security hardening
- Performance alerting and automatic optimization systems
- Resource usage monitoring and leak detection

## Technical Requirements

### 1. Performance Monitoring System

#### 1.1 Core Metrics Collection
```rust
// New file: src-tauri/src/performance/metrics.rs
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceMetric {
    pub name: String,
    pub value: f64,
    pub unit: MetricUnit,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub tags: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum MetricUnit {
    Milliseconds,
    Seconds,
    Bytes,
    Kilobytes,
    Megabytes,
    Count,
    Percentage,
    Rate,
}

#[derive(Debug)]
pub struct MetricsCollector {
    metrics: Arc<RwLock<HashMap<String, Vec<PerformanceMetric>>>>,
    start_time: Instant,
    retention_duration: Duration,
}

impl MetricsCollector {
    pub fn new() -> Self {
        Self {
            metrics: Arc::new(RwLock::new(HashMap::new())),
            start_time: Instant::now(),
            retention_duration: Duration::from_secs(3600), // 1 hour retention
        }
    }
    
    pub async fn record_timing(&self, operation: &str, duration: Duration, tags: HashMap<String, String>) {
        let metric = PerformanceMetric {
            name: format!("timing.{}", operation),
            value: duration.as_millis() as f64,
            unit: MetricUnit::Milliseconds,
            timestamp: chrono::Utc::now(),
            tags,
        };
        
        self.record_metric(metric).await;
    }
    
    pub async fn record_memory_usage(&self, component: &str, bytes: u64, tags: HashMap<String, String>) {
        let metric = PerformanceMetric {
            name: format!("memory.{}", component),
            value: bytes as f64,
            unit: MetricUnit::Bytes,
            timestamp: chrono::Utc::now(),
            tags,
        };
        
        self.record_metric(metric).await;
    }
    
    pub async fn record_counter(&self, name: &str, value: f64, tags: HashMap<String, String>) {
        let metric = PerformanceMetric {
            name: name.to_string(),
            value,
            unit: MetricUnit::Count,
            timestamp: chrono::Utc::now(),
            tags,
        };
        
        self.record_metric(metric).await;
    }
    
    pub async fn record_gauge(&self, name: &str, value: f64, unit: MetricUnit, tags: HashMap<String, String>) {
        let metric = PerformanceMetric {
            name: name.to_string(),
            value,
            unit,
            timestamp: chrono::Utc::now(),
            tags,
        };
        
        self.record_metric(metric).await;
    }
    
    async fn record_metric(&self, metric: PerformanceMetric) {
        let mut metrics = self.metrics.write().await;
        let metric_list = metrics.entry(metric.name.clone()).or_insert_with(Vec::new);
        metric_list.push(metric);
        
        // Cleanup old metrics
        self.cleanup_old_metrics(&mut metrics).await;
    }
    
    async fn cleanup_old_metrics(&self, metrics: &mut HashMap<String, Vec<PerformanceMetric>>) {
        let cutoff = chrono::Utc::now() - chrono::Duration::from_std(self.retention_duration).unwrap();
        
        for metric_list in metrics.values_mut() {
            metric_list.retain(|m| m.timestamp > cutoff);
        }
        
        metrics.retain(|_, metric_list| !metric_list.is_empty());
    }
    
    pub async fn get_metrics(&self, metric_name: Option<&str>) -> HashMap<String, Vec<PerformanceMetric>> {
        let metrics = self.metrics.read().await;
        
        if let Some(name) = metric_name {
            if let Some(metric_list) = metrics.get(name) {
                return [(name.to_string(), metric_list.clone())].into_iter().collect();
            }
            return HashMap::new();
        }
        
        metrics.clone()
    }
    
    pub async fn get_metric_statistics(&self, metric_name: &str) -> Option<MetricStatistics> {
        let metrics = self.metrics.read().await;
        let metric_list = metrics.get(metric_name)?;
        
        if metric_list.is_empty() {
            return None;
        }
        
        let values: Vec<f64> = metric_list.iter().map(|m| m.value).collect();
        let count = values.len();
        let sum = values.iter().sum::<f64>();
        let avg = sum / count as f64;
        
        let mut sorted_values = values.clone();
        sorted_values.sort_by(|a, b| a.partial_cmp(b).unwrap());
        
        let min = sorted_values[0];
        let max = sorted_values[count - 1];
        let median = if count % 2 == 0 {
            (sorted_values[count / 2 - 1] + sorted_values[count / 2]) / 2.0
        } else {
            sorted_values[count / 2]
        };
        
        let p95_index = ((count as f64) * 0.95).ceil() as usize - 1;
        let p95 = sorted_values[p95_index.min(count - 1)];
        
        let p99_index = ((count as f64) * 0.99).ceil() as usize - 1;
        let p99 = sorted_values[p99_index.min(count - 1)];
        
        Some(MetricStatistics {
            count,
            sum,
            avg,
            min,
            max,
            median,
            p95,
            p99,
        })
    }
    
    pub fn uptime(&self) -> Duration {
        self.start_time.elapsed()
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricStatistics {
    pub count: usize,
    pub sum: f64,
    pub avg: f64,
    pub min: f64,
    pub max: f64,
    pub median: f64,
    pub p95: f64,
    pub p99: f64,
}
```

#### 1.2 Performance Instrumentation Macros
```rust
// New file: src-tauri/src/performance/instrumentation.rs
use std::collections::HashMap;
use std::time::Instant;
use super::metrics::MetricsCollector;

pub struct TimingGuard<'a> {
    collector: &'a MetricsCollector,
    operation: String,
    start: Instant,
    tags: HashMap<String, String>,
}

impl<'a> TimingGuard<'a> {
    pub fn new(collector: &'a MetricsCollector, operation: String, tags: HashMap<String, String>) -> Self {
        Self {
            collector,
            operation,
            start: Instant::now(),
            tags,
        }
    }
    
    pub async fn finish(self) {
        let duration = self.start.elapsed();
        self.collector.record_timing(&self.operation, duration, self.tags).await;
    }
}

impl<'a> Drop for TimingGuard<'a> {
    fn drop(&mut self) {
        let duration = self.start.elapsed();
        let collector = self.collector;
        let operation = self.operation.clone();
        let tags = self.tags.clone();
        
        tokio::spawn(async move {
            collector.record_timing(&operation, duration, tags).await;
        });
    }
}

// Convenience macros for instrumentation
#[macro_export]
macro_rules! time_operation {
    ($collector:expr, $operation:expr, $code:expr) => {{
        let _guard = $crate::performance::instrumentation::TimingGuard::new(
            $collector, 
            $operation.to_string(), 
            std::collections::HashMap::new()
        );
        let result = $code;
        _guard.finish().await;
        result
    }};
    
    ($collector:expr, $operation:expr, $tags:expr, $code:expr) => {{
        let _guard = $crate::performance::instrumentation::TimingGuard::new(
            $collector, 
            $operation.to_string(), 
            $tags
        );
        let result = $code;
        _guard.finish().await;
        result
    }};
}

#[macro_export]
macro_rules! record_memory {
    ($collector:expr, $component:expr, $bytes:expr) => {
        $collector.record_memory_usage($component, $bytes, std::collections::HashMap::new()).await;
    };
    
    ($collector:expr, $component:expr, $bytes:expr, $tags:expr) => {
        $collector.record_memory_usage($component, $bytes, $tags).await;
    };
}

// System metrics collection
pub struct SystemMetrics;

impl SystemMetrics {
    pub async fn collect_system_info(collector: &MetricsCollector) {
        // CPU usage
        if let Ok(cpu_usage) = Self::get_cpu_usage() {
            collector.record_gauge(
                "system.cpu_usage", 
                cpu_usage, 
                crate::performance::metrics::MetricUnit::Percentage,
                HashMap::new()
            ).await;
        }
        
        // Memory usage
        if let Ok(memory_info) = Self::get_memory_info() {
            collector.record_gauge(
                "system.memory_used", 
                memory_info.used as f64, 
                crate::performance::metrics::MetricUnit::Bytes,
                HashMap::new()
            ).await;
            
            collector.record_gauge(
                "system.memory_total", 
                memory_info.total as f64, 
                crate::performance::metrics::MetricUnit::Bytes,
                HashMap::new()
            ).await;
            
            let usage_percent = (memory_info.used as f64 / memory_info.total as f64) * 100.0;
            collector.record_gauge(
                "system.memory_usage", 
                usage_percent, 
                crate::performance::metrics::MetricUnit::Percentage,
                HashMap::new()
            ).await;
        }
        
        // Disk usage
        if let Ok(disk_info) = Self::get_disk_info() {
            collector.record_gauge(
                "system.disk_used", 
                disk_info.used as f64, 
                crate::performance::metrics::MetricUnit::Bytes,
                HashMap::new()
            ).await;
            
            let usage_percent = (disk_info.used as f64 / disk_info.total as f64) * 100.0;
            collector.record_gauge(
                "system.disk_usage", 
                usage_percent, 
                crate::performance::metrics::MetricUnit::Percentage,
                HashMap::new()
            ).await;
        }
    }
    
    fn get_cpu_usage() -> Result<f64, Box<dyn std::error::Error>> {
        // Implementation depends on platform
        #[cfg(target_os = "linux")]
        {
            use std::fs;
            let contents = fs::read_to_string("/proc/stat")?;
            let line = contents.lines().next().ok_or("No CPU stats")?;
            let values: Vec<u64> = line.split_whitespace()
                .skip(1)
                .take(7)
                .map(|s| s.parse().unwrap_or(0))
                .collect();
            
            let idle = values[3];
            let total: u64 = values.iter().sum();
            let usage = 100.0 - (idle as f64 / total as f64 * 100.0);
            Ok(usage)
        }
        
        #[cfg(not(target_os = "linux"))]
        Ok(0.0) // Platform-specific implementation needed
    }
    
    fn get_memory_info() -> Result<MemoryInfo, Box<dyn std::error::Error>> {
        #[cfg(target_os = "linux")]
        {
            use std::fs;
            let contents = fs::read_to_string("/proc/meminfo")?;
            let mut total = 0u64;
            let mut available = 0u64;
            
            for line in contents.lines() {
                if line.starts_with("MemTotal:") {
                    total = line.split_whitespace().nth(1)
                        .ok_or("Invalid MemTotal format")?
                        .parse::<u64>()? * 1024; // Convert kB to bytes
                } else if line.starts_with("MemAvailable:") {
                    available = line.split_whitespace().nth(1)
                        .ok_or("Invalid MemAvailable format")?
                        .parse::<u64>()? * 1024; // Convert kB to bytes
                }
            }
            
            Ok(MemoryInfo {
                total,
                used: total - available,
                available,
            })
        }
        
        #[cfg(not(target_os = "linux"))]
        Ok(MemoryInfo { total: 0, used: 0, available: 0 })
    }
    
    fn get_disk_info() -> Result<DiskInfo, Box<dyn std::error::Error>> {
        #[cfg(unix)]
        {
            use std::ffi::CString;
            use std::mem;
            
            let path = CString::new("/")?;
            let mut statvfs: libc::statvfs = unsafe { mem::zeroed() };
            
            unsafe {
                if libc::statvfs(path.as_ptr(), &mut statvfs) != 0 {
                    return Err("statvfs failed".into());
                }
            }
            
            let total = statvfs.f_blocks * statvfs.f_frsize;
            let available = statvfs.f_bavail * statvfs.f_frsize;
            let used = total - available;
            
            Ok(DiskInfo { total, used, available })
        }
        
        #[cfg(not(unix))]
        Ok(DiskInfo { total: 0, used: 0, available: 0 })
    }
}

#[derive(Debug)]
struct MemoryInfo {
    total: u64,
    used: u64,
    available: u64,
}

#[derive(Debug)]
struct DiskInfo {
    total: u64,
    used: u64,
    available: u64,
}
```

### 2. Memory Management and Resource Cleanup

#### 2.1 Memory Pool Management
```rust
// New file: src-tauri/src/performance/memory.rs
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{Mutex, RwLock};
use bytes::BytesMut;

#[derive(Debug)]
pub struct MemoryPool {
    pools: HashMap<usize, Arc<Mutex<Vec<BytesMut>>>>,
    max_pool_size: usize,
    buffer_sizes: Vec<usize>,
    stats: Arc<RwLock<PoolStats>>,
}

#[derive(Debug, Default)]
struct PoolStats {
    allocations: u64,
    deallocations: u64,
    cache_hits: u64,
    cache_misses: u64,
    total_allocated_bytes: u64,
    peak_allocated_bytes: u64,
}

impl MemoryPool {
    pub fn new() -> Self {
        let buffer_sizes = vec![1024, 4096, 16384, 65536, 262144, 1048576]; // 1KB to 1MB
        let mut pools = HashMap::new();
        
        for &size in &buffer_sizes {
            pools.insert(size, Arc::new(Mutex::new(Vec::new())));
        }
        
        Self {
            pools,
            max_pool_size: 100, // Max 100 buffers per size
            buffer_sizes,
            stats: Arc::new(RwLock::new(PoolStats::default())),
        }
    }
    
    pub async fn get_buffer(&self, min_size: usize) -> BytesMut {
        let actual_size = self.round_up_size(min_size);
        
        if let Some(pool) = self.pools.get(&actual_size) {
            let mut pool_guard = pool.lock().await;
            if let Some(buffer) = pool_guard.pop() {
                // Update stats
                {
                    let mut stats = self.stats.write().await;
                    stats.cache_hits += 1;
                    stats.total_allocated_bytes += actual_size as u64;
                    if stats.total_allocated_bytes > stats.peak_allocated_bytes {
                        stats.peak_allocated_bytes = stats.total_allocated_bytes;
                    }
                }
                return buffer;
            }
        }
        
        // No buffer available, allocate new one
        let buffer = BytesMut::with_capacity(actual_size);
        
        // Update stats
        {
            let mut stats = self.stats.write().await;
            stats.cache_misses += 1;
            stats.allocations += 1;
            stats.total_allocated_bytes += actual_size as u64;
            if stats.total_allocated_bytes > stats.peak_allocated_bytes {
                stats.peak_allocated_bytes = stats.total_allocated_bytes;
            }
        }
        
        buffer
    }
    
    pub async fn return_buffer(&self, mut buffer: BytesMut) {
        let size = buffer.capacity();
        let actual_size = self.round_up_size(size);
        
        if let Some(pool) = self.pools.get(&actual_size) {
            let mut pool_guard = pool.lock().await;
            if pool_guard.len() < self.max_pool_size {
                buffer.clear(); // Clear data but keep capacity
                pool_guard.push(buffer);
                
                // Update stats
                {
                    let mut stats = self.stats.write().await;
                    stats.deallocations += 1;
                    stats.total_allocated_bytes = stats.total_allocated_bytes.saturating_sub(actual_size as u64);
                }
                return;
            }
        }
        
        // Pool is full or doesn't exist, just drop the buffer
        {
            let mut stats = self.stats.write().await;
            stats.deallocations += 1;
            stats.total_allocated_bytes = stats.total_allocated_bytes.saturating_sub(actual_size as u64);
        }
    }
    
    fn round_up_size(&self, size: usize) -> usize {
        self.buffer_sizes.iter()
            .find(|&&pool_size| pool_size >= size)
            .copied()
            .unwrap_or_else(|| {
                // For very large buffers, round up to nearest MB
                ((size + 1048575) / 1048576) * 1048576
            })
    }
    
    pub async fn get_stats(&self) -> PoolStats {
        self.stats.read().await.clone()
    }
    
    pub async fn cleanup(&self) {
        for pool in self.pools.values() {
            pool.lock().await.clear();
        }
        
        let mut stats = self.stats.write().await;
        stats.total_allocated_bytes = 0;
    }
}

#[derive(Debug, Clone)]
pub struct PoolStats {
    pub allocations: u64,
    pub deallocations: u64,
    pub cache_hits: u64,
    pub cache_misses: u64,
    pub total_allocated_bytes: u64,
    pub peak_allocated_bytes: u64,
}

impl PoolStats {
    pub fn hit_rate(&self) -> f64 {
        if self.cache_hits + self.cache_misses == 0 {
            return 0.0;
        }
        self.cache_hits as f64 / (self.cache_hits + self.cache_misses) as f64
    }
}

// Resource tracker for monitoring object lifetimes
#[derive(Debug)]
pub struct ResourceTracker {
    resources: Arc<RwLock<HashMap<String, ResourceInfo>>>,
}

#[derive(Debug, Clone)]
struct ResourceInfo {
    count: u64,
    total_size: u64,
    created_at: Instant,
    last_accessed: Instant,
}

impl ResourceTracker {
    pub fn new() -> Self {
        Self {
            resources: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    pub async fn track_allocation(&self, resource_type: &str, size: u64) {
        let mut resources = self.resources.write().await;
        let info = resources.entry(resource_type.to_string()).or_insert_with(|| ResourceInfo {
            count: 0,
            total_size: 0,
            created_at: Instant::now(),
            last_accessed: Instant::now(),
        });
        
        info.count += 1;
        info.total_size += size;
        info.last_accessed = Instant::now();
    }
    
    pub async fn track_deallocation(&self, resource_type: &str, size: u64) {
        let mut resources = self.resources.write().await;
        if let Some(info) = resources.get_mut(resource_type) {
            info.count = info.count.saturating_sub(1);
            info.total_size = info.total_size.saturating_sub(size);
            info.last_accessed = Instant::now();
            
            // Remove entries with zero count
            if info.count == 0 {
                resources.remove(resource_type);
            }
        }
    }
    
    pub async fn get_resource_info(&self) -> HashMap<String, ResourceInfo> {
        self.resources.read().await.clone()
    }
    
    pub async fn detect_leaks(&self, threshold: Duration) -> Vec<String> {
        let resources = self.resources.read().await;
        let now = Instant::now();
        
        resources.iter()
            .filter(|(_, info)| now.duration_since(info.last_accessed) > threshold)
            .map(|(name, _)| name.clone())
            .collect()
    }
}
```

#### 2.2 Automatic Resource Cleanup
```rust
// New file: src-tauri/src/performance/cleanup.rs
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::RwLock;
use tokio::time::{interval, Instant};
use crate::performance::{memory::MemoryPool, metrics::MetricsCollector};

#[derive(Debug)]
pub struct ResourceCleanupManager {
    memory_pool: Arc<MemoryPool>,
    metrics_collector: Arc<MetricsCollector>,
    cleanup_interval: Duration,
    max_idle_time: Duration,
    running: Arc<RwLock<bool>>,
}

impl ResourceCleanupManager {
    pub fn new(
        memory_pool: Arc<MemoryPool>,
        metrics_collector: Arc<MetricsCollector>,
    ) -> Self {
        Self {
            memory_pool,
            metrics_collector,
            cleanup_interval: Duration::from_secs(60), // Clean up every minute
            max_idle_time: Duration::from_secs(300),   // 5 minutes idle threshold
            running: Arc::new(RwLock::new(false)),
        }
    }
    
    pub async fn start(&self) {
        {
            let mut running = self.running.write().await;
            if *running {
                return; // Already running
            }
            *running = true;
        }
        
        let memory_pool = Arc::clone(&self.memory_pool);
        let metrics_collector = Arc::clone(&self.metrics_collector);
        let cleanup_interval = self.cleanup_interval;
        let max_idle_time = self.max_idle_time;
        let running = Arc::clone(&self.running);
        
        tokio::spawn(async move {
            let mut interval = interval(cleanup_interval);
            
            while *running.read().await {
                interval.tick().await;
                
                // Perform cleanup operations
                Self::cleanup_expired_resources(&memory_pool, &metrics_collector, max_idle_time).await;
                Self::collect_garbage().await;
                Self::monitor_memory_usage(&metrics_collector).await;
            }
        });
    }
    
    pub async fn stop(&self) {
        let mut running = self.running.write().await;
        *running = false;
    }
    
    async fn cleanup_expired_resources(
        memory_pool: &MemoryPool,
        metrics_collector: &MetricsCollector,
        max_idle_time: Duration,
    ) {
        let start = Instant::now();
        
        // Get current memory pool stats
        let stats_before = memory_pool.get_stats().await;
        
        // Force cleanup of unused buffers (simplified - in real implementation,
        // we'd track buffer last access times)
        if stats_before.hit_rate() < 0.8 { // If hit rate is low, cleanup aggressively
            memory_pool.cleanup().await;
        }
        
        let stats_after = memory_pool.get_stats().await;
        let cleanup_duration = start.elapsed();
        
        // Record cleanup metrics
        metrics_collector.record_timing(
            "cleanup.memory_pool", 
            cleanup_duration, 
            std::collections::HashMap::new()
        ).await;
        
        metrics_collector.record_counter(
            "cleanup.bytes_freed", 
            (stats_before.total_allocated_bytes - stats_after.total_allocated_bytes) as f64,
            std::collections::HashMap::new()
        ).await;
    }
    
    async fn collect_garbage() {
        // Force garbage collection hint to runtime
        // This is a no-op in Rust but can be useful for monitoring
        tokio::task::yield_now().await;
    }
    
    async fn monitor_memory_usage(metrics_collector: &MetricsCollector) {
        use crate::performance::instrumentation::SystemMetrics;
        SystemMetrics::collect_system_info(metrics_collector).await;
        
        // Check for memory pressure and trigger alerts if needed
        if let Ok(memory_info) = Self::get_process_memory_info() {
            metrics_collector.record_gauge(
                "process.memory_rss", 
                memory_info.rss as f64, 
                crate::performance::metrics::MetricUnit::Bytes,
                std::collections::HashMap::new()
            ).await;
            
            metrics_collector.record_gauge(
                "process.memory_vms", 
                memory_info.vms as f64, 
                crate::performance::metrics::MetricUnit::Bytes,
                std::collections::HashMap::new()
            ).await;
        }
    }
    
    fn get_process_memory_info() -> Result<ProcessMemoryInfo, Box<dyn std::error::Error>> {
        #[cfg(target_os = "linux")]
        {
            use std::fs;
            let contents = fs::read_to_string("/proc/self/status")?;
            let mut rss = 0u64;
            let mut vms = 0u64;
            
            for line in contents.lines() {
                if line.starts_with("VmRSS:") {
                    rss = line.split_whitespace().nth(1)
                        .ok_or("Invalid VmRSS format")?
                        .parse::<u64>()? * 1024; // Convert kB to bytes
                } else if line.starts_with("VmSize:") {
                    vms = line.split_whitespace().nth(1)
                        .ok_or("Invalid VmSize format")?
                        .parse::<u64>()? * 1024; // Convert kB to bytes
                }
            }
            
            Ok(ProcessMemoryInfo { rss, vms })
        }
        
        #[cfg(not(target_os = "linux"))]
        Ok(ProcessMemoryInfo { rss: 0, vms: 0 })
    }
}

#[derive(Debug)]
struct ProcessMemoryInfo {
    rss: u64, // Resident Set Size
    vms: u64, // Virtual Memory Size
}

// RAII wrapper for automatic resource cleanup
#[derive(Debug)]
pub struct ManagedResource<T> {
    resource: Option<T>,
    cleanup_fn: Option<Box<dyn FnOnce(T) + Send + 'static>>,
}

impl<T> ManagedResource<T> {
    pub fn new<F>(resource: T, cleanup_fn: F) -> Self 
    where 
        F: FnOnce(T) + Send + 'static,
    {
        Self {
            resource: Some(resource),
            cleanup_fn: Some(Box::new(cleanup_fn)),
        }
    }
    
    pub fn get(&self) -> Option<&T> {
        self.resource.as_ref()
    }
    
    pub fn get_mut(&mut self) -> Option<&mut T> {
        self.resource.as_mut()
    }
    
    pub fn take(mut self) -> Option<T> {
        self.cleanup_fn = None; // Prevent cleanup on drop
        self.resource.take()
    }
}

impl<T> Drop for ManagedResource<T> {
    fn drop(&mut self) {
        if let (Some(resource), Some(cleanup_fn)) = (self.resource.take(), self.cleanup_fn.take()) {
            cleanup_fn(resource);
        }
    }
}
```

### 3. Connection Pooling and Caching

#### 3.1 HTTP Connection Pool
```rust
// New file: src-tauri/src/performance/connection_pool.rs
use reqwest::{Client, ClientBuilder};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::RwLock;
use crate::claude::error::{ClaudeError, ClaudeResult};

#[derive(Debug, Clone)]
pub struct ConnectionPoolConfig {
    pub max_connections_per_host: usize,
    pub max_idle_connections: usize,
    pub idle_timeout: Duration,
    pub connection_timeout: Duration,
    pub request_timeout: Duration,
    pub tcp_keepalive: Duration,
    pub http2_keep_alive_interval: Duration,
}

impl Default for ConnectionPoolConfig {
    fn default() -> Self {
        Self {
            max_connections_per_host: 10,
            max_idle_connections: 20,
            idle_timeout: Duration::from_secs(30),
            connection_timeout: Duration::from_secs(10),
            request_timeout: Duration::from_secs(120),
            tcp_keepalive: Duration::from_secs(60),
            http2_keep_alive_interval: Duration::from_secs(10),
        }
    }
}

#[derive(Debug)]
pub struct HttpConnectionPool {
    clients: Arc<RwLock<HashMap<String, Client>>>,
    config: ConnectionPoolConfig,
}

impl HttpConnectionPool {
    pub fn new(config: ConnectionPoolConfig) -> Self {
        Self {
            clients: Arc::new(RwLock::new(HashMap::new())),
            config,
        }
    }
    
    pub async fn get_client(&self, base_url: &str) -> ClaudeResult<Client> {
        // Try to get existing client
        {
            let clients = self.clients.read().await;
            if let Some(client) = clients.get(base_url) {
                return Ok(client.clone());
            }
        }
        
        // Create new client
        let client = self.create_client().await?;
        
        // Store client
        {
            let mut clients = self.clients.write().await;
            clients.insert(base_url.to_string(), client.clone());
        }
        
        Ok(client)
    }
    
    async fn create_client(&self) -> ClaudeResult<Client> {
        let client = ClientBuilder::new()
            .pool_max_idle_per_host(self.config.max_idle_connections)
            .pool_idle_timeout(self.config.idle_timeout)
            .timeout(self.config.request_timeout)
            .connect_timeout(self.config.connection_timeout)
            .tcp_keepalive(self.config.tcp_keepalive)
            .http2_keep_alive_interval(self.config.http2_keep_alive_interval)
            .http2_keep_alive_timeout(Duration::from_secs(30))
            .http2_keep_alive_while_idle(true)
            .user_agent("claude-desktop-agent/1.0")
            .build()
            .map_err(|e| ClaudeError::HttpError(e))?;
        
        Ok(client)
    }
    
    pub async fn clear_cache(&self) {
        let mut clients = self.clients.write().await;
        clients.clear();
    }
    
    pub async fn get_stats(&self) -> ConnectionPoolStats {
        let clients = self.clients.read().await;
        ConnectionPoolStats {
            active_connections: clients.len(),
            max_connections: self.config.max_connections_per_host,
        }
    }
}

#[derive(Debug, Clone)]
pub struct ConnectionPoolStats {
    pub active_connections: usize,
    pub max_connections: usize,
}

// Response caching system
#[derive(Debug)]
pub struct ResponseCache {
    cache: Arc<RwLock<HashMap<String, CachedResponse>>>,
    max_entries: usize,
    default_ttl: Duration,
}

#[derive(Debug, Clone)]
struct CachedResponse {
    data: bytes::Bytes,
    cached_at: std::time::Instant,
    ttl: Duration,
    hit_count: u64,
}

impl ResponseCache {
    pub fn new(max_entries: usize, default_ttl: Duration) -> Self {
        Self {
            cache: Arc::new(RwLock::new(HashMap::new())),
            max_entries,
            default_ttl,
        }
    }
    
    pub async fn get(&self, key: &str) -> Option<bytes::Bytes> {
        let mut cache = self.cache.write().await;
        
        if let Some(entry) = cache.get_mut(key) {
            // Check if entry is still valid
            if entry.cached_at.elapsed() < entry.ttl {
                entry.hit_count += 1;
                return Some(entry.data.clone());
            } else {
                // Entry expired, remove it
                cache.remove(key);
            }
        }
        
        None
    }
    
    pub async fn put(&self, key: String, data: bytes::Bytes, ttl: Option<Duration>) {
        let mut cache = self.cache.write().await;
        
        // Evict oldest entries if cache is full
        if cache.len() >= self.max_entries {
            let oldest_key = cache.iter()
                .min_by_key(|(_, entry)| entry.cached_at)
                .map(|(k, _)| k.clone());
                
            if let Some(oldest_key) = oldest_key {
                cache.remove(&oldest_key);
            }
        }
        
        let cached_response = CachedResponse {
            data,
            cached_at: std::time::Instant::now(),
            ttl: ttl.unwrap_or(self.default_ttl),
            hit_count: 0,
        };
        
        cache.insert(key, cached_response);
    }
    
    pub async fn invalidate(&self, key: &str) {
        let mut cache = self.cache.write().await;
        cache.remove(key);
    }
    
    pub async fn clear(&self) {
        let mut cache = self.cache.write().await;
        cache.clear();
    }
    
    pub async fn get_stats(&self) -> CacheStats {
        let cache = self.cache.read().await;
        let total_hits: u64 = cache.values().map(|entry| entry.hit_count).sum();
        let total_size: usize = cache.values().map(|entry| entry.data.len()).sum();
        
        CacheStats {
            entries: cache.len(),
            total_hits,
            total_size,
            max_entries: self.max_entries,
        }
    }
    
    pub async fn cleanup_expired(&self) {
        let mut cache = self.cache.write().await;
        let now = std::time::Instant::now();
        
        cache.retain(|_, entry| now.duration_since(entry.cached_at) < entry.ttl);
    }
}

#[derive(Debug, Clone)]
pub struct CacheStats {
    pub entries: usize,
    pub total_hits: u64,
    pub total_size: usize,
    pub max_entries: usize,
}

impl CacheStats {
    pub fn hit_rate(&self) -> f64 {
        if self.total_hits == 0 {
            return 0.0;
        }
        // This is a simplified calculation - real hit rate would need miss tracking
        self.total_hits as f64 / (self.total_hits as f64 + 1.0)
    }
    
    pub fn average_entry_size(&self) -> f64 {
        if self.entries == 0 {
            return 0.0;
        }
        self.total_size as f64 / self.entries as f64
    }
}
```

### 4. Production Configuration and Hardening

#### 4.1 Production Configuration System
```rust
// New file: src-tauri/src/production/config.rs
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use std::time::Duration;
use crate::claude::error::{ClaudeError, ClaudeResult};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProductionConfig {
    pub environment: Environment,
    pub security: SecurityConfig,
    pub performance: PerformanceConfig,
    pub monitoring: MonitoringConfig,
    pub logging: LoggingConfig,
    pub deployment: DeploymentConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum Environment {
    Development,
    Staging,
    Production,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SecurityConfig {
    pub enable_tls: bool,
    pub tls_cert_path: Option<PathBuf>,
    pub tls_key_path: Option<PathBuf>,
    pub api_key_rotation_interval: Duration,
    pub max_request_size: usize,
    pub rate_limit_requests_per_minute: u32,
    pub enable_request_signing: bool,
    pub whitelist_enforcement_level: WhitelistLevel,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum WhitelistLevel {
    Disabled,
    Warn,
    Enforce,
    Strict,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceConfig {
    pub max_concurrent_requests: usize,
    pub request_timeout: Duration,
    pub connection_pool_size: usize,
    pub memory_pool_max_size: usize,
    pub cache_size_mb: usize,
    pub cache_ttl: Duration,
    pub gc_interval: Duration,
    pub enable_compression: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MonitoringConfig {
    pub enable_metrics: bool,
    pub metrics_export_interval: Duration,
    pub enable_health_checks: bool,
    pub health_check_interval: Duration,
    pub enable_alerting: bool,
    pub alert_webhook_url: Option<String>,
    pub performance_thresholds: PerformanceThresholds,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceThresholds {
    pub max_response_time_ms: u64,
    pub max_memory_usage_mb: u64,
    pub max_cpu_usage_percent: f64,
    pub max_error_rate_percent: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LoggingConfig {
    pub level: LogLevel,
    pub enable_structured_logging: bool,
    pub log_file_path: Option<PathBuf>,
    pub max_log_file_size_mb: usize,
    pub log_rotation_count: usize,
    pub enable_audit_logging: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum LogLevel {
    Error,
    Warn,
    Info,
    Debug,
    Trace,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeploymentConfig {
    pub version: String,
    pub build_hash: Option<String>,
    pub deployment_id: String,
    pub data_directory: PathBuf,
    pub temp_directory: PathBuf,
    pub enable_auto_updates: bool,
    pub backup_interval: Duration,
}

impl Default for ProductionConfig {
    fn default() -> Self {
        Self {
            environment: Environment::Development,
            security: SecurityConfig::default(),
            performance: PerformanceConfig::default(),
            monitoring: MonitoringConfig::default(),
            logging: LoggingConfig::default(),
            deployment: DeploymentConfig::default(),
        }
    }
}

impl Default for SecurityConfig {
    fn default() -> Self {
        Self {
            enable_tls: false,
            tls_cert_path: None,
            tls_key_path: None,
            api_key_rotation_interval: Duration::from_secs(86400 * 30), // 30 days
            max_request_size: 10 * 1024 * 1024, // 10MB
            rate_limit_requests_per_minute: 100,
            enable_request_signing: false,
            whitelist_enforcement_level: WhitelistLevel::Warn,
        }
    }
}

impl Default for PerformanceConfig {
    fn default() -> Self {
        Self {
            max_concurrent_requests: 50,
            request_timeout: Duration::from_secs(120),
            connection_pool_size: 20,
            memory_pool_max_size: 100,
            cache_size_mb: 100,
            cache_ttl: Duration::from_secs(300),
            gc_interval: Duration::from_secs(60),
            enable_compression: true,
        }
    }
}

impl Default for MonitoringConfig {
    fn default() -> Self {
        Self {
            enable_metrics: true,
            metrics_export_interval: Duration::from_secs(60),
            enable_health_checks: true,
            health_check_interval: Duration::from_secs(30),
            enable_alerting: false,
            alert_webhook_url: None,
            performance_thresholds: PerformanceThresholds::default(),
        }
    }
}

impl Default for PerformanceThresholds {
    fn default() -> Self {
        Self {
            max_response_time_ms: 5000,
            max_memory_usage_mb: 512,
            max_cpu_usage_percent: 80.0,
            max_error_rate_percent: 5.0,
        }
    }
}

impl Default for LoggingConfig {
    fn default() -> Self {
        Self {
            level: LogLevel::Info,
            enable_structured_logging: true,
            log_file_path: None,
            max_log_file_size_mb: 100,
            log_rotation_count: 5,
            enable_audit_logging: true,
        }
    }
}

impl Default for DeploymentConfig {
    fn default() -> Self {
        Self {
            version: env!("CARGO_PKG_VERSION").to_string(),
            build_hash: None,
            deployment_id: uuid::Uuid::new_v4().to_string(),
            data_directory: PathBuf::from("./data"),
            temp_directory: PathBuf::from("./tmp"),
            enable_auto_updates: false,
            backup_interval: Duration::from_secs(86400), // Daily
        }
    }
}

impl ProductionConfig {
    pub fn load_from_file(path: &PathBuf) -> ClaudeResult<Self> {
        if !path.exists() {
            // Create default config file
            let default_config = Self::default();
            default_config.save_to_file(path)?;
            return Ok(default_config);
        }
        
        let contents = std::fs::read_to_string(path)
            .map_err(|e| ClaudeError::ConfigError {
                message: format!("Failed to read config file: {}", e),
                context: None,
            })?;
        
        let config: ProductionConfig = toml::from_str(&contents)
            .map_err(|e| ClaudeError::ConfigError {
                message: format!("Failed to parse config file: {}", e),
                context: None,
            })?;
        
        config.validate()?;
        Ok(config)
    }
    
    pub fn save_to_file(&self, path: &PathBuf) -> ClaudeResult<()> {
        let contents = toml::to_string_pretty(self)
            .map_err(|e| ClaudeError::ConfigError {
                message: format!("Failed to serialize config: {}", e),
                context: None,
            })?;
        
        // Ensure parent directory exists
        if let Some(parent) = path.parent() {
            std::fs::create_dir_all(parent)
                .map_err(|e| ClaudeError::ConfigError {
                    message: format!("Failed to create config directory: {}", e),
                    context: None,
                })?;
        }
        
        std::fs::write(path, contents)
            .map_err(|e| ClaudeError::ConfigError {
                message: format!("Failed to write config file: {}", e),
                context: None,
            })?;
        
        Ok(())
    }
    
    pub fn validate(&self) -> ClaudeResult<()> {
        // Validate security config
        if self.security.enable_tls {
            if self.security.tls_cert_path.is_none() || self.security.tls_key_path.is_none() {
                return Err(ClaudeError::ConfigError {
                    message: "TLS cert and key paths must be specified when TLS is enabled".to_string(),
                    context: None,
                });
            }
        }
        
        // Validate performance config
        if self.performance.max_concurrent_requests == 0 {
            return Err(ClaudeError::ConfigError {
                message: "max_concurrent_requests must be greater than 0".to_string(),
                context: None,
            });
        }
        
        if self.performance.connection_pool_size == 0 {
            return Err(ClaudeError::ConfigError {
                message: "connection_pool_size must be greater than 0".to_string(),
                context: None,
            });
        }
        
        // Validate monitoring config
        if self.monitoring.enable_alerting && self.monitoring.alert_webhook_url.is_none() {
            return Err(ClaudeError::ConfigError {
                message: "alert_webhook_url must be specified when alerting is enabled".to_string(),
                context: None,
            });
        }
        
        // Validate deployment config
        if !self.deployment.data_directory.is_absolute() {
            return Err(ClaudeError::ConfigError {
                message: "data_directory must be an absolute path".to_string(),
                context: None,
            });
        }
        
        Ok(())
    }
    
    pub fn is_production(&self) -> bool {
        matches!(self.environment, Environment::Production)
    }
    
    pub fn get_log_level(&self) -> tracing::Level {
        match self.logging.level {
            LogLevel::Error => tracing::Level::ERROR,
            LogLevel::Warn => tracing::Level::WARN,
            LogLevel::Info => tracing::Level::INFO,
            LogLevel::Debug => tracing::Level::DEBUG,
            LogLevel::Trace => tracing::Level::TRACE,
        }
    }
}
```

### 5. Health Monitoring and Alerting

#### 5.1 Health Check System
```rust
// New file: src-tauri/src/production/health.rs
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use serde::{Deserialize, Serialize};
use crate::claude::error::{ClaudeError, ClaudeResult};
use crate::performance::metrics::MetricsCollector;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthStatus {
    pub status: HealthState,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub uptime: Duration,
    pub checks: HashMap<String, CheckResult>,
    pub version: String,
    pub build_hash: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum HealthState {
    Healthy,
    Degraded,
    Unhealthy,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CheckResult {
    pub status: CheckStatus,
    pub message: String,
    pub duration: Duration,
    pub last_success: Option<chrono::DateTime<chrono::Utc>>,
    pub consecutive_failures: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum CheckStatus {
    Pass,
    Warn,
    Fail,
}

#[derive(Debug)]
pub struct HealthMonitor {
    checks: Arc<RwLock<HashMap<String, Box<dyn HealthCheck + Send + Sync>>>>,
    status: Arc<RwLock<HealthStatus>>,
    metrics_collector: Arc<MetricsCollector>,
    start_time: Instant,
    config: HealthConfig,
}

#[derive(Debug, Clone)]
pub struct HealthConfig {
    pub check_interval: Duration,
    pub timeout: Duration,
    pub max_consecutive_failures: u32,
}

impl Default for HealthConfig {
    fn default() -> Self {
        Self {
            check_interval: Duration::from_secs(30),
            timeout: Duration::from_secs(10),
            max_consecutive_failures: 3,
        }
    }
}

#[async_trait::async_trait]
pub trait HealthCheck {
    async fn check(&self) -> CheckResult;
    fn name(&self) -> &str;
}

impl HealthMonitor {
    pub fn new(
        metrics_collector: Arc<MetricsCollector>,
        config: HealthConfig,
    ) -> Self {
        let status = HealthStatus {
            status: HealthState::Healthy,
            timestamp: chrono::Utc::now(),
            uptime: Duration::from_secs(0),
            checks: HashMap::new(),
            version: env!("CARGO_PKG_VERSION").to_string(),
            build_hash: option_env!("BUILD_HASH").map(|s| s.to_string()),
        };
        
        Self {
            checks: Arc::new(RwLock::new(HashMap::new())),
            status: Arc::new(RwLock::new(status)),
            metrics_collector,
            start_time: Instant::now(),
            config,
        }
    }
    
    pub async fn register_check<T>(&self, check: T) 
    where 
        T: HealthCheck + Send + Sync + 'static,
    {
        let mut checks = self.checks.write().await;
        checks.insert(check.name().to_string(), Box::new(check));
    }
    
    pub async fn start_monitoring(&self) {
        let checks = Arc::clone(&self.checks);
        let status = Arc::clone(&self.status);
        let metrics_collector = Arc::clone(&self.metrics_collector);
        let config = self.config.clone();
        let start_time = self.start_time;
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(config.check_interval);
            
            loop {
                interval.tick().await;
                
                let check_start = Instant::now();
                let mut check_results = HashMap::new();
                let mut overall_status = HealthState::Healthy;
                
                // Run all health checks
                {
                    let checks_guard = checks.read().await;
                    for (name, check) in checks_guard.iter() {
                        let result = tokio::time::timeout(config.timeout, check.check()).await;
                        
                        let check_result = match result {
                            Ok(result) => result,
                            Err(_) => CheckResult {
                                status: CheckStatus::Fail,
                                message: "Health check timed out".to_string(),
                                duration: config.timeout,
                                last_success: None,
                                consecutive_failures: 1,
                            },
                        };
                        
                        // Update overall status based on check result
                        match check_result.status {
                            CheckStatus::Fail => {
                                if check_result.consecutive_failures >= config.max_consecutive_failures {
                                    overall_status = HealthState::Unhealthy;
                                } else if matches!(overall_status, HealthState::Healthy) {
                                    overall_status = HealthState::Degraded;
                                }
                            }
                            CheckStatus::Warn => {
                                if matches!(overall_status, HealthState::Healthy) {
                                    overall_status = HealthState::Degraded;
                                }
                            }
                            CheckStatus::Pass => {}
                        }
                        
                        // Record metrics
                        let mut tags = HashMap::new();
                        tags.insert("check_name".to_string(), name.clone());
                        tags.insert("status".to_string(), format!("{:?}", check_result.status));
                        
                        metrics_collector.record_timing(
                            "health_check.duration",
                            check_result.duration,
                            tags.clone(),
                        ).await;
                        
                        metrics_collector.record_counter(
                            "health_check.count",
                            1.0,
                            tags,
                        ).await;
                        
                        check_results.insert(name.clone(), check_result);
                    }
                }
                
                // Update status
                {
                    let mut status_guard = status.write().await;
                    status_guard.status = overall_status;
                    status_guard.timestamp = chrono::Utc::now();
                    status_guard.uptime = start_time.elapsed();
                    status_guard.checks = check_results;
                }
                
                // Record overall health metrics
                let health_score = Self::calculate_health_score(&status.read().await);
                metrics_collector.record_gauge(
                    "health.score",
                    health_score,
                    crate::performance::metrics::MetricUnit::Percentage,
                    HashMap::new(),
                ).await;
                
                let check_duration = check_start.elapsed();
                metrics_collector.record_timing(
                    "health_check.cycle",
                    check_duration,
                    HashMap::new(),
                ).await;
            }
        });
    }
    
    pub async fn get_status(&self) -> HealthStatus {
        self.status.read().await.clone()
    }
    
    fn calculate_health_score(status: &HealthStatus) -> f64 {
        if status.checks.is_empty() {
            return 100.0;
        }
        
        let total_checks = status.checks.len() as f64;
        let passing_checks = status.checks.values()
            .filter(|check| matches!(check.status, CheckStatus::Pass))
            .count() as f64;
        
        (passing_checks / total_checks) * 100.0
    }
}

// Built-in health checks
#[derive(Debug)]
pub struct SystemResourceCheck {
    max_memory_mb: u64,
    max_cpu_percent: f64,
}

impl SystemResourceCheck {
    pub fn new(max_memory_mb: u64, max_cpu_percent: f64) -> Self {
        Self {
            max_memory_mb,
            max_cpu_percent,
        }
    }
}

#[async_trait::async_trait]
impl HealthCheck for SystemResourceCheck {
    async fn check(&self) -> CheckResult {
        let start = Instant::now();
        
        // Check memory usage
        let memory_usage = match Self::get_memory_usage() {
            Ok(usage) => usage,
            Err(e) => {
                return CheckResult {
                    status: CheckStatus::Fail,
                    message: format!("Failed to get memory usage: {}", e),
                    duration: start.elapsed(),
                    last_success: None,
                    consecutive_failures: 1,
                };
            }
        };
        
        let memory_mb = memory_usage / (1024 * 1024);
        if memory_mb > self.max_memory_mb {
            return CheckResult {
                status: CheckStatus::Fail,
                message: format!("Memory usage {} MB exceeds limit {} MB", memory_mb, self.max_memory_mb),
                duration: start.elapsed(),
                last_success: None,
                consecutive_failures: 1,
            };
        }
        
        // Check CPU usage
        let cpu_usage = match Self::get_cpu_usage().await {
            Ok(usage) => usage,
            Err(e) => {
                return CheckResult {
                    status: CheckStatus::Warn,
                    message: format!("Failed to get CPU usage: {}", e),
                    duration: start.elapsed(),
                    last_success: Some(chrono::Utc::now()),
                    consecutive_failures: 0,
                };
            }
        };
        
        let status = if cpu_usage > self.max_cpu_percent {
            CheckStatus::Warn
        } else {
            CheckStatus::Pass
        };
        
        CheckResult {
            status,
            message: format!("Memory: {} MB, CPU: {:.1}%", memory_mb, cpu_usage),
            duration: start.elapsed(),
            last_success: Some(chrono::Utc::now()),
            consecutive_failures: 0,
        }
    }
    
    fn name(&self) -> &str {
        "system_resources"
    }
}

impl SystemResourceCheck {
    fn get_memory_usage() -> Result<u64, Box<dyn std::error::Error>> {
        #[cfg(target_os = "linux")]
        {
            use std::fs;
            let contents = fs::read_to_string("/proc/self/status")?;
            for line in contents.lines() {
                if line.starts_with("VmRSS:") {
                    let kb = line.split_whitespace().nth(1)
                        .ok_or("Invalid VmRSS format")?
                        .parse::<u64>()?;
                    return Ok(kb * 1024); // Convert kB to bytes
                }
            }
            Err("VmRSS not found".into())
        }
        
        #[cfg(not(target_os = "linux"))]
        Ok(0) // Platform-specific implementation needed
    }
    
    async fn get_cpu_usage() -> Result<f64, Box<dyn std::error::Error>> {
        // Simplified CPU usage check
        tokio::time::sleep(Duration::from_millis(100)).await;
        Ok(0.0) // Platform-specific implementation needed
    }
}

#[derive(Debug)]
pub struct ApiConnectivityCheck {
    api_base_url: String,
}

impl ApiConnectivityCheck {
    pub fn new(api_base_url: String) -> Self {
        Self { api_base_url }
    }
}

#[async_trait::async_trait]
impl HealthCheck for ApiConnectivityCheck {
    async fn check(&self) -> CheckResult {
        let start = Instant::now();
        
        // Simple connectivity check
        let client = reqwest::Client::new();
        let result = client.get(&self.api_base_url)
            .timeout(Duration::from_secs(5))
            .send()
            .await;
        
        match result {
            Ok(response) => {
                let status = if response.status().is_success() {
                    CheckStatus::Pass
                } else {
                    CheckStatus::Warn
                };
                
                CheckResult {
                    status,
                    message: format!("API returned status: {}", response.status()),
                    duration: start.elapsed(),
                    last_success: if matches!(status, CheckStatus::Pass) {
                        Some(chrono::Utc::now())
                    } else {
                        None
                    },
                    consecutive_failures: 0,
                }
            }
            Err(e) => CheckResult {
                status: CheckStatus::Fail,
                message: format!("API connectivity failed: {}", e),
                duration: start.elapsed(),
                last_success: None,
                consecutive_failures: 1,
            },
        }
    }
    
    fn name(&self) -> &str {
        "api_connectivity"
    }
}
```

## Architecture Changes

### 1. Current vs. Target Architecture

**Current Architecture:**
```
Basic tool execution
    ↓
Simple error handling
    ↓
No performance monitoring
    ↓
No resource management
```

**Target Architecture:**
```
Performance-Instrumented Tool Execution
    ↓
Comprehensive Metrics Collection
    ↓
Memory Pool Management
    ↓
Connection Pooling & Caching
    ↓
Health Monitoring & Alerting
    ↓
Production Configuration System
    ↓
Automatic Resource Cleanup
```

### 2. File Structure Changes

**New Files:**
- `src-tauri/src/performance/` - Performance monitoring and optimization module
  - `metrics.rs` - Metrics collection and statistics
  - `instrumentation.rs` - Performance instrumentation macros and utilities
  - `memory.rs` - Memory pool management and resource tracking
  - `cleanup.rs` - Automatic resource cleanup manager
  - `connection_pool.rs` - HTTP connection pooling and response caching
  - `mod.rs` - Module exports and configuration
- `src-tauri/src/production/` - Production deployment and hardening
  - `config.rs` - Production configuration system
  - `health.rs` - Health monitoring and alerting
  - `deployment.rs` - Deployment utilities and scripts
  - `security.rs` - Security hardening configurations
  - `mod.rs` - Module exports

**Modified Files:**
- `src-tauri/src/main.rs` - Integration of performance monitoring and production systems
- `src-tauri/src/claude/client.rs` - Performance instrumentation integration
- `src-tauri/src/claude/tools.rs` - Memory-efficient tool execution
- `src-tauri/Cargo.toml` - New dependencies for monitoring and production features

### 3. Integration Points

**Performance Monitoring Integration:**
```rust
// Integration with existing Claude client
impl ClaudeClient {
    pub async fn send_message_with_monitoring(
        &self,
        message: &str,
        metrics_collector: &MetricsCollector,
    ) -> ClaudeResult<String> {
        let start = Instant::now();
        let mut tags = HashMap::new();
        tags.insert("operation".to_string(), "send_message".to_string());
        
        let result = time_operation!(
            metrics_collector,
            "claude.send_message",
            tags.clone(),
            self.send_message(message).await
        );
        
        // Record additional metrics
        match &result {
            Ok(_) => {
                tags.insert("status".to_string(), "success".to_string());
                metrics_collector.record_counter("claude.requests.success", 1.0, tags).await;
            }
            Err(_) => {
                tags.insert("status".to_string(), "error".to_string());
                metrics_collector.record_counter("claude.requests.error", 1.0, tags).await;
            }
        }
        
        result
    }
}
```

## Implementation Plan

### Phase 1: Core Performance Infrastructure (Week 1-2)
**Target: ~500 LOC**

1. **Metrics Collection System** (3 days)
   - Implement `MetricsCollector` and basic metric types
   - Add performance instrumentation macros
   - Create system metrics collection utilities

2. **Memory Management System** (3 days)
   - Implement memory pool for buffer management
   - Add resource tracking and leak detection
   - Create automatic cleanup mechanisms

3. **Performance Instrumentation** (2 days)
   - Add timing guards and measurement utilities
   - Integrate metrics collection into existing code
   - Test basic performance monitoring

### Phase 2: Connection Optimization and Caching (Week 2-3)
**Target: ~400 LOC**

1. **HTTP Connection Pool** (3 days)
   - Implement connection pool with configurable limits
   - Add connection reuse and keep-alive management
   - Optimize for Claude API communication patterns

2. **Response Caching System** (3 days)
   - Implement LRU cache with TTL support
   - Add cache hit/miss tracking
   - Integrate with API response handling

3. **Resource Cleanup Manager** (2 days)
   - Create automatic cleanup scheduler
   - Add memory pressure detection
   - Implement garbage collection triggers

### Phase 3: Production Configuration (Week 3-4)
**Target: ~300 LOC**

1. **Production Config System** (3 days)
   - Create comprehensive configuration structure
   - Add TOML-based configuration loading
   - Implement configuration validation

2. **Security Hardening** (3 days)
   - Add TLS configuration options
   - Implement rate limiting and request signing
   - Create security validation utilities

3. **Environment Detection** (1 day)
   - Add environment-specific configurations
   - Implement deployment-aware settings
   - Test configuration loading

### Phase 4: Health Monitoring and Alerting (Week 4-5)
**Target: ~400 LOC**

1. **Health Check Framework** (3 days)
   - Implement health check interface and runner
   - Add built-in system resource checks
   - Create API connectivity monitoring

2. **Alerting System** (3 days)
   - Implement webhook-based alerting
   - Add threshold monitoring
   - Create alert escalation logic

3. **Monitoring Dashboard** (2 days)
   - Add metrics export endpoints
   - Create health status API
   - Test monitoring integration

### Phase 5: Production Deployment Support (Week 5-6)
**Target: ~400 LOC**

1. **Deployment Utilities** (3 days)
   - Create deployment configuration scripts
   - Add backup and recovery utilities
   - Implement version management

2. **Performance Optimization** (2 days)
   - Add performance profiling tools
   - Implement optimization recommendations
   - Create performance baselines

3. **Integration Testing** (2 days)
   - Test production configuration scenarios
   - Validate performance improvements
   - Test failover and recovery

## Testing Strategy

### 1. Unit Tests (300 LOC)

#### Performance Metrics Tests
```rust
#[cfg(test)]
mod metrics_tests {
    use super::*;
    use std::time::Duration;
    
    #[tokio::test]
    async fn test_metrics_collection() {
        let collector = MetricsCollector::new();
        
        collector.record_timing(
            "test_operation",
            Duration::from_millis(100),
            HashMap::new(),
        ).await;
        
        let metrics = collector.get_metrics(Some("timing.test_operation")).await;
        assert!(!metrics.is_empty());
        
        let stats = collector.get_metric_statistics("timing.test_operation").await;
        assert!(stats.is_some());
        assert_eq!(stats.unwrap().count, 1);
    }
    
    #[tokio::test]
    async fn test_metrics_cleanup() {
        let collector = MetricsCollector::new();
        
        // Record old metric
        collector.record_counter("test_counter", 1.0, HashMap::new()).await;
        
        // Wait for cleanup (in real test, we'd mock time)
        tokio::time::sleep(Duration::from_millis(10)).await;
        
        let metrics = collector.get_metrics(None).await;
        // Cleanup logic would remove old metrics
    }
}
```

#### Memory Pool Tests
```rust
#[cfg(test)]
mod memory_tests {
    use super::*;
    
    #[tokio::test]
    async fn test_memory_pool_allocation() {
        let pool = MemoryPool::new();
        
        let buffer1 = pool.get_buffer(1024).await;
        assert!(buffer1.capacity() >= 1024);
        
        let buffer2 = pool.get_buffer(1024).await;
        assert!(buffer2.capacity() >= 1024);
        
        // Return buffers
        pool.return_buffer(buffer1).await;
        pool.return_buffer(buffer2).await;
        
        // Get buffer again, should reuse
        let buffer3 = pool.get_buffer(1024).await;
        let stats = pool.get_stats().await;
        assert!(stats.cache_hits > 0);
    }
    
    #[tokio::test]
    async fn test_resource_tracking() {
        let tracker = ResourceTracker::new();
        
        tracker.track_allocation("test_resource", 1024).await;
        tracker.track_allocation("test_resource", 2048).await;
        
        let info = tracker.get_resource_info().await;
        assert!(info.contains_key("test_resource"));
        assert_eq!(info["test_resource"].count, 2);
        assert_eq!(info["test_resource"].total_size, 3072);
        
        tracker.track_deallocation("test_resource", 1024).await;
        let info = tracker.get_resource_info().await;
        assert_eq!(info["test_resource"].count, 1);
        assert_eq!(info["test_resource"].total_size, 2048);
    }
}
```

### 2. Integration Tests (200 LOC)

#### Performance Integration Tests
```rust
#[tokio::test]
async fn test_end_to_end_performance_monitoring() {
    let metrics_collector = Arc::new(MetricsCollector::new());
    let memory_pool = Arc::new(MemoryPool::new());
    let cleanup_manager = ResourceCleanupManager::new(
        Arc::clone(&memory_pool),
        Arc::clone(&metrics_collector),
    );
    
    cleanup_manager.start().await;
    
    // Simulate workload
    for i in 0..100 {
        let buffer = memory_pool.get_buffer(1024).await;
        metrics_collector.record_counter(
            "test_operations", 
            1.0, 
            HashMap::new()
        ).await;
        memory_pool.return_buffer(buffer).await;
        
        if i % 10 == 0 {
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    }
    
    // Check metrics
    let stats = metrics_collector.get_metric_statistics("test_operations").await;
    assert!(stats.is_some());
    assert_eq!(stats.unwrap().count, 100);
    
    let pool_stats = memory_pool.get_stats().await;
    assert!(pool_stats.cache_hits > 0);
    
    cleanup_manager.stop().await;
}
```

#### Health Monitoring Tests
```rust
#[tokio::test]
async fn test_health_monitoring_system() {
    let metrics_collector = Arc::new(MetricsCollector::new());
    let health_monitor = HealthMonitor::new(
        Arc::clone(&metrics_collector),
        HealthConfig::default(),
    );
    
    // Register test health check
    health_monitor.register_check(TestHealthCheck::new()).await;
    
    health_monitor.start_monitoring().await;
    
    // Wait for health check cycle
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    let status = health_monitor.get_status().await;
    assert!(!status.checks.is_empty());
    assert!(matches!(status.status, HealthState::Healthy));
}

struct TestHealthCheck;

impl TestHealthCheck {
    fn new() -> Self {
        Self
    }
}

#[async_trait::async_trait]
impl HealthCheck for TestHealthCheck {
    async fn check(&self) -> CheckResult {
        CheckResult {
            status: CheckStatus::Pass,
            message: "Test check passed".to_string(),
            duration: Duration::from_millis(1),
            last_success: Some(chrono::Utc::now()),
            consecutive_failures: 0,
        }
    }
    
    fn name(&self) -> &str {
        "test_check"
    }
}
```

### 3. Performance Tests (100 LOC)

#### Benchmark Tests
```rust
#[cfg(test)]
mod benchmarks {
    use super::*;
    use std::time::Instant;
    
    #[tokio::test]
    async fn benchmark_metrics_collection() {
        let collector = MetricsCollector::new();
        let start = Instant::now();
        
        for i in 0..10000 {
            collector.record_counter(
                "benchmark_counter", 
                i as f64, 
                HashMap::new()
            ).await;
        }
        
        let duration = start.elapsed();
        assert!(duration.as_millis() < 1000, "Metrics collection too slow: {:?}", duration);
    }
    
    #[tokio::test]
    async fn benchmark_memory_pool() {
        let pool = MemoryPool::new();
        let start = Instant::now();
        
        for _ in 0..1000 {
            let buffer = pool.get_buffer(1024).await;
            pool.return_buffer(buffer).await;
        }
        
        let duration = start.elapsed();
        assert!(duration.as_millis() < 100, "Memory pool too slow: {:?}", duration);
    }
}
```

## Dependencies & Integration

### 1. Required Dependencies
This issue requires completion of:
- **Issue 2.3**: Advanced Streaming Implementation (for streaming performance optimization)
- **Issue 2.4**: Enhanced Error Handling and Retry Mechanisms (for robust error handling integration)

### 2. Required Crate Dependencies
```toml
# Add to Cargo.toml
[dependencies]
tokio = { version = "1.0", features = ["full"] }
bytes = "1.5"
reqwest = { version = "0.11", features = ["json", "stream"] }
toml = "0.8"
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1.0", features = ["v4", "serde"] }
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["json"] }
async-trait = "0.1"
libc = "0.2"  # For system metrics on Unix
```

### 3. Integration with Existing Systems

#### 3.1 Enhanced Claude Client Integration
```rust
// Update existing ClaudeClient to use performance monitoring
impl ClaudeClient {
    pub fn new_with_performance(
        config: ClaudeConfig,
        metrics_collector: Arc<MetricsCollector>,
        connection_pool: Arc<HttpConnectionPool>,
    ) -> ClaudeResult<Self> {
        // Enhanced client with performance monitoring
        Ok(Self {
            config,
            metrics_collector: Some(metrics_collector),
            connection_pool: Some(connection_pool),
            // ... other fields
        })
    }
}
```

#### 3.2 Tauri Command Updates
```rust
// Add performance monitoring to Tauri commands
#[tauri::command]
async fn send_message_to_claude_optimized(
    message: String,
    state: tauri::State<'_, AppState>,
) -> Result<String, String> {
    let metrics_collector = &state.metrics_collector;
    let start = Instant::now();
    
    let result = send_message_to_claude(message, state).await;
    
    // Record performance metrics
    let duration = start.elapsed();
    metrics_collector.record_timing(
        "tauri.send_message",
        duration,
        HashMap::new(),
    ).await;
    
    result
}
```

## Security Considerations

### 1. Production Security Hardening
- TLS certificate validation and rotation
- API key security and rotation policies
- Request signing and validation
- Rate limiting and DDoS protection

### 2. Monitoring Security
- Secure metrics export (no sensitive data in metrics)
- Encrypted health check endpoints
- Audit logging for all administrative operations
- Secure webhook endpoints for alerting

### 3. Resource Security
- Memory limit enforcement to prevent resource exhaustion
- Connection pool limits to prevent resource abuse
- Secure cleanup of sensitive data from memory pools
- File system permission validation for log files

### 4. Configuration Security
- Secure storage of production configuration files
- Validation of all configuration parameters
- Principle of least privilege for file access
- Secure defaults for all security-related settings

## Acceptance Criteria

### 1. Performance Monitoring
- [ ] Comprehensive metrics collection system is implemented
- [ ] System resource monitoring (CPU, memory, disk) works correctly
- [ ] Performance instrumentation macros are available and functional
- [ ] Metrics retention and cleanup work as configured
- [ ] Statistics calculation (percentiles, averages) is accurate

### 2. Memory Management
- [ ] Memory pool provides efficient buffer allocation and reuse
- [ ] Resource tracking detects and reports memory leaks
- [ ] Automatic cleanup manager runs on schedule
- [ ] Memory pressure detection triggers appropriate responses
- [ ] Resource cleanup is triggered properly on application shutdown

### 3. Connection Optimization
- [ ] HTTP connection pooling reduces connection overhead
- [ ] Response caching improves performance for repeated requests
- [ ] Connection reuse works correctly across multiple requests
- [ ] Cache hit rates are tracked and reported accurately
- [ ] Connection pool limits are enforced properly

### 4. Production Configuration
- [ ] TOML-based configuration system loads and validates correctly
- [ ] Environment-specific configurations work properly
- [ ] Security settings are applied correctly in production mode
- [ ] Configuration validation catches invalid settings
- [ ] Default configurations are secure and reasonable

### 5. Health Monitoring
- [ ] Health check framework executes checks on schedule
- [ ] Built-in health checks (system resources, API connectivity) work
- [ ] Health status aggregation reflects actual system state
- [ ] Alerting system sends notifications when thresholds are exceeded
- [ ] Health check timeouts and failures are handled gracefully

### 6. Performance Improvements
- [ ] Tool execution performance is improved by at least 20%
- [ ] Memory usage is reduced by at least 15% for long-running sessions
- [ ] API response times are improved through caching and connection pooling
- [ ] Resource cleanup prevents memory leaks in extended operation
- [ ] Performance monitoring overhead is less than 5% of total CPU usage

### 7. Production Readiness
- [ ] All production configurations are documented
- [ ] Deployment scripts and utilities are functional
- [ ] Backup and recovery procedures work correctly
- [ ] Log rotation and management work as configured
- [ ] Security hardening measures are properly implemented

## References

### 1. Performance and Monitoring
- [Rust Performance Book](https://nnethercote.github.io/perf-book/)
- [Tokio Performance Guide](https://docs.rs/tokio/latest/tokio/task/index.html#performance)
- [Metrics Collection Best Practices](https://prometheus.io/docs/practices/naming/)
- [Memory Management in Rust](https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html)

### 2. Production Deployment
- [TOML Configuration Format](https://toml.io/en/)
- [Tracing and Logging in Rust](https://docs.rs/tracing/latest/tracing/)
- [Health Check Patterns](https://microservices.io/patterns/observability/health-check-api.html)
- [Connection Pooling Best Practices](https://docs.rs/reqwest/latest/reqwest/struct.Client.html)

### 3. GitHub Issues
- [Issue 2.3: Advanced Streaming Implementation](https://github.com/user/repo/issues/2.3)
- [Issue 2.4: Enhanced Error Handling](https://github.com/user/repo/issues/2.4)
- [Implementation Sequencing Guide](../implementation-sequencing.md)

### 4. Internal Documentation
- [Architecture Overview](../architecture/overview.md)
- [Security Guidelines](../security/guidelines.md)
- [Performance Testing Guide](../development/performance-testing.md)
- [Production Deployment Guide](../deployment/production.md)

## Estimated Lines of Code

**Implementation: ~1,500 LOC**
- Performance monitoring system: ~500 LOC
- Memory management and cleanup: ~400 LOC
- Connection pooling and caching: ~300 LOC
- Production configuration system: ~200 LOC
- Health monitoring and alerting: ~400 LOC

**Testing: ~600 LOC**
- Unit tests: ~300 LOC
- Integration tests: ~200 LOC
- Performance benchmarks: ~100 LOC

**Total: ~2,100 LOC**

This comprehensive implementation establishes production-ready performance monitoring, optimization, and hardening systems that ensure the Claude Desktop Agent can operate reliably and efficiently in production environments with comprehensive observability and automatic resource management.