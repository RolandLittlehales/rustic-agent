# [4.1] Enhancements: Monitoring and Resilience Enhancement Package

## Overview

This issue implements comprehensive monitoring and resilience patterns to enhance the reliability and observability of the agent application. The package focuses on three core areas: circuit breakers for graceful degradation, health checks for system status monitoring, and structured logging/metrics integration for operational visibility.

### Goals
- **Prevent Cascading Failures**: Implement circuit breakers to isolate failures and prevent system-wide degradation
- **Enable Proactive Monitoring**: Provide health check endpoints for monitoring system components
- **Improve Observability**: Integrate structured logging and metrics collection
- **Enhance Recovery**: Implement automatic recovery mechanisms with exponential backoff
- **Support Operations**: Provide actionable insights for debugging and performance optimization

### Key Components
1. **Circuit Breaker System**: Generic circuit breaker implementation with configurable thresholds
2. **Health Check Framework**: Extensible health check system for all critical components
3. **Metrics Collection**: Integration with OpenTelemetry for metrics and tracing
4. **Structured Logging**: Enhanced logging with correlation IDs and context
5. **Resilience Patterns**: Retry logic, timeouts, and bulkhead isolation

## Technical Requirements

### 1. Circuit Breaker Implementation

```rust
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use thiserror::Error;

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum CircuitState {
    Closed,
    Open,
    HalfOpen,
}

#[derive(Debug, Error)]
pub enum CircuitBreakerError {
    #[error("Circuit breaker is open")]
    CircuitOpen,
    #[error("Operation failed: {0}")]
    OperationFailed(String),
}

#[derive(Debug, Clone)]
pub struct CircuitBreakerConfig {
    pub failure_threshold: u32,
    pub success_threshold: u32,
    pub timeout: Duration,
    pub half_open_max_calls: u32,
}

impl Default for CircuitBreakerConfig {
    fn default() -> Self {
        Self {
            failure_threshold: 5,
            success_threshold: 2,
            timeout: Duration::from_secs(60),
            half_open_max_calls: 3,
        }
    }
}

pub struct CircuitBreaker {
    config: CircuitBreakerConfig,
    state: Arc<RwLock<CircuitBreakerState>>,
}

struct CircuitBreakerState {
    current: CircuitState,
    failure_count: u32,
    success_count: u32,
    last_failure_time: Option<Instant>,
    half_open_calls: u32,
}

impl CircuitBreaker {
    pub fn new(config: CircuitBreakerConfig) -> Self {
        Self {
            config,
            state: Arc::new(RwLock::new(CircuitBreakerState {
                current: CircuitState::Closed,
                failure_count: 0,
                success_count: 0,
                last_failure_time: None,
                half_open_calls: 0,
            })),
        }
    }

    pub async fn call<F, T, E>(&self, operation: F) -> Result<T, CircuitBreakerError>
    where
        F: FnOnce() -> futures::future::BoxFuture<'static, Result<T, E>>,
        E: std::error::Error + Send + Sync + 'static,
    {
        // Check if we should attempt the operation
        if !self.should_attempt().await {
            return Err(CircuitBreakerError::CircuitOpen);
        }

        // Execute the operation
        match operation().await {
            Ok(result) => {
                self.record_success().await;
                Ok(result)
            }
            Err(err) => {
                self.record_failure().await;
                Err(CircuitBreakerError::OperationFailed(err.to_string()))
            }
        }
    }

    async fn should_attempt(&self) -> bool {
        let mut state = self.state.write().await;

        match state.current {
            CircuitState::Closed => true,
            CircuitState::Open => {
                // Check if timeout has elapsed
                if let Some(last_failure) = state.last_failure_time {
                    if last_failure.elapsed() >= self.config.timeout {
                        // Transition to half-open
                        state.current = CircuitState::HalfOpen;
                        state.half_open_calls = 0;
                        true
                    } else {
                        false
                    }
                } else {
                    false
                }
            }
            CircuitState::HalfOpen => {
                // Allow limited calls in half-open state
                if state.half_open_calls < self.config.half_open_max_calls {
                    state.half_open_calls += 1;
                    true
                } else {
                    false
                }
            }
        }
    }

    async fn record_success(&self) {
        let mut state = self.state.write().await;

        match state.current {
            CircuitState::Closed => {
                state.failure_count = 0;
            }
            CircuitState::HalfOpen => {
                state.success_count += 1;
                if state.success_count >= self.config.success_threshold {
                    // Transition to closed
                    state.current = CircuitState::Closed;
                    state.failure_count = 0;
                    state.success_count = 0;
                    state.half_open_calls = 0;
                }
            }
            CircuitState::Open => {} // Shouldn't happen
        }
    }

    async fn record_failure(&self) {
        let mut state = self.state.write().await;
        state.last_failure_time = Some(Instant::now());

        match state.current {
            CircuitState::Closed => {
                state.failure_count += 1;
                if state.failure_count >= self.config.failure_threshold {
                    // Transition to open
                    state.current = CircuitState::Open;
                    state.success_count = 0;
                }
            }
            CircuitState::HalfOpen => {
                // Transition back to open
                state.current = CircuitState::Open;
                state.failure_count = self.config.failure_threshold;
                state.success_count = 0;
                state.half_open_calls = 0;
            }
            CircuitState::Open => {} // Already open
        }
    }

    pub async fn get_state(&self) -> CircuitState {
        self.state.read().await.current
    }
}
```

### 2. Health Check Framework

```rust
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum HealthStatus {
    Healthy,
    Degraded,
    Unhealthy,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheckResult {
    pub status: HealthStatus,
    pub message: Option<String>,
    pub details: HashMap<String, serde_json::Value>,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[async_trait]
pub trait HealthCheck: Send + Sync {
    async fn check(&self) -> HealthCheckResult;
    fn name(&self) -> &str;
}

pub struct HealthCheckRegistry {
    checks: Arc<RwLock<HashMap<String, Box<dyn HealthCheck>>>>,
}

impl HealthCheckRegistry {
    pub fn new() -> Self {
        Self {
            checks: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub async fn register(&self, check: Box<dyn HealthCheck>) {
        let mut checks = self.checks.write().await;
        checks.insert(check.name().to_string(), check);
    }

    pub async fn check_all(&self) -> HashMap<String, HealthCheckResult> {
        let checks = self.checks.read().await;
        let mut results = HashMap::new();

        for (name, check) in checks.iter() {
            results.insert(name.clone(), check.check().await);
        }

        results
    }

    pub async fn get_overall_status(&self) -> HealthStatus {
        let results = self.check_all().await;
        
        let has_unhealthy = results.values().any(|r| matches!(r.status, HealthStatus::Unhealthy));
        let has_degraded = results.values().any(|r| matches!(r.status, HealthStatus::Degraded));

        if has_unhealthy {
            HealthStatus::Unhealthy
        } else if has_degraded {
            HealthStatus::Degraded
        } else {
            HealthStatus::Healthy
        }
    }
}

// Example health checks
pub struct ClaudeAPIHealthCheck {
    client: Arc<ClaudeClient>,
    circuit_breaker: Arc<CircuitBreaker>,
}

#[async_trait]
impl HealthCheck for ClaudeAPIHealthCheck {
    async fn check(&self) -> HealthCheckResult {
        let mut details = HashMap::new();
        
        // Check circuit breaker state
        let circuit_state = self.circuit_breaker.get_state().await;
        details.insert(
            "circuit_breaker".to_string(),
            serde_json::json!({
                "state": format!("{:?}", circuit_state)
            })
        );

        // Check API connectivity
        match self.client.test_connection().await {
            Ok(response_time) => {
                details.insert(
                    "response_time_ms".to_string(),
                    serde_json::json!(response_time.as_millis())
                );
                
                HealthCheckResult {
                    status: if circuit_state == CircuitState::Open {
                        HealthStatus::Degraded
                    } else {
                        HealthStatus::Healthy
                    },
                    message: None,
                    details,
                    timestamp: chrono::Utc::now(),
                }
            }
            Err(err) => HealthCheckResult {
                status: HealthStatus::Unhealthy,
                message: Some(format!("API connection failed: {}", err)),
                details,
                timestamp: chrono::Utc::now(),
            }
        }
    }

    fn name(&self) -> &str {
        "claude_api"
    }
}

pub struct FileSystemHealthCheck {
    whitelist: Arc<RwLock<WhitelistConfig>>,
}

#[async_trait]
impl HealthCheck for FileSystemHealthCheck {
    async fn check(&self) -> HealthCheckResult {
        let mut details = HashMap::new();
        let whitelist = self.whitelist.read().await;
        
        details.insert(
            "whitelist_enabled".to_string(),
            serde_json::json!(whitelist.enabled)
        );
        
        details.insert(
            "whitelist_directories".to_string(),
            serde_json::json!(whitelist.allowed_directories.len())
        );

        // Check if temp directory is writable
        let temp_check = tokio::fs::File::create(std::env::temp_dir().join(".health_check"))
            .await
            .and_then(|_| tokio::fs::remove_file(std::env::temp_dir().join(".health_check")))
            .await;

        match temp_check {
            Ok(_) => HealthCheckResult {
                status: HealthStatus::Healthy,
                message: None,
                details,
                timestamp: chrono::Utc::now(),
            },
            Err(err) => HealthCheckResult {
                status: HealthStatus::Unhealthy,
                message: Some(format!("Filesystem not writable: {}", err)),
                details,
                timestamp: chrono::Utc::now(),
            }
        }
    }

    fn name(&self) -> &str {
        "filesystem"
    }
}
```

### 3. Metrics and Monitoring Integration

```rust
use opentelemetry::{
    metrics::{Counter, Histogram, Meter, Unit},
    KeyValue,
};
use std::time::Instant;

pub struct MetricsCollector {
    meter: Meter,
    api_requests: Counter<u64>,
    api_errors: Counter<u64>,
    api_latency: Histogram<f64>,
    tool_executions: Counter<u64>,
    tool_errors: Counter<u64>,
    tool_latency: Histogram<f64>,
}

impl MetricsCollector {
    pub fn new(meter: Meter) -> Self {
        let api_requests = meter
            .u64_counter("claude_api_requests")
            .with_description("Total number of Claude API requests")
            .init();

        let api_errors = meter
            .u64_counter("claude_api_errors")
            .with_description("Total number of Claude API errors")
            .init();

        let api_latency = meter
            .f64_histogram("claude_api_latency")
            .with_description("Claude API request latency")
            .with_unit(Unit::new("ms"))
            .init();

        let tool_executions = meter
            .u64_counter("tool_executions")
            .with_description("Total number of tool executions")
            .init();

        let tool_errors = meter
            .u64_counter("tool_errors")
            .with_description("Total number of tool execution errors")
            .init();

        let tool_latency = meter
            .f64_histogram("tool_latency")
            .with_description("Tool execution latency")
            .with_unit(Unit::new("ms"))
            .init();

        Self {
            meter,
            api_requests,
            api_errors,
            api_latency,
            tool_executions,
            tool_errors,
            tool_latency,
        }
    }

    pub fn record_api_request(&self, success: bool, latency: Duration, model: &str) {
        let attributes = vec![
            KeyValue::new("model", model.to_string()),
            KeyValue::new("success", success.to_string()),
        ];

        self.api_requests.add(1, &attributes);
        
        if !success {
            self.api_errors.add(1, &attributes);
        }

        self.api_latency.record(latency.as_millis() as f64, &attributes);
    }

    pub fn record_tool_execution(&self, tool_name: &str, success: bool, latency: Duration) {
        let attributes = vec![
            KeyValue::new("tool", tool_name.to_string()),
            KeyValue::new("success", success.to_string()),
        ];

        self.tool_executions.add(1, &attributes);
        
        if !success {
            self.tool_errors.add(1, &attributes);
        }

        self.tool_latency.record(latency.as_millis() as f64, &attributes);
    }
}
```

### 4. Enhanced Tauri Commands with Monitoring

```rust
#[tauri::command]
async fn get_health_status(state: tauri::State<'_, AppState>) -> Result<serde_json::Value, String> {
    let health_registry = &state.health_registry;
    let results = health_registry.check_all().await;
    let overall = health_registry.get_overall_status().await;

    Ok(serde_json::json!({
        "overall_status": overall,
        "checks": results,
        "timestamp": chrono::Utc::now()
    }))
}

#[tauri::command]
async fn get_metrics(state: tauri::State<'_, AppState>) -> Result<serde_json::Value, String> {
    // This would integrate with OpenTelemetry exporters
    // For now, return a placeholder
    Ok(serde_json::json!({
        "message": "Metrics endpoint - integrate with Prometheus/Grafana"
    }))
}
```

## Acceptance Criteria

1. **Circuit Breaker Implementation**
   - Generic circuit breaker that can wrap any async operation
   - Configurable failure/success thresholds and timeout
   - Proper state transitions (Closed → Open → Half-Open → Closed)
   - Integration with Claude API client

2. **Health Check System**
   - Extensible health check framework
   - Built-in checks for Claude API, filesystem, and memory
   - Aggregated health status endpoint
   - Async health check execution

3. **Metrics Collection**
   - OpenTelemetry integration
   - Key metrics: API latency, error rates, tool execution times
   - Prometheus-compatible metrics endpoint
   - Correlation IDs for request tracing

4. **Resilience Patterns**
   - Exponential backoff retry logic
   - Timeout enforcement on all external calls
   - Bulkhead isolation for concurrent operations
   - Graceful degradation when services fail

5. **Monitoring Dashboard**
   - Health status display in UI
   - Real-time metrics visualization
   - Alert configuration for critical thresholds
   - Historical data retention

## Quality Control

### Unit Tests
- Circuit breaker state machine tests
- Health check framework tests
- Metrics collection accuracy tests
- Retry logic verification

### Integration Tests
- End-to-end circuit breaker behavior
- Health check aggregation
- Metrics export validation
- Failure injection testing

### Performance Tests
- Circuit breaker overhead measurement
- Health check execution time
- Metrics collection impact
- Memory usage under load

## Documentation Requirements

- Architecture documentation for resilience patterns
- Monitoring setup guide
- Alert configuration reference
- Troubleshooting playbook

## Dependencies

- [3.1] Error Handling and Recovery System - For consistent error patterns
- [3.2] Performance Optimization Suite - For baseline metrics
- OpenTelemetry Rust SDK
- Prometheus client library

## Health Check System Implementation

### Overview
The health check system provides comprehensive monitoring of application components, external dependencies, and system resources. It supports concurrent execution, customizable timeouts, and detailed status reporting.

### Health Check Components

#### 1. Endpoint Monitoring
```rust
// New file: src-tauri/src/monitoring/endpoint_health.rs
use super::health::{HealthCheck, HealthCheckResult, HealthStatus};
use std::time::Duration;
use async_trait::async_trait;
use reqwest::Client;
use std::collections::HashMap;

/// HTTP endpoint health check with configurable parameters
pub struct HttpEndpointHealthCheck {
    name: String,
    url: String,
    client: Client,
    expected_status: u16,
    timeout: Duration,
    headers: HashMap<String, String>,
}

impl HttpEndpointHealthCheck {
    pub fn new(name: impl Into<String>, url: impl Into<String>) -> Self {
        Self {
            name: name.into(),
            url: url.into(),
            client: Client::new(),
            expected_status: 200,
            timeout: Duration::from_secs(5),
            headers: HashMap::new(),
        }
    }

    pub fn with_expected_status(mut self, status: u16) -> Self {
        self.expected_status = status;
        self
    }

    pub fn with_timeout(mut self, timeout: Duration) -> Self {
        self.timeout = timeout;
        self
    }

    pub fn with_header(mut self, key: impl Into<String>, value: impl Into<String>) -> Self {
        self.headers.insert(key.into(), value.into());
        self
    }
}

#[async_trait]
impl HealthCheck for HttpEndpointHealthCheck {
    async fn check(&self) -> HealthCheckResult {
        let mut request = self.client.get(&self.url);
        
        // Add headers
        for (key, value) in &self.headers {
            request = request.header(key, value);
        }

        match request.send().await {
            Ok(response) => {
                let status_code = response.status().as_u16();
                let response_time = response
                    .headers()
                    .get("x-response-time")
                    .and_then(|v| v.to_str().ok())
                    .and_then(|v| v.parse::<f64>().ok());

                if status_code == self.expected_status {
                    let mut result = HealthCheckResult::healthy(
                        format!("Endpoint {} is responding correctly", self.name)
                    )
                    .with_detail("status_code", status_code)
                    .with_detail("url", self.url.clone());

                    if let Some(time) = response_time {
                        result = result.with_detail("response_time_ms", time);
                    }

                    result
                } else {
                    HealthCheckResult::unhealthy(
                        format!("Endpoint {} returned unexpected status", self.name)
                    )
                    .with_detail("expected_status", self.expected_status)
                    .with_detail("actual_status", status_code)
                    .with_detail("url", self.url.clone())
                }
            }
            Err(err) => {
                HealthCheckResult::unhealthy(
                    format!("Failed to reach endpoint {}", self.name)
                )
                .with_detail("error", err.to_string())
                .with_detail("url", self.url.clone())
            }
        }
    }

    fn name(&self) -> &str {
        &self.name
    }

    fn timeout(&self) -> Duration {
        self.timeout
    }
}

/// Multi-endpoint health check with aggregated status
pub struct MultiEndpointHealthCheck {
    name: String,
    endpoints: Vec<HttpEndpointHealthCheck>,
    min_healthy_percentage: f64,
}

impl MultiEndpointHealthCheck {
    pub fn new(name: impl Into<String>) -> Self {
        Self {
            name: name.into(),
            endpoints: Vec::new(),
            min_healthy_percentage: 0.5, // Default: at least 50% must be healthy
        }
    }

    pub fn add_endpoint(mut self, endpoint: HttpEndpointHealthCheck) -> Self {
        self.endpoints.push(endpoint);
        self
    }

    pub fn with_min_healthy_percentage(mut self, percentage: f64) -> Self {
        self.min_healthy_percentage = percentage.clamp(0.0, 1.0);
        self
    }
}

#[async_trait]
impl HealthCheck for MultiEndpointHealthCheck {
    async fn check(&self) -> HealthCheckResult {
        if self.endpoints.is_empty() {
            return HealthCheckResult::unhealthy("No endpoints configured");
        }

        let mut tasks = Vec::new();
        
        // Check all endpoints concurrently
        for endpoint in &self.endpoints {
            let endpoint = endpoint.clone();
            tasks.push(tokio::spawn(async move {
                endpoint.check().await
            }));
        }

        let mut healthy_count = 0;
        let mut results = Vec::new();

        for (i, task) in tasks.into_iter().enumerate() {
            match task.await {
                Ok(result) => {
                    if result.status == HealthStatus::Healthy {
                        healthy_count += 1;
                    }
                    results.push((self.endpoints[i].name(), result));
                }
                Err(err) => {
                    results.push((
                        self.endpoints[i].name(),
                        HealthCheckResult::unhealthy(format!("Task failed: {}", err))
                    ));
                }
            }
        }

        let healthy_percentage = healthy_count as f64 / self.endpoints.len() as f64;
        
        let mut aggregated_result = if healthy_percentage >= self.min_healthy_percentage {
            if healthy_percentage == 1.0 {
                HealthCheckResult::healthy("All endpoints are healthy")
            } else {
                HealthCheckResult::degraded(
                    format!("{}/{} endpoints are healthy", healthy_count, self.endpoints.len())
                )
            }
        } else {
            HealthCheckResult::unhealthy(
                format!("Only {}/{} endpoints are healthy", healthy_count, self.endpoints.len())
            )
        };

        // Add individual endpoint results as details
        for (name, result) in results {
            aggregated_result = aggregated_result.with_detail(
                format!("endpoint_{}", name),
                serde_json::json!({
                    "status": result.status,
                    "message": result.message,
                    "details": result.details
                })
            );
        }

        aggregated_result
            .with_detail("healthy_count", healthy_count)
            .with_detail("total_count", self.endpoints.len())
            .with_detail("healthy_percentage", healthy_percentage)
            .with_detail("min_healthy_percentage", self.min_healthy_percentage)
    }

    fn name(&self) -> &str {
        &self.name
    }

    fn timeout(&self) -> Duration {
        Duration::from_secs(10) // Allow more time for multi-endpoint checks
    }
}
```

#### 2. Dependency Health Checks
```rust
// New file: src-tauri/src/monitoring/dependency_health.rs
use super::health::{HealthCheck, HealthCheckResult, HealthStatus};
use std::time::Duration;
use async_trait::async_trait;
use tokio::process::Command;
use std::path::Path;

/// External command health check (for checking CLI tools, services)
pub struct CommandHealthCheck {
    name: String,
    command: String,
    args: Vec<String>,
    expected_exit_code: i32,
    timeout: Duration,
}

impl CommandHealthCheck {
    pub fn new(name: impl Into<String>, command: impl Into<String>) -> Self {
        Self {
            name: name.into(),
            command: command.into(),
            args: Vec::new(),
            expected_exit_code: 0,
            timeout: Duration::from_secs(5),
        }
    }

    pub fn with_args(mut self, args: Vec<String>) -> Self {
        self.args = args;
        self
    }

    pub fn with_expected_exit_code(mut self, code: i32) -> Self {
        self.expected_exit_code = code;
        self
    }
}

#[async_trait]
impl HealthCheck for CommandHealthCheck {
    async fn check(&self) -> HealthCheckResult {
        let mut cmd = Command::new(&self.command);
        cmd.args(&self.args);

        match tokio::time::timeout(self.timeout, cmd.output()).await {
            Ok(Ok(output)) => {
                let exit_code = output.status.code().unwrap_or(-1);
                
                if exit_code == self.expected_exit_code {
                    HealthCheckResult::healthy(
                        format!("Command {} executed successfully", self.name)
                    )
                    .with_detail("command", self.command.clone())
                    .with_detail("exit_code", exit_code)
                    .with_detail("stdout", String::from_utf8_lossy(&output.stdout).to_string())
                } else {
                    HealthCheckResult::unhealthy(
                        format!("Command {} returned unexpected exit code", self.name)
                    )
                    .with_detail("command", self.command.clone())
                    .with_detail("expected_exit_code", self.expected_exit_code)
                    .with_detail("actual_exit_code", exit_code)
                    .with_detail("stderr", String::from_utf8_lossy(&output.stderr).to_string())
                }
            }
            Ok(Err(err)) => {
                HealthCheckResult::unhealthy(
                    format!("Failed to execute command {}", self.name)
                )
                .with_detail("command", self.command.clone())
                .with_detail("error", err.to_string())
            }
            Err(_) => {
                HealthCheckResult::unhealthy(
                    format!("Command {} timed out", self.name)
                )
                .with_detail("command", self.command.clone())
                .with_detail("timeout_seconds", self.timeout.as_secs())
            }
        }
    }

    fn name(&self) -> &str {
        &self.name
    }

    fn timeout(&self) -> Duration {
        self.timeout
    }
}

/// Database connection health check
pub struct DatabaseHealthCheck {
    name: String,
    connection_string: String,
    query: String,
    timeout: Duration,
}

impl DatabaseHealthCheck {
    pub fn new(
        name: impl Into<String>,
        connection_string: impl Into<String>,
    ) -> Self {
        Self {
            name: name.into(),
            connection_string: connection_string.into(),
            query: "SELECT 1".to_string(),
            timeout: Duration::from_secs(5),
        }
    }

    pub fn with_query(mut self, query: impl Into<String>) -> Self {
        self.query = query.into();
        self
    }
}

#[async_trait]
impl HealthCheck for DatabaseHealthCheck {
    async fn check(&self) -> HealthCheckResult {
        // Note: This is a simplified example. In real implementation,
        // you would use actual database drivers like sqlx, diesel, etc.
        
        // For demonstration, we'll simulate a database check
        if self.connection_string.contains("localhost") {
            HealthCheckResult::healthy("Database connection successful")
                .with_detail("query", self.query.clone())
                .with_detail("response_time_ms", 15)
        } else {
            HealthCheckResult::unhealthy("Cannot connect to database")
                .with_detail("error", "Connection refused")
        }
    }

    fn name(&self) -> &str {
        &self.name
    }

    fn timeout(&self) -> Duration {
        self.timeout
    }
}
```

#### 3. Status Aggregation
```rust
// New file: src-tauri/src/monitoring/status_aggregator.rs
use super::health::{HealthCheckResult, HealthStatus, SystemHealthReport};
use std::collections::HashMap;
use serde::{Serialize, Deserialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComponentStatus {
    pub name: String,
    pub category: String,
    pub status: HealthStatus,
    pub message: String,
    pub last_check: chrono::DateTime<chrono::Utc>,
    pub dependencies: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AggregatedHealthStatus {
    pub overall_status: HealthStatus,
    pub components: HashMap<String, ComponentStatus>,
    pub categories: HashMap<String, CategoryStatus>,
    pub dependencies: DependencyGraph,
    pub timestamp: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CategoryStatus {
    pub name: String,
    pub status: HealthStatus,
    pub healthy_count: usize,
    pub degraded_count: usize,
    pub unhealthy_count: usize,
    pub total_count: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DependencyGraph {
    pub nodes: HashMap<String, DependencyNode>,
    pub edges: Vec<(String, String)>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DependencyNode {
    pub name: String,
    pub status: HealthStatus,
    pub impact_level: ImpactLevel,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ImpactLevel {
    Critical,  // System cannot function without this
    High,      // Major functionality impaired
    Medium,    // Some features unavailable
    Low,       // Minor impact on functionality
}

pub struct StatusAggregator {
    component_categories: HashMap<String, String>,
    dependency_map: HashMap<String, Vec<String>>,
    impact_levels: HashMap<String, ImpactLevel>,
}

impl StatusAggregator {
    pub fn new() -> Self {
        Self {
            component_categories: HashMap::new(),
            dependency_map: HashMap::new(),
            impact_levels: HashMap::new(),
        }
    }

    pub fn register_component(
        &mut self,
        name: impl Into<String>,
        category: impl Into<String>,
        impact: ImpactLevel,
        dependencies: Vec<String>,
    ) {
        let name = name.into();
        self.component_categories.insert(name.clone(), category.into());
        self.dependency_map.insert(name.clone(), dependencies);
        self.impact_levels.insert(name, impact);
    }

    pub fn aggregate_health_report(
        &self,
        report: &SystemHealthReport,
    ) -> AggregatedHealthStatus {
        let mut components = HashMap::new();
        let mut categories: HashMap<String, CategoryStatus> = HashMap::new();

        // Process each health check result
        for (name, result) in &report.checks {
            let category = self.component_categories
                .get(name)
                .cloned()
                .unwrap_or_else(|| "uncategorized".to_string());

            let dependencies = self.dependency_map
                .get(name)
                .cloned()
                .unwrap_or_default();

            let component = ComponentStatus {
                name: name.clone(),
                category: category.clone(),
                status: result.status.clone(),
                message: result.message.clone(),
                last_check: result.timestamp,
                dependencies,
            };

            components.insert(name.clone(), component);

            // Update category statistics
            let cat_status = categories.entry(category).or_insert_with(|| {
                CategoryStatus {
                    name: category.clone(),
                    status: HealthStatus::Healthy,
                    healthy_count: 0,
                    degraded_count: 0,
                    unhealthy_count: 0,
                    total_count: 0,
                }
            });

            cat_status.total_count += 1;
            match result.status {
                HealthStatus::Healthy => cat_status.healthy_count += 1,
                HealthStatus::Degraded => cat_status.degraded_count += 1,
                HealthStatus::Unhealthy => cat_status.unhealthy_count += 1,
            }
        }

        // Calculate category statuses
        for (_, cat_status) in categories.iter_mut() {
            if cat_status.unhealthy_count > 0 {
                cat_status.status = HealthStatus::Unhealthy;
            } else if cat_status.degraded_count > 0 {
                cat_status.status = HealthStatus::Degraded;
            } else {
                cat_status.status = HealthStatus::Healthy;
            }
        }

        // Build dependency graph
        let dependency_graph = self.build_dependency_graph(&components);

        // Calculate overall status considering dependencies
        let overall_status = self.calculate_overall_status(&components, &dependency_graph);

        AggregatedHealthStatus {
            overall_status,
            components,
            categories,
            dependencies: dependency_graph,
            timestamp: chrono::Utc::now(),
        }
    }

    fn build_dependency_graph(
        &self,
        components: &HashMap<String, ComponentStatus>,
    ) -> DependencyGraph {
        let mut nodes = HashMap::new();
        let mut edges = Vec::new();

        for (name, component) in components {
            let impact = self.impact_levels
                .get(name)
                .cloned()
                .unwrap_or(ImpactLevel::Medium);

            nodes.insert(name.clone(), DependencyNode {
                name: name.clone(),
                status: component.status.clone(),
                impact_level: impact,
            });

            for dep in &component.dependencies {
                edges.push((name.clone(), dep.clone()));
            }
        }

        DependencyGraph { nodes, edges }
    }

    fn calculate_overall_status(
        &self,
        components: &HashMap<String, ComponentStatus>,
        graph: &DependencyGraph,
    ) -> HealthStatus {
        let mut has_critical_failure = false;
        let mut has_degradation = false;

        for (name, node) in &graph.nodes {
            match (&node.status, &node.impact_level) {
                (HealthStatus::Unhealthy, ImpactLevel::Critical) => {
                    has_critical_failure = true;
                }
                (HealthStatus::Unhealthy, ImpactLevel::High) => {
                    has_degradation = true;
                }
                (HealthStatus::Degraded, ImpactLevel::Critical | ImpactLevel::High) => {
                    has_degradation = true;
                }
                _ => {}
            }

            // Check if unhealthy dependencies affect this component
            if let Some(component) = components.get(name) {
                for dep_name in &component.dependencies {
                    if let Some(dep_node) = graph.nodes.get(dep_name) {
                        if dep_node.status == HealthStatus::Unhealthy {
                            match node.impact_level {
                                ImpactLevel::Critical => has_critical_failure = true,
                                ImpactLevel::High => has_degradation = true,
                                _ => {}
                            }
                        }
                    }
                }
            }
        }

        if has_critical_failure {
            HealthStatus::Unhealthy
        } else if has_degradation {
            HealthStatus::Degraded
        } else {
            HealthStatus::Healthy
        }
    }
}

impl Default for StatusAggregator {
    fn default() -> Self {
        Self::new()
    }
}
```

### Usage Examples

#### Setting Up Health Checks
```rust
// Example setup in main application
let health_manager = HealthCheckManager::new()
    .with_interval(Duration::from_secs(30));

// Register endpoint health checks
let api_endpoints = MultiEndpointHealthCheck::new("api_cluster")
    .add_endpoint(
        HttpEndpointHealthCheck::new("api_primary", "https://api.primary.com/health")
            .with_expected_status(200)
    )
    .add_endpoint(
        HttpEndpointHealthCheck::new("api_secondary", "https://api.secondary.com/health")
            .with_expected_status(200)
    )
    .with_min_healthy_percentage(0.5); // At least one endpoint must be healthy

health_manager.register_check(Arc::new(api_endpoints)).await;

// Register dependency checks
health_manager.register_check(Arc::new(
    CommandHealthCheck::new("git_available", "git")
        .with_args(vec!["--version".to_string()])
)).await;

health_manager.register_check(Arc::new(
    DatabaseHealthCheck::new("main_db", "postgresql://localhost/myapp")
        .with_query("SELECT version()")
)).await;

// Setup status aggregator
let mut aggregator = StatusAggregator::new();
aggregator.register_component(
    "claude_api",
    "external_services",
    ImpactLevel::Critical,
    vec![]
);
aggregator.register_component(
    "api_cluster",
    "external_services",
    ImpactLevel::High,
    vec!["claude_api".to_string()]
);

// Start periodic health checks
health_manager.start_periodic_checks().await;
```

#### Retrieving Health Status
```rust
#[tauri::command]
async fn get_health_status(
    state: tauri::State<'_, AppState>,
) -> Result<AggregatedHealthStatus, String> {
    let report = state.health_manager.run_all_checks().await;
    let aggregated = state.status_aggregator.aggregate_health_report(&report);
    
    Ok(aggregated)
}

#[tauri::command]
async fn get_component_health(
    component_name: String,
    state: tauri::State<'_, AppState>,
) -> Result<HealthCheckResult, String> {
    state.health_manager
        .run_check(&component_name)
        .await
        .ok_or_else(|| format!("Component '{}' not found", component_name))
}
```

## Estimated Scope

~1300 lines of code total:
- Circuit breaker implementation: 200 lines
- Health check framework: 250 lines
- Endpoint monitoring: 150 lines
- Dependency checks: 100 lines
- Status aggregation: 200 lines
- Metrics integration: 200 lines
- Resilience utilities: 100 lines
- Tauri command updates: 100 lines

### Implementation Features:
- HTTP endpoint monitoring with concurrent execution
- Command-based dependency health checks
- Database connection health verification
- Multi-endpoint aggregated status checks
- Intelligent status aggregation with dependency analysis
- Category-based health organization
- Impact-level based overall status calculation
- Configurable health check timeouts and thresholds

## Appendix: Detailed Circuit Breaker Implementation

### Circuit Breaker State Machine Implementation

```rust
// src-tauri/src/monitoring/circuit_breaker_impl.rs

use std::sync::atomic::{AtomicU64, AtomicU32, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{RwLock, Mutex};
use serde::{Serialize, Deserialize};
use thiserror::Error;

/// Circuit breaker errors
#[derive(Debug, Error)]
pub enum CircuitBreakerError<E> {
    #[error("Circuit breaker is open")]
    CircuitOpen,
    #[error("Operation failed: {0}")]
    OperationFailed(E),
    #[error("Circuit breaker timeout")]
    Timeout,
}

/// Circuit breaker state machine states
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum CircuitState {
    /// Normal operation - requests are allowed
    Closed,
    /// Failure state - requests are rejected
    Open,
    /// Recovery testing - limited requests allowed
    HalfOpen,
}

/// Advanced circuit breaker configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CircuitBreakerConfig {
    /// Number of consecutive failures to open circuit
    pub failure_threshold: u32,
    /// Number of consecutive successes to close circuit from half-open
    pub success_threshold: u32,
    /// Duration to wait before attempting recovery
    pub timeout_duration: Duration,
    /// Failure rate threshold (0.0 - 1.0)
    pub failure_rate_threshold: f64,
    /// Minimum requests before failure rate is considered
    pub minimum_requests: u32,
    /// Time window for failure rate calculation
    pub sliding_window_size: Duration,
    /// Maximum concurrent requests in half-open state
    pub half_open_max_requests: u32,
    /// Exponential backoff multiplier for repeated opens
    pub backoff_multiplier: f64,
    /// Maximum backoff duration
    pub max_backoff_duration: Duration,
}

impl Default for CircuitBreakerConfig {
    fn default() -> Self {
        Self {
            failure_threshold: 5,
            success_threshold: 3,
            timeout_duration: Duration::from_secs(60),
            failure_rate_threshold: 0.5,
            minimum_requests: 10,
            sliding_window_size: Duration::from_secs(60),
            half_open_max_requests: 3,
            backoff_multiplier: 2.0,
            max_backoff_duration: Duration::from_secs(300),
        }
    }
}

/// Time-based sliding window for failure rate calculation
#[derive(Debug)]
struct SlidingWindow {
    window_size: Duration,
    buckets: Vec<(Instant, AtomicU32, AtomicU32)>, // (timestamp, successes, failures)
    current_bucket: usize,
}

impl SlidingWindow {
    fn new(window_size: Duration, bucket_count: usize) -> Self {
        let bucket_duration = window_size.as_secs() / bucket_count as u64;
        let now = Instant::now();
        
        let buckets = (0..bucket_count)
            .map(|i| {
                let timestamp = now - Duration::from_secs(i as u64 * bucket_duration);
                (timestamp, AtomicU32::new(0), AtomicU32::new(0))
            })
            .collect();

        Self {
            window_size,
            buckets,
            current_bucket: 0,
        }
    }

    fn record_success(&mut self) {
        self.rotate_buckets_if_needed();
        self.buckets[self.current_bucket].1.fetch_add(1, Ordering::Relaxed);
    }

    fn record_failure(&mut self) {
        self.rotate_buckets_if_needed();
        self.buckets[self.current_bucket].2.fetch_add(1, Ordering::Relaxed);
    }

    fn get_failure_rate(&self) -> (u32, u32, f64) {
        let cutoff = Instant::now() - self.window_size;
        let mut total_successes = 0;
        let mut total_failures = 0;

        for (timestamp, successes, failures) in &self.buckets {
            if *timestamp >= cutoff {
                total_successes += successes.load(Ordering::Relaxed);
                total_failures += failures.load(Ordering::Relaxed);
            }
        }

        let total = total_successes + total_failures;
        let rate = if total == 0 {
            0.0
        } else {
            total_failures as f64 / total as f64
        };

        (total_successes, total_failures, rate)
    }

    fn rotate_buckets_if_needed(&mut self) {
        let now = Instant::now();
        let bucket_duration = self.window_size.as_secs() / self.buckets.len() as u64;
        let current_timestamp = &self.buckets[self.current_bucket].0;
        
        if now.duration_since(*current_timestamp).as_secs() >= bucket_duration {
            // Move to next bucket
            self.current_bucket = (self.current_bucket + 1) % self.buckets.len();
            
            // Reset the bucket we're moving to
            self.buckets[self.current_bucket].0 = now;
            self.buckets[self.current_bucket].1.store(0, Ordering::Relaxed);
            self.buckets[self.current_bucket].2.store(0, Ordering::Relaxed);
        }
    }
}

/// Advanced circuit breaker with state machine
pub struct CircuitBreaker {
    config: Arc<CircuitBreakerConfig>,
    state: Arc<RwLock<CircuitState>>,
    consecutive_failures: AtomicU32,
    consecutive_successes: AtomicU32,
    half_open_requests: AtomicU32,
    last_state_change: Arc<RwLock<Instant>>,
    failure_count: AtomicU64,
    sliding_window: Arc<Mutex<SlidingWindow>>,
    open_count: AtomicU32,
    current_timeout: Arc<RwLock<Duration>>,
    state_listeners: Arc<RwLock<Vec<Box<dyn Fn(CircuitState, CircuitState) + Send + Sync>>>>,
}

impl CircuitBreaker {
    pub fn new(config: CircuitBreakerConfig) -> Self {
        let timeout = config.timeout_duration;
        let window_size = config.sliding_window_size;
        
        Self {
            config: Arc::new(config),
            state: Arc::new(RwLock::new(CircuitState::Closed)),
            consecutive_failures: AtomicU32::new(0),
            consecutive_successes: AtomicU32::new(0),
            half_open_requests: AtomicU32::new(0),
            last_state_change: Arc::new(RwLock::new(Instant::now())),
            failure_count: AtomicU64::new(0),
            sliding_window: Arc::new(Mutex::new(SlidingWindow::new(window_size, 10))),
            open_count: AtomicU32::new(0),
            current_timeout: Arc::new(RwLock::new(timeout)),
            state_listeners: Arc::new(RwLock::new(Vec::new())),
        }
    }

    /// Execute an operation through the circuit breaker
    pub async fn call<F, T, E, Fut>(&self, operation: F) -> Result<T, CircuitBreakerError<E>>
    where
        F: FnOnce() -> Fut,
        Fut: std::future::Future<Output = Result<T, E>>,
        E: std::error::Error,
    {
        // Check if request should be allowed
        match self.should_allow_request().await {
            RequestDecision::Allow => {},
            RequestDecision::Reject => {
                return Err(CircuitBreakerError::CircuitOpen);
            }
            RequestDecision::AllowLimited => {
                // In half-open state, check concurrent request limit
                let current = self.half_open_requests.fetch_add(1, Ordering::Relaxed);
                if current >= self.config.half_open_max_requests {
                    self.half_open_requests.fetch_sub(1, Ordering::Relaxed);
                    return Err(CircuitBreakerError::CircuitOpen);
                }
            }
        }

        // Execute operation with timeout
        let timeout_duration = Duration::from_secs(30); // Operation timeout
        let result = tokio::time::timeout(timeout_duration, operation()).await;

        // Handle half-open cleanup
        if *self.state.read().await == CircuitState::HalfOpen {
            self.half_open_requests.fetch_sub(1, Ordering::Relaxed);
        }

        // Process result
        match result {
            Ok(Ok(value)) => {
                self.on_success().await;
                Ok(value)
            }
            Ok(Err(error)) => {
                self.on_failure().await;
                Err(CircuitBreakerError::OperationFailed(error))
            }
            Err(_) => {
                self.on_failure().await;
                Err(CircuitBreakerError::Timeout)
            }
        }
    }

    /// Decision for request handling
    #[derive(Debug)]
    enum RequestDecision {
        Allow,
        Reject,
        AllowLimited,
    }

    async fn should_allow_request(&self) -> RequestDecision {
        let current_state = *self.state.read().await;
        
        match current_state {
            CircuitState::Closed => {
                // Check failure rate in closed state
                if self.should_check_failure_rate().await {
                    RequestDecision::Allow
                } else {
                    // Preemptively open if failure rate is too high
                    self.transition_to_state(CircuitState::Open).await;
                    RequestDecision::Reject
                }
            }
            CircuitState::Open => {
                let last_change = *self.last_state_change.read().await;
                let current_timeout = *self.current_timeout.read().await;
                
                if last_change.elapsed() >= current_timeout {
                    // Attempt recovery
                    self.transition_to_state(CircuitState::HalfOpen).await;
                    RequestDecision::AllowLimited
                } else {
                    RequestDecision::Reject
                }
            }
            CircuitState::HalfOpen => RequestDecision::AllowLimited,
        }
    }

    async fn should_check_failure_rate(&self) -> bool {
        let window = self.sliding_window.lock().await;
        let (successes, failures, rate) = window.get_failure_rate();
        let total = successes + failures;
        
        if total < self.config.minimum_requests {
            return true; // Not enough data
        }
        
        rate < self.config.failure_rate_threshold
    }

    async fn on_success(&self) {
        // Update sliding window
        self.sliding_window.lock().await.record_success();
        
        // Reset consecutive failures
        self.consecutive_failures.store(0, Ordering::Relaxed);
        
        let current_state = *self.state.read().await;
        
        match current_state {
            CircuitState::HalfOpen => {
                let successes = self.consecutive_successes.fetch_add(1, Ordering::Relaxed) + 1;
                
                if successes >= self.config.success_threshold {
                    self.transition_to_state(CircuitState::Closed).await;
                    
                    // Reset backoff on successful recovery
                    *self.current_timeout.write().await = self.config.timeout_duration;
                    self.open_count.store(0, Ordering::Relaxed);
                }
            }
            CircuitState::Closed => {
                // Normal operation, just track success
                self.consecutive_successes.fetch_add(1, Ordering::Relaxed);
            }
            CircuitState::Open => {
                // Shouldn't happen, but handle gracefully
                self.transition_to_state(CircuitState::HalfOpen).await;
            }
        }
    }

    async fn on_failure(&self) {
        // Update sliding window
        self.sliding_window.lock().await.record_failure();
        
        // Track failure
        self.failure_count.fetch_add(1, Ordering::Relaxed);
        self.consecutive_successes.store(0, Ordering::Relaxed);
        
        let failures = self.consecutive_failures.fetch_add(1, Ordering::Relaxed) + 1;
        let current_state = *self.state.read().await;
        
        match current_state {
            CircuitState::Closed => {
                if failures >= self.config.failure_threshold {
                    self.transition_to_state(CircuitState::Open).await;
                    self.apply_backoff().await;
                }
            }
            CircuitState::HalfOpen => {
                // Single failure in half-open returns to open
                self.transition_to_state(CircuitState::Open).await;
                self.apply_backoff().await;
            }
            CircuitState::Open => {
                // Already open, just track the failure
            }
        }
    }

    async fn apply_backoff(&self) {
        let open_count = self.open_count.fetch_add(1, Ordering::Relaxed) + 1;
        
        let mut new_timeout = self.config.timeout_duration;
        for _ in 1..open_count {
            new_timeout = Duration::from_secs_f64(
                new_timeout.as_secs_f64() * self.config.backoff_multiplier
            );
            
            if new_timeout > self.config.max_backoff_duration {
                new_timeout = self.config.max_backoff_duration;
                break;
            }
        }
        
        *self.current_timeout.write().await = new_timeout;
    }

    async fn transition_to_state(&self, new_state: CircuitState) {
        let mut current_state = self.state.write().await;
        let old_state = *current_state;
        
        if old_state != new_state {
            *current_state = new_state;
            *self.last_state_change.write().await = Instant::now();
            
            // Reset counters based on transition
            match new_state {
                CircuitState::Open => {
                    self.consecutive_failures.store(0, Ordering::Relaxed);
                    self.consecutive_successes.store(0, Ordering::Relaxed);
                    self.half_open_requests.store(0, Ordering::Relaxed);
                }
                CircuitState::HalfOpen => {
                    self.consecutive_successes.store(0, Ordering::Relaxed);
                    self.half_open_requests.store(0, Ordering::Relaxed);
                }
                CircuitState::Closed => {
                    self.consecutive_failures.store(0, Ordering::Relaxed);
                    self.consecutive_successes.store(0, Ordering::Relaxed);
                    self.half_open_requests.store(0, Ordering::Relaxed);
                }
            }
            
            // Notify listeners
            let listeners = self.state_listeners.read().await;
            for listener in listeners.iter() {
                listener(old_state, new_state);
            }
            
            drop(listeners);
            drop(current_state);
            
            // Log state transition
            self.log_state_transition(old_state, new_state).await;
        }
    }

    async fn log_state_transition(&self, from: CircuitState, to: CircuitState) {
        let window = self.sliding_window.lock().await;
        let (successes, failures, rate) = window.get_failure_rate();
        
        println!(
            "Circuit breaker state transition: {:?} -> {:?} | Success: {} | Failures: {} | Rate: {:.2}%",
            from, to, successes, failures, rate * 100.0
        );
    }

    /// Add a state change listener
    pub async fn add_state_listener<F>(&self, listener: F)
    where
        F: Fn(CircuitState, CircuitState) + Send + Sync + 'static,
    {
        self.state_listeners.write().await.push(Box::new(listener));
    }

    /// Get current circuit state
    pub async fn get_state(&self) -> CircuitState {
        *self.state.read().await
    }

    /// Get circuit breaker statistics
    pub async fn get_stats(&self) -> CircuitBreakerStats {
        let window = self.sliding_window.lock().await;
        let (successes, failures, failure_rate) = window.get_failure_rate();
        
        CircuitBreakerStats {
            state: *self.state.read().await,
            total_successes: successes,
            total_failures: failures,
            failure_rate,
            consecutive_failures: self.consecutive_failures.load(Ordering::Relaxed),
            consecutive_successes: self.consecutive_successes.load(Ordering::Relaxed),
            half_open_requests: self.half_open_requests.load(Ordering::Relaxed),
            current_timeout: *self.current_timeout.read().await,
            open_count: self.open_count.load(Ordering::Relaxed),
        }
    }

    /// Force the circuit to a specific state (for testing/admin)
    pub async fn force_state(&self, state: CircuitState) {
        self.transition_to_state(state).await;
    }

    /// Reset all statistics
    pub async fn reset_stats(&self) {
        self.consecutive_failures.store(0, Ordering::Relaxed);
        self.consecutive_successes.store(0, Ordering::Relaxed);
        self.failure_count.store(0, Ordering::Relaxed);
        self.open_count.store(0, Ordering::Relaxed);
        *self.current_timeout.write().await = self.config.timeout_duration;
        
        // Reset sliding window
        let mut window = self.sliding_window.lock().await;
        *window = SlidingWindow::new(self.config.sliding_window_size, 10);
    }
}

/// Circuit breaker statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CircuitBreakerStats {
    pub state: CircuitState,
    pub total_successes: u32,
    pub total_failures: u32,
    pub failure_rate: f64,
    pub consecutive_failures: u32,
    pub consecutive_successes: u32,
    pub half_open_requests: u32,
    pub current_timeout: Duration,
    pub open_count: u32,
}

/// Circuit breaker builder for fluent configuration
pub struct CircuitBreakerBuilder {
    config: CircuitBreakerConfig,
}

impl CircuitBreakerBuilder {
    pub fn new() -> Self {
        Self {
            config: CircuitBreakerConfig::default(),
        }
    }

    pub fn failure_threshold(mut self, threshold: u32) -> Self {
        self.config.failure_threshold = threshold;
        self
    }

    pub fn success_threshold(mut self, threshold: u32) -> Self {
        self.config.success_threshold = threshold;
        self
    }

    pub fn timeout_duration(mut self, duration: Duration) -> Self {
        self.config.timeout_duration = duration;
        self
    }

    pub fn failure_rate_threshold(mut self, rate: f64) -> Self {
        self.config.failure_rate_threshold = rate.clamp(0.0, 1.0);
        self
    }

    pub fn minimum_requests(mut self, min: u32) -> Self {
        self.config.minimum_requests = min;
        self
    }

    pub fn sliding_window_size(mut self, duration: Duration) -> Self {
        self.config.sliding_window_size = duration;
        self
    }

    pub fn half_open_max_requests(mut self, max: u32) -> Self {
        self.config.half_open_max_requests = max;
        self
    }

    pub fn backoff_multiplier(mut self, multiplier: f64) -> Self {
        self.config.backoff_multiplier = multiplier.max(1.0);
        self
    }

    pub fn max_backoff_duration(mut self, duration: Duration) -> Self {
        self.config.max_backoff_duration = duration;
        self
    }

    pub fn build(self) -> CircuitBreaker {
        CircuitBreaker::new(self.config)
    }
}

impl Default for CircuitBreakerBuilder {
    fn default() -> Self {
        Self::new()
    }
}
```

This detailed implementation provides a comprehensive circuit breaker with:

1. **State Machine Implementation**:
   - Three states: Closed, Open, and HalfOpen
   - Proper state transitions with guards and conditions
   - State change notifications via listeners

2. **Failure Detection**:
   - Consecutive failure counting
   - Sliding window for failure rate calculation
   - Configurable thresholds and time windows
   - Preemptive opening based on failure rates

3. **Recovery Mechanisms**:
   - Automatic transition to half-open after timeout
   - Limited concurrent requests in half-open state
   - Exponential backoff for repeated failures
   - Successful recovery resets backoff

4. **Advanced Features**:
   - Time-based sliding window with buckets
   - Configurable backoff strategies
   - State change listeners for monitoring
   - Comprehensive statistics and metrics
   - Builder pattern for easy configuration

The implementation is thread-safe, performant, and suitable for production use in the Claude Agent application.

## Monitoring Platform Integration (Prometheus & Grafana)

### 1. Prometheus Configuration and Setup

#### 1.1 Prometheus Server Configuration
```yaml
# prometheus.yml - Main Prometheus configuration
global:
  scrape_interval: 15s
  evaluation_interval: 15s

# Alerting configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets: ['localhost:9093']

# Rule files
rule_files:
  - 'claude_agent_rules.yml'

# Scrape configurations
scrape_configs:
  - job_name: 'claude_agent'
    static_configs:
      - targets: ['localhost:9090']
    metrics_path: '/metrics'
    scrape_interval: 10s
    scrape_timeout: 5s
    honor_labels: true
    honor_timestamps: true
```

#### 1.2 Alert Rules Configuration
```yaml
# claude_agent_rules.yml - Prometheus alert rules
groups:
  - name: claude_agent_critical
    interval: 30s
    rules:
      - alert: ClaudeAgentDown
        expr: up{job="claude_agent"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Claude Agent is down"
          description: "Claude Agent has been down for more than 2 minutes"

      - alert: CircuitBreakerOpen
        expr: claude_agent_circuit_breaker_state == 1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Circuit breaker is open"
          description: "Circuit breaker {{ $labels.circuit_name }} has been open for more than 1 minute"

  - name: claude_agent_warnings
    interval: 60s
    rules:
      - alert: HighErrorRate
        expr: rate(claude_agent_errors_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes"

      - alert: HighMemoryUsage
        expr: claude_agent_memory_usage_bytes > 2147483648
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanize1024 }}B"
```

### 2. Grafana Dashboard Definitions

#### 2.1 Overview Dashboard
```json
{
  "dashboard": {
    "title": "Claude Agent - System Overview",
    "uid": "claude-overview",
    "tags": ["claude-agent", "overview"],
    "time": {"from": "now-1h", "to": "now"},
    "refresh": "30s",
    "panels": [
      {
        "title": "Service Status",
        "type": "stat",
        "gridPos": {"h": 6, "w": 4, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "up{job=\"claude_agent\"}",
            "legendFormat": "Status"
          }
        ],
        "options": {
          "colorMode": "background",
          "graphMode": "none"
        },
        "fieldConfig": {
          "defaults": {
            "mappings": [
              {"type": "value", "value": "0", "text": "DOWN", "color": "red"},
              {"type": "value", "value": "1", "text": "UP", "color": "green"}
            ]
          }
        }
      },
      {
        "title": "Request Rate",
        "type": "graph",
        "gridPos": {"h": 8, "w": 10, "x": 4, "y": 0},
        "targets": [
          {
            "expr": "rate(claude_agent_requests_total[5m])",
            "legendFormat": "Requests/sec"
          }
        ],
        "yAxes": [
          {"label": "Requests per second", "min": 0}
        ]
      },
      {
        "title": "Error Rate",
        "type": "stat",
        "gridPos": {"h": 6, "w": 4, "x": 14, "y": 0},
        "targets": [
          {
            "expr": "rate(claude_agent_errors_total[5m]) / rate(claude_agent_requests_total[5m]) * 100",
            "legendFormat": "Error %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 1},
                {"color": "red", "value": 5}
              ]
            }
          }
        }
      }
    ]
  }
}
```

### 3. Enhanced Metrics Exporter

#### 3.1 Prometheus Format Enhancement
```rust
// Enhanced metrics exporter in src-tauri/src/monitoring/prometheus.rs
pub struct PrometheusExporter {
    registry: Arc<MetricsRegistry>,
    server: Option<tokio::task::JoinHandle<()>>,
}

impl PrometheusExporter {
    pub async fn start_server(&mut self, port: u16) -> Result<(), Box<dyn std::error::Error>> {
        let registry = self.registry.clone();
        
        let app = Router::new()
            .route("/metrics", get(metrics_endpoint))
            .route("/health", get(health_endpoint))
            .with_state(registry);
        
        let listener = TcpListener::bind(format!("0.0.0.0:{}", port)).await?;
        
        let server = tokio::spawn(async move {
            axum::serve(listener, app).await.unwrap();
        });
        
        self.server = Some(server);
        println!("Prometheus metrics server started on port {}", port);
        Ok(())
    }

    pub fn format_metrics(&self, metrics: HashMap<String, MetricValue>) -> String {
        let mut output = String::new();
        
        // Add HELP and TYPE comments for each metric
        for (name, value) in &metrics {
            let metric_type = determine_metric_type(name);
            output.push_str(&format!("# HELP claude_agent_{} {}\n", name, name));
            output.push_str(&format!("# TYPE claude_agent_{} {}\n", name, metric_type));
            
            // Format metric line
            if value.labels.is_empty() {
                output.push_str(&format!("claude_agent_{} {}\n", name, value.value));
            } else {
                let labels = format_labels(&value.labels);
                output.push_str(&format!("claude_agent_{}{{{}}} {}\n", name, labels, value.value));
            }
        }
        
        output
    }
}

async fn metrics_endpoint(State(registry): State<Arc<MetricsRegistry>>) -> impl IntoResponse {
    let metrics = registry.collect_all_metrics().await;
    let prometheus_output = format_prometheus_metrics(metrics);
    
    Response::builder()
        .status(StatusCode::OK)
        .header("Content-Type", "text/plain; version=0.0.4; charset=utf-8")
        .body(prometheus_output)
        .unwrap()
}

fn determine_metric_type(name: &str) -> &'static str {
    if name.ends_with("_total") || name.ends_with("_count") {
        "counter"
    } else if name.contains("_duration") || name.contains("_latency") {
        "histogram"
    } else {
        "gauge"
    }
}
```

### 4. Docker Compose Monitoring Stack

#### 4.1 Complete Monitoring Setup
```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: claude_prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:10.1.0
    container_name: claude_grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=claude_admin_2024
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    ports:
      - "3000:3000"
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    networks:
      - monitoring

volumes:
  prometheus_data:
  grafana_data:

networks:
  monitoring:
    driver: bridge
```

### 5. Monitoring Integration Module

#### 5.1 Integration Service
```rust
// src-tauri/src/monitoring/integration.rs
use super::{MetricsRegistry, PrometheusExporter};
use std::sync::Arc;

pub struct MonitoringIntegration {
    metrics: Arc<MetricsRegistry>,
    exporter: PrometheusExporter,
}

impl MonitoringIntegration {
    pub async fn new() -> Result<Self, Box<dyn std::error::Error>> {
        let metrics = Arc::new(MetricsRegistry::new());
        let exporter = PrometheusExporter::new(metrics.clone());
        
        // Setup default metrics
        Self::setup_metrics(&metrics).await;
        
        Ok(Self { metrics, exporter })
    }
    
    async fn setup_metrics(registry: &MetricsRegistry) {
        // Core application metrics
        registry.register_counter("requests_total").await;
        registry.register_counter("errors_total").await;
        registry.register_histogram("request_duration_seconds").await;
        registry.register_gauge("memory_usage_bytes").await;
        registry.register_gauge("circuit_breaker_state").await;
    }
    
    pub async fn start_prometheus_server(&self) -> Result<(), Box<dyn std::error::Error>> {
        self.exporter.start_server(9090).await
    }
    
    pub async fn record_request(&self, duration: f64, success: bool) {
        self.metrics.get_counter("requests_total").inc();
        if !success {
            self.metrics.get_counter("errors_total").inc();
        }
        self.metrics.get_histogram("request_duration_seconds").observe(duration);
    }
}
```

This monitoring platform integration provides comprehensive Prometheus and Grafana setup with metrics exporters, dashboards, Docker deployment, and integration components targeting ~250 lines of implementation code.